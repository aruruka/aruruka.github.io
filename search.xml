<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>YARN Architecture</title>
      <link href="2020/10/26/yarn-architecture/"/>
      <url>2020/10/26/yarn-architecture/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是-YARN-以及-YARN-的架构"><a href="#什么是-YARN-以及-YARN-的架构" class="headerlink" title="什么是 YARN 以及 YARN 的架构"></a>什么是 YARN 以及 YARN 的架构</h1><h2 id="什么是-YARN"><a href="#什么是-YARN" class="headerlink" title="什么是 YARN"></a>什么是 YARN</h2><p>YARN的基本思想是将资源管理和作业调度/监视的功能拆分为单独的守护程序。这个想法是拥有一个全局ResourceManager（RM）和对每个应用程序拥有一个ApplicationMaster（AM）。</p><p>应用程序可以是单个作业(e.g. MapReduce Job)，也可以是作业的DAG。</p><h3 id="什么是-DAG"><a href="#什么是-DAG" class="headerlink" title="什么是 DAG"></a>什么是 DAG</h3><p>在数学，尤其是图论和计算机科学中，<strong>有向无环图</strong>是没有<strong>有向环</strong>的<strong>有向图</strong>。<br>这听起来跟没解释一样，没办法，我的数学基础为零。</p><p>先看下维基百科上对于DAG(directed acyclic graph)的解释：</p><blockquote><p>也就是说，它由顶点(vertices)和边(edge)（也称为圆弧）组成，每个边都从一个顶点指向另一个顶点，并且在这个图(graph)内不存在从一个顶点出发最终会回到这个顶点的情况，也就是不存在环(cycle)。<br>等效地，DAG是有向图，它具有拓扑顺序，即一系列顶点，使得每个边缘从该序列的较早方向到最后位置。</p></blockquote><p>听起来稍微有点理解，但还是比较模糊。还是看看图片帮助自己理解一下。<br><strong><em>图例 1 - 链表、树和图</em></strong></p><p><img src="https://i.ibb.co/R9x451g/chain-tree-graph.jpg" alt="链表、树和图"></p><p>上图是图论中的几个基本概念 – 链表、树和图。图片来自知乎专栏 – <a href="https://zhuanlan.zhihu.com/p/52424180" target="_blank" rel="noopener">有向无环图 DAG</a>。</p><p>再来看一个维基百科上的DAG的图例：<br><strong><em>图例 2 - 一个DAG示例图</em></strong></p><p><img src="https://i.ibb.co/pQ4N76Y/Directed-acyclic-graph-2-svg.png" alt="一个DAG的图例"></p><p>圆圈就是顶点，箭头就是边。</p><p>最后看一下环是什么：<br><strong><em>图例 3 - 一个环状图的示例图</em></strong></p><p><img src="https://i.ibb.co/TvycgMk/Directed-Cycleof-Length8.png" alt="环状图的图例"></p><p>由此可见上面<strong>图例 1</strong>中的“图”也是一个DAG。</p><p>那么归纳一下，在YARN中，可以理解为：<br>一个DAG job集中的job位于一个整体的工作流中，且经过job-j这个阶段后，此工作流不会再经过job-j。</p><h2 id="YARN-的架构"><a href="#YARN-的架构" class="headerlink" title="YARN 的架构"></a>YARN 的架构</h2><p>YARN属于典型的“一主多从”架构。“主”为资源管理器(ResourceManager)，“从”为节点管理器(NodeManager)。</p><p>资源管理器负责整个集群的资源调度管理；节点管理器负责具体服务器上的资源和任务管理。因为Hadoop是将存储与计算放在一起来进行分布式计算的，这是基本思想，所以节点管理器通常是与HDFS的DataNode一起出现。</p><p>整体架构图如下：</p><p><img src="https://i.ibb.co/zsb8tf6/YARNArchitecture.jpg" alt="YARN架构图"></p><p>具体说来，资源管理器又包括两个主要组件：调度器和应用程序管理器。</p><p>调度器其实就是一个资源分配算法，根据应用程序（Client）提交的资源申请和当前服务器集群的资源状况进行资源分配。<br>Yarn 内置了几种资源调度算法，包括 Fair Scheduler、Capacity Scheduler 等，你也可以开发自己的资源调度算法供 Yarn 调用。<br>Yarn 进行资源分配的单位是容器（Container），每个容器包含了一定量的内存、CPU 等计算资源，默认配置下，每个容器包含一个 CPU 核心。<br>容器由 NodeManager 进程启动和管理，NodeManger 进程会监控本节点上容器的运行状况并向 ResourceManger 进程汇报。</p><p>应用程序管理器负责应用程序的提交、监控应用程序运行状态等。应用程序启动后需要在集群中运行一个 ApplicationMaster，ApplicationMaster 也需要运行在容器里面。<br>每个应用程序启动后都会先启动自己的 ApplicationMaster，由 ApplicationMaster 根据应用程序的资源需求进一步向 ResourceManager 进程申请容器资源，得到容器以后就会分发自己的应用程序代码到容器上启动，进而开始分布式计算。</p><p>我们以一个 MapReduce 程序为例，来看一下 Yarn 的整个工作流程。</p><ol><li><p>我们向 Yarn 提交应用程序，包括 MapReduce ApplicationMaster、</p><p>我们的 MapReduce 程序，以及 MapReduce Application 启动命令。</p></li><li><p>ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个容器，</p><p>并将 MapReduce ApplicationMaster 分发到这个容器上面，并在容器里面启动 MapReduce ApplicationMaster。</p></li><li><p>MapReduce ApplicationMaster 启动后立即向 ResourceManager 进程注册，并为自己的应用程序申请容器资源。</p></li><li><p>MapReduce ApplicationMaster 申请到需要的容器后，立即和相应的 NodeManager 进程通信，将用户 MapReduce<br>程序分发到 NodeManager 进程所在服务器，并在容器中运行，运行的就是 Map 或者 Reduce 任务。</p></li><li><p>Map 或者 Reduce 任务在运行期和 MapReduce ApplicationMaster 通信，汇报自己的运行状态，如果运行结束，</p><p>MapReduce ApplicationMaster 向 ResourceManager 进程注销并释放所有的容器资源。</p></li></ol><p>MapReduce 如果想在 Yarn 上运行，就需要开发遵循 Yarn 规范的 MapReduce ApplicationMaster，<br>相应地，其他大数据计算框架也可以开发遵循 Yarn 规范的 ApplicationMaster，这样在一个 Yarn 集群中就可以同时并发执行各种不同的大数据计算框架，实现资源的统一调度管理。</p><h3 id="YARN-组件之间的交互"><a href="#YARN-组件之间的交互" class="headerlink" title="YARN 组件之间的交互"></a>YARN 组件之间的交互</h3><p>上面我提到了资源管理负责整个集群的资源调度管理，那么具体来看下有哪些逻辑单元呢？</p><p><img src="https://i.ibb.co/fQj8TsK/YARNComponent-Interaction.png" alt="YARN组件之间的交互"></p><p>资源管理器在内部保存着2种状态，一个是对于集群整体，另一个是对于应用程序。</p><h4 id="集群状态"><a href="#集群状态" class="headerlink" title="集群状态"></a>集群状态</h4><p>通过节点管理器默认地每秒钟发送<strong>心跳</strong>到资源管理器，来创建集群状态。<br>如果有一个节点超过10分钟没有心跳，资源管理器则会将此节点从调度中剥离。<br>另外除了心跳机制以外，一个节点还有可能变得<strong>“不健康”</strong>，如果如此，资源管理器也会将此节点拉黑并从调度观点中摘除。</p><p>那么节点管理器是否健康取决于什么呢？<br>基本上有2个原因会导致节点管理器不健康：</p><ol><li><p>节点的健康状况通过一个health-check脚本来检查，用户可以自定义此脚本，这是在多数后台进程中常见的方式。</p></li><li><p>磁盘验证器发现节点上超过75％的磁盘使用率已经达到90%。一般来说，我们不应该将高水位线和低水位线设置为相同的阈值。</p><p>例如，如果我们为容器配置了1个磁盘，并且该磁盘已满90％，则我们有100％的磁盘状况不佳，因为我们达到了90％的高水位线。<br> 这将导致NodeManager异常。 如果将低水位线配置为85％，则只有当我们在此节点上至少有15％的可用空间时，磁盘才会被视为正常，并且NodeManager会被标记为HEALTHY，并且可再次用于调度目的。</p></li></ol><h4 id="应用状态"><a href="#应用状态" class="headerlink" title="应用状态"></a>应用状态</h4><p>当应用程序的Application Master开始运行时，将设置应用程序状态。 节点管理器跟踪其内存中所有正在运行的应用程序，最多10,000个应用程序。</p><h3 id="YARN-应用程序提交"><a href="#YARN-应用程序提交" class="headerlink" title="YARN 应用程序提交"></a>YARN 应用程序提交</h3><p>在本文先前的架构总览部分中介绍过YARN的“工作流程”，现在对此展开详细一点的介绍。如果看了上面的部分无法解决问题，可以看看下面这段描述。这对于分析YARN作业出问题的部位会有帮助。</p><p><img src="https://i.ibb.co/M7gjj2n/yarn-Application-Submission.png" alt="YARN应用程序提交"></p><h4 id="Step-1-客户端应用程序请求"><a href="#Step-1-客户端应用程序请求" class="headerlink" title="Step 1: 客户端应用程序请求"></a>Step 1: 客户端应用程序请求</h4><p>YARN应用程序从客户端应用程序请求开始。 上图说明了客户端与资源管理器的ApplicationManager组件进行通信以启动此过程的初始步骤。 客户端必须首先通知资源管理器它要提交应用程序。<br>资源管理器使用ApplicationID和有关集群功能的信息进行响应，该信息将帮助客户端请求资源。</p><h4 id="Step-2-ApplicationMaster-容器分配"><a href="#Step-2-ApplicationMaster-容器分配" class="headerlink" title="Step 2: ApplicationMaster 容器分配"></a>Step 2: ApplicationMaster 容器分配</h4><p>接下来，客户端以“应用程序提交上下文”进行响应。 应用程序提交上下文包含应用程序ID，用户，队列和其他启动ApplicationMaster所需的信息。<br>另外，“容器启动上下文”（CLC）被发送到ResourceManager。 CLC提供了资源要求（内存/ CPU），作业文件，安全令牌以及在节点上启动ApplicationMaster所需的其他信息。</p><p>当资源管理器从客户端接收到应用程序提交上下文时，它将为ApplicationMaster计划可用的容器。<br>该容器通常被称为“容器0”，因为它是ApplicationMaster，它必须请求其他容器。<br>如果没有适用的容器，则请求必须等待。如果可以找到合适的容器，则资源管理器与合适的NodeManager联系并启动ApplicationMaster。<br>作为此步骤的一部分，将建立ApplicationMaster RPC端口和用于监视应用程序状态的跟踪URL。</p><h3 id="YARN-组件"><a href="#YARN-组件" class="headerlink" title="YARN 组件"></a>YARN 组件</h3><h4 id="资源管理器中的服务"><a href="#资源管理器中的服务" class="headerlink" title="资源管理器中的服务"></a>资源管理器中的服务</h4><p><img src="https://i.ibb.co/b5xYqXw/resource-Manager-Services.png" alt="资源管理器中的服务"></p><ul><li><p>Client Service</p><p>此组件（监听8032端口）处理从客户端到ResourceManager的所有远程过程调用（RPC）通信，包括以下操作：</p><ul><li>应用提交</li><li>应用终止</li><li>暴露有关应用程序，队列，集群统计信息，用户ACL等的信息</li></ul></li><li><p>Admin Service</p><p>此组件（监听8033端口）处理到客户端的其他RPC端口（默认情况下为8033）上的Admin操作调用，以便普通用户的请求不会占用admin请求的资源带宽。<br>它处理如下请求：</p><ul><li>添加新队列，删除现有队列以及重新配置队列以更改其某些属性（例如容量，限制等）</li><li>添加新安装的节点或停用现有节点</li><li>添加新的“用户与组”的映射，添加/更新管理员ACL，修改超级用户列表等等</li></ul></li><li><p>Web Server</p><p>这是显而易见的。 它是Web应用程序，它公开有关群集指标，应用程序列表，应用程序状态等的信息。在非SSL群集中，它运行在端口8088上，在SSL群集中运行在端口8090上。</p></li><li><p>ApplicationACLsManager</p><p>资源管理器需要控制诸如客户端和管理请求之类的面向用户的API，以便仅让授权的用户才能访问它们。 该组件维护每个应用程序的ACL并执行它们。</p></li><li><p>ApplicationsManager</p><ul><li>这是RM中已提交应用程序的应用程序“仓库”。<br>它会验证应用程序的规范，并拒绝一些应用程序的请求，这些请求将导致该应用程序的ApplicationMaster资源不够用（也就是说这个情况是集群中没有足够的资源来运行该应用的ApplicationMaster本身）。</li><li>然后，它可以确保没有其他应用程序以相同的应用程序ID提交（这种情况可能是由错误或恶意的客户端引起的）。 最后，它将已接受的应用程序转发给scheduler。</li><li>在应用程序结束后，为了支持用户对应用程序数据的请求（通过Web UI或命令行），它还能保留已完成的应用程序的缓存。</li></ul></li><li><p>ApplicationMasters Service</p><p>此组件在端口8030响应来自所有ApplicationMaster的请求。它实现ApplicationMasterProtocol，<br>这是ApplicationMaster用来与ResourceManager通信的唯一协议。 它负责以下任务：</p><ul><li>注册新的ApplicationMaster</li><li>终止/注销任何完成的ApplicationMaster的请求</li><li>授权来自各种ApplicationMaster的所有请求以确保仅有效的ApplicationMaster将请求发送到驻留在ResourceManager中的相应Application实体</li><li>从所有正在运行的ApplicationMaster获得容器分配和释放请求，并将它们异步转发到YarnScheduler</li></ul><p>ApplicationMastersService具有其他逻辑，以确保在任何时间点，任何ApplicationMaster中只有一个线程可以将请求发送到ResourceManager。<br>来自ApplicationMaster的所有RPC都在ResourceManager上序列化，因此可以预期ApplicationMaster中只有一个线程会发出这些请求。</p></li><li><p>AM LivelinessMonitor</p><p>为了帮助管理活动的ApplicationMaster和死的/未响应的ApplicationMaster列表，此监视器跟踪每个ApplicationMaster及其最后的心跳时间。<br>在所配置的时间间隔（默认为10分钟）内未产生心跳的任何ApplicationMaster都被视为已死，并且被ResourceManager处理为过期。<br>当前正在运行/已分配给到期的ApplicationMaster的所有容器都标记为已失效。<br>ResourceManager重新计划了同一应用程序，以便在新容器上运行新的“应用程序尝试”，默认情况下最多允许两次此类尝试。</p></li><li><p>ContainerAllocationExpirer</p><p>该组件负责确保所有分配的容器最终由ApplicationMaster使用，并随后在相应的NodeManager上启动。<br>ApplicationMaster是不受信任的用户代码，它们可能在获得了资源分配后而不实际使用它（也就是启动具体任务）。因此，它们可能导致群集资源的利用率不足和滥用。<br>为了解决这个问题，ContainerAllocationExpirer维护了已分配但尚未在相应的NodeManager上使用的容器列表。<br>对于任何容器，如果相应的NodeManager均未在配置的时间间隔（默认为10分钟）内向ResourceManager报告该容器已开始运行，则该容器被视为已失效，并且被ResourceManager处理为过期。</p><p>此外，NodeManager会独立查看此到期时间，该到期时间编码在与容器绑定的ContainerToken中，并拒绝在到期时间过后提交启动的容器。</p></li><li><p>Scheduler (Fair)</p><p>它负责根据容量，队列等的约束将资源分配给各种正在运行的应用程序。</p></li><li><p>Resource Tracker Service</p><p>默认情况下，NodeManager会定期在端口8031上将心跳信号发送到资源管理器，并且资源管理器的此组件负责响应来自所有节点的此类RPC。<br>具体来说，它负责以下任务：</p><ol><li>注册新节点/取消注册现有节点，</li><li>接受来自先前注册的节点的节点心跳</li><li>确保只有“有效”节点才能与ResourceManager交互，并拒绝任何其他节点</li></ol></li><li><p>NodeManagers Liveliness Monitor</p><p>为了跟踪活动的节点并明确标识任何死的节点，此组件跟踪每个节点的标识符（ID）及其最后的心跳时间。<br><em>在配置的时间间隔内（默认情况下为10分钟）未发送心跳的任何节点都被视为已死，并且被ResourceManager过期。</em>当前在过期节点上运行的所有容器都标记为已失效，并且在该节点上不会安排任何新容器。<br>一旦此类节点重新启动（自动或通过管理员的干预）并重新注册，将再次考虑对其进行调度。</p></li><li><p>Nodes-List Manager</p><p>节点列表管理器是资源管理器内存中有效节点和排除节点的集合。它负责读取通过<code>yarn.resourcemanager.nodes.include-path</code>和<code>yarn.resourcemanager.nodes.exclude-path</code>配置属性指定的主机配置文件，并基于这些文件在节点上<em>做种</em>文件的初始列表。<br>它还会随着时间的推移跟踪被管理员明确停用的节点。</p><blockquote><p>注：此处的做种为peer-to-peer文件共享领域中的seeding的意思。</p></blockquote><p>在这里，我不会过多地讨论安全层面，而只是说资源管理器负责为YARN组件生成令牌以及更新委托令牌，以防止各个容器访问KDC来验证自己的身份以访问服务。<br>有关更多讨论，请参见博客<a href="https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/</a>。</p></li></ul><h4 id="节点管理器中的服务"><a href="#节点管理器中的服务" class="headerlink" title="节点管理器中的服务"></a>节点管理器中的服务</h4><p><img src="https://i.ibb.co/ZBjW1gc/node-Manager-Services.png" alt="节点管理器中的服务"></p><p>从高层角度来看节点管理器的职责：</p><p>1.跟踪节点运行状况，<br>2.容器生命周期管理；监视各个容器的资源使用情况（例如内存，CPU），<br>3.资源本地化，<br>4.管理容器生成的日志，包括日志聚合，<br>5.允许可插拔的辅助服务，例如 ShuffleHandler，<br>6.与资源管理器通讯并保持最新状态（通过心跳到资源管理器）</p><ul><li><p>NodeStatusUpdater</p><p>在启动时，此组件向资源管理器注册。 它发送有关此节点上可用资源的信息，并标识节点管理器的Web服务器和RPC服务器正在侦听的端口。<br>向资源管理器注册后，此服务将启动一个线程，用于每1秒对资源管理器进行心跳。<br>节点管理器至资源管理器心跳间隔是通过以下设置配置的：<code>yarn.resourcemanager.nodemanagers.heartbeat-interval-ms</code>。<br>后续的NodeManager–ResourceManager通信为ResourceManager提供有关现有容器状态的任何更新，包括由ApplicationMaster在节点上启动的新容器，已完成的容器等等。<br>此外，ResourceManager可能会通过此组件向NodeManager发出信号，使其可能杀死当前正在运行的容器，例如，由于调度策略会在运维人员显式停用或在发生网络问题时重新同步NodeManager等情况下关闭NodeManager。<br>最后，当任何应用程序在ResourceManager上完成时，ResourceManager会向NodeManager发出信号，以清理NodeManager上的各种特定于应用程序的实体（例如，内部的每个应用程序的数据结构和应用程序级别的本地资源），然后启动并完成每个应用程序的日志聚合，并记录到文件系统上。</p></li><li><p>Container Manager</p><p>该组件是NodeManager的核心。 它由以下子组件组成，每个子组件执行管理节点上运行的容器所需的功能子集：</p><ul><li><p>RPC Server</p><p>ContainerManager接受来自AM的请求以启动新容器或停止运行它们。它与NMToken SecretManager和<br>ContainerToken SecretManager一起使用以对所有请求进行身份验证和授权。</p></li><li><p>Resource Localization Service</p><p>它负责安全地下载和组织容器所需的各种文件资源。它会尽力在所有可用磁盘上分发文件。它还对下载的文件强制执行访问控制限制，并对它们施加适当的使用限制。</p></li><li><p>Auxiliary Services</p><p>NodeManager提供了一个通过配置这些服务来扩展其功能的框架。此功能允许特定（计算）框架（例如Spark等）可能需要的以节点为单位的自定义服务，但将它们放置在与NodeManager其余部分分开的本地“沙盒”中。<br>必须在NodeManager启动之前配置这些服务。当应用程序的第一个容器在节点上启动时，每当容器启动或完成时，以及最终在应用程序被视为完成时，都会通知辅助服务。</p></li><li><p>Container Launcher</p><p>Containers Launcher维护<em>一个线程池，以尽快准备和启动容器</em>。当ResourceManager通过NodeStatusUpdater发送此类请求或ApplicationMaster通过RPC服务器发送请求时，它还会清理容器进程。</p></li><li><p>Container Monitor</p><p>ContainersMonitor持续监视每个容器的使用情况。如果容器超出其分配范围，则此组件会发信号通知容器被杀死。进行此检查是为了防止任何失控的容器对在同一节点上运行的其他行为良好的容器产生不利影响。</p></li><li><p>Log Handler</p><p>使用YARN，可将属于单个应用程序且在给定NodeManager上运行的所有容器的日志汇总并写到指定文件系统中配置位置的单个（可能是压缩的）日志文件中。在当前的实现中，一个应用程序完成后，将拥有一个应用程序级别的日志目录和一个每个节点的日志文件，该文件由在该节点上运行的应用程序所有容器的日志组成。LogAggregationService实现LogHandler。</p></li><li><p>Container Executor</p><p>这本身就是一个话题。简而言之，此组件与基础操作系统进行交互，以安全地放置容器所需的文件和目录，并随后以安全的方式启动和清理与容器相对应的进程。</p></li></ul></li><li><p>Node Health Checker Service</p><p>NodeHealthCheckerService通过频繁运行配置的脚本来检查节点的运行状况。 它还经常通过在磁盘上创建临时文件来监视磁盘的运行状况。<br>系统运行状况的任何更改都将发送到NodeStatusUpdater（如前所述），后者将信息传递给ResourceManager。</p></li><li><p>Web Server</p><p>此组件提供以下信息：应用程序列表、在给定时间点在节点上正在运行的容器、与节点运行状况相关的信息以及容器产生的日志。</p></li><li><p>Deletion Service</p><p>在NodeManager内运行并根据指示删除本地路径的服务。</p></li></ul><h3 id="YARN-应用程序的声明周期"><a href="#YARN-应用程序的声明周期" class="headerlink" title="YARN 应用程序的声明周期"></a>YARN 应用程序的声明周期</h3><p><img src="https://i.ibb.co/cN6LcpL/lifecycle-Yarn-App1.png" alt="生命周期1"><br><img src="https://i.ibb.co/BCvdhkY/lifecycle-Yarn-App2.png" alt="生命周期2"><br><img src="https://i.ibb.co/svv95j0/lifecycle-Yarn-App3.png" alt="生命周期3"></p><p>这些图详细介绍了应用程序生命周期，尤其是应用程序的不同状态类型，应用程序任务尝试，容器状态等。</p><p>建议在控制台和NodeManager进程上启用DEBUG日志，以更好地了解上述事件的顺序。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要参考：</p><ul><li><p><strong>极客时间专栏 - <a href="https://time.geekbang.org/column/intro/133" target="_blank" rel="noopener">李智慧 - 从0开始学大数据</a></strong></p></li><li><p>Cloudera培训资料</p></li><li><p>Cloudera Blog:</p><ul><li><a href="https://blog.cloudera.com/untangling-apache-hadoop-yarn-part-1-cluster-and-yarn-basics/" target="_blank" rel="noopener">Cluster and YARN Basics - Cloudera Blog</a></li></ul></li></ul><p>YARN是一个集群资源调度框架，对于大数据应用程序开发者来说，虽然几乎不需要自定义YARN本身的程序，但是由于计算框架需要实现Application Master，以及我们要使用这些计算框架集成的YARN模块，所以对YARN的实现原理有所了解是很有帮助的。</p><p>另外，YARN对于管理员来说，还需要管理员熟悉如何查看日志、如何配置队列、用户ACL等操作，这些实操性比较强的工作，最好结合管理套件一齐操作，这也是Cloudera产品的核心价值所在。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> YARN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ClouderaManager Setup Kerberos Missing Credentials</title>
      <link href="2020/10/19/clouderamanager-setup-kerberos-missing-credentials/"/>
      <url>2020/10/19/clouderamanager-setup-kerberos-missing-credentials/</url>
      
        <content type="html"><![CDATA[<h1 id="记一次Cloudera-Manager关于Kerberos的credentials-missing问题"><a href="#记一次Cloudera-Manager关于Kerberos的credentials-missing问题" class="headerlink" title="记一次Cloudera Manager关于Kerberos的credentials missing问题"></a>记一次Cloudera Manager关于Kerberos的credentials missing问题</h1><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>我在Linux服务器上安装了MIT Kerberos server，然后在Cloudera Manager上通过Web UI向导进行启用Kerberos的初始化。<br>过程执行失败了，刷新CLoudera Manager的首页后发现许多Configuration Issues，提示说各个服务的Kerberos Credentials是missing的状态。</p><p>然而通过Web UI上的Generate Missing Credentials并不能顺利生成Credential文件。<br>查看log后发现是后台执行脚本时所需要的<code>keytab</code>不存在，这个<code>keytab</code>是给CDH Hadoop各个组件使用的。<br>生成这些<code>keytab</code>则是需要由拥有admin权限的Kerberos user（通过<code>kadmin -q &quot;addprinc...&quot;</code>命令生成）来生成。</p><p>下面记录一下排查错误的步骤。</p><h2 id="搜索和错误相关的日志"><a href="#搜索和错误相关的日志" class="headerlink" title="搜索和错误相关的日志"></a>搜索和错误相关的日志</h2><p>进入Cloudera Manager（该环境版本为V6.2.1），依次进入Diagnostic –&gt; Logs。</p><p><img src="https://i.ibb.co/KXr6NR9/Cloudera-Manager-Logs.png" alt="ClouderaManager Web-UI Logs"></p><p>由于是报错说找不到credentials，所以直缩小定时间范围，指定关键字搜索。</p><p><img src="https://i.ibb.co/VWpPF2N/no-keytab-error.png" alt="Cloudera Kerberos No keytab Error"></p><p>可以看到是Cloudera Manager的server执行此脚本 –<br><code>/opt/cloudera/cm/bin/gen_credentials.sh</code> 失败导致生成不了各个Hadoop service所需的Kerberos的credential。</p><p>而脚本出错的地方在于没有成功生成keytab文件。</p><h2 id="调查脚本出错的原因"><a href="#调查脚本出错的原因" class="headerlink" title="调查脚本出错的原因"></a>调查脚本出错的原因</h2><p>于是到Cloudera Manager的server上找此脚本看看。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cat</span> /opt/cloudera/cm/bin/gen_credentials.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#!/usr/bin/env bash</span><span class="token comment" spellcheck="true"># Copyright (c) 2011 Cloudera, Inc. All rights reserved.</span><span class="token keyword">set</span> -e<span class="token keyword">set</span> -x<span class="token comment" spellcheck="true"># Explicitly add RHEL5/6, SLES11/12 locations to path</span><span class="token function">export</span> PATH<span class="token operator">=</span>/usr/kerberos/bin:/usr/kerberos/sbin:/usr/lib/mit/sbin:/usr/sbin:/usr/lib/mit/bin:/usr/bin:<span class="token variable">$PATH</span>CMF_REALM<span class="token operator">=</span>$<span class="token punctuation">{</span>CMF_PRINCIPAL<span class="token comment" spellcheck="true">##*\@}</span>KEYTAB_OUT<span class="token operator">=</span><span class="token variable">$1</span>PRINC<span class="token operator">=</span><span class="token variable">$2</span>MAX_RENEW_LIFE<span class="token operator">=</span><span class="token variable">$3</span>KADMIN<span class="token operator">=</span><span class="token string">"kadmin -k -t <span class="token variable">$CMF_KEYTAB_FILE</span> -p <span class="token variable">$CMF_PRINCIPAL</span> -r <span class="token variable">$CMF_REALM</span>"</span>RENEW_ARG<span class="token operator">=</span><span class="token string">""</span><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$MAX_RENEW_LIFE</span> -gt 0 <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  RENEW_ARG<span class="token operator">=</span><span class="token string">"-maxrenewlife \"<span class="token variable">$MAX_RENEW_LIFE</span> sec\""</span><span class="token keyword">fi</span><span class="token keyword">if</span> <span class="token punctuation">[</span> -z <span class="token string">"<span class="token variable">$KRB5_CONFIG</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  <span class="token keyword">echo</span> <span class="token string">"Using system default krb5.conf path."</span><span class="token keyword">else</span>  <span class="token keyword">echo</span> <span class="token string">"Using custom config path '<span class="token variable">$KRB5_CONFIG</span>', contents below:"</span>  <span class="token function">cat</span> <span class="token variable">$KRB5_CONFIG</span><span class="token keyword">fi</span><span class="token variable">$KADMIN</span> -q <span class="token string">"addprinc <span class="token variable">$RENEW_ARG</span> -randkey <span class="token variable">$PRINC</span>"</span><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$MAX_RENEW_LIFE</span> -gt 0 <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  RENEW_LIFETIME<span class="token operator">=</span>`<span class="token variable">$KADMIN</span> -q <span class="token string">"getprinc -terse <span class="token variable">$PRINC</span>"</span> <span class="token operator">|</span> <span class="token function">tail</span> -1 <span class="token operator">|</span> <span class="token function">cut</span> -f 12`  <span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$RENEW_LIFETIME</span> -eq 0 <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"Unable to set maxrenewlife"</span>    <span class="token keyword">exit</span> 1  <span class="token keyword">fi</span><span class="token keyword">fi</span><span class="token variable">$KADMIN</span> -q <span class="token string">"xst -k <span class="token variable">$KEYTAB_OUT</span> <span class="token variable">$PRINC</span>"</span><span class="token function">chmod</span> 600 <span class="token variable">$KEYTAB_OUT</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从上面的脚本可以看出，生成各个Hadoop service用的Kerberros的keytab，需要用到<code>CMF_KEYTAB_FILE</code>，<br>而这个文件是根据我在Cloudera Manager的WebUI上输入的<strong>Kerberos Account Manager Credentials</strong>生成的。</p><p>我试着在Cloudera Manager的Web-UI上重新输入了一次在kdc server上手动创建的kdc admin账号信息，可是还是不行。</p><p>Web-UI上的入口：<strong>Cloudera Manager首页 –&gt; 最上方的Administration –&gt; Security –&gt;<br>Kerberos Credentials –&gt; Import Kerberos Account Manager Credentials</strong>。</p><p>可以确定的是kdc admin的账号信息没有错误。</p><p>于是对比了一下kdc server的配置文件和正常的kdc server的配置文件，发现缺少了logging部分。于是把logging部分补充上。</p><p>以下内容在kdc server的服务器上执行：</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">cat</span> /var/kerberos/krb5kdc/kdc.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-plain-text"><code class="language-plain-text">[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] SHOUNENG.COM = {  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab # supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal   supported_enctypes = des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal des-cbc-crc:v4 des-cbc-crc:afs3  max_renewable_life = 30m  master_key_type = des3-hmac-sha1  acl_file = /var/kerberos/krb5kdc/kadm5.acl  dict_file = /usr/share/dict/words  max_life = 30d  max_renewable_life = 31d  #removed supported_enctypes aes256-cts:normal and aes128-cts:normal  supported_enctypes = des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal des-cbc-crc:v4 des-cbc-crc:afs3 }# 以下logging部分为补充的内容[logging]admin_server = FILE:/var/log/kdc_admin.logkdc = FILE:/var/log/kdc.log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>打开了logging之后，重新尝试创建Hadoop的service的credential，并观察kdc server的日志，发现如下错误：</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">sed</span> -n <span class="token string">'1,50p'</span> /var/log/kdc_admin.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-plain-text"><code class="language-plain-text">Oct 15 23:41:13 host-10-17-100-90 kadmind[19124](info): setting up network...kadmind: setsockopt(10,IPV6_V6ONLY,1) workedkadmind: setsockopt(12,IPV6_V6ONLY,1) workedkadmind: setsockopt(14,IPV6_V6ONLY,1) workedOct 15 23:41:13 host-10-17-100-90 kadmind[19124](info): set up 6 socketsOct 15 23:41:13 host-10-17-100-90 kadmind[19124](Error): /var/kerberos/krb5kdc/kadm5.acl: syntax error at line 1 <*/admin@SHOUNENG.COM*...>Oct 15 23:41:13 host-10-17-100-90 kadmind[19125](info): Seeding random number generatorOct 15 23:41:13 host-10-17-100-90 kadmind[19125](info): startingOct 15 23:42:24 host-10-17-100-90 kadmind[19125](Notice): Request: kadm5_init, root/admin@SHOUNENG.COM, success, client=root/admin@SHOUNENG.COM, service=kadmin/host-10-17-100-90.coe.cloudera.com@SHOUNENG.COM, addr=10.17.101.160, vers=4, flavor=6Oct 15 23:42:29 host-10-17-100-90 kadmind[19125](Notice): Unauthorized request: kadm5_get_principals, *, client=root/admin@SHOUNENG.COM, service=kadmin/host-10-17-100-90.coe.cloudera.com@SHOUNENG.COM, addr=10.17.101.160Oct 15 23:42:46 host-10-17-100-90 kadmind[19125](Notice): Unauthorized request: kadm5_get_principals, *, client=root/admin@SHOUNENG.COM, service=kadmin/host-10-17-100-90.coe.cloudera.com@SHOUNENG.COM, addr=10.17.101.160Oct 15 23:43:07 host-10-17-100-90 kadmind[19125](Notice): Unauthorized request: kadm5_get_policy, default, client=root/admin@SHOUNENG.COM, service=kadmin/host-10-17-100-90.coe.cloudera.com@SHOUNENG.COM, addr=10.17.101.160Oct 15 23:43:07 host-10-17-100-90 kadmind[19125](Notice): Unauthorized request: kadm5_create_principal, zyx1@ZYX.COM, client=root/admin@SHOUNENG.COM, service=kadmin/host-10-17-100-90.coe.cloudera.com@SHOUNENG.COM, addr=10.17.101.160<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我看到有个语法错误：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">kadmind[19124](Error): /var/kerberos/krb5kdc/kadm5.acl: syntax error at line 1 <*/admin@SHOUNENG.COM*...><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>于是对比正常的kdc server，发现<code>*/admin@SHOUNENG.COM*</code>应该改成<code>*/admin@SHOUNENG.COM *</code>（少了一个空格）。</p><p>修改之后重新启动kdc的service，再重新创建credentials就成功了。</p>]]></content>
      
      
      <categories>
          
          <category> 信息安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kerberos </tag>
            
            <tag> ClouderaManager </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce Computation Framework</title>
      <link href="2020/10/15/mapreduce-computation-framework/"/>
      <url>2020/10/15/mapreduce-computation-framework/</url>
      
        <content type="html"><![CDATA[<h1 id="MapReduce-计算框架概要"><a href="#MapReduce-计算框架概要" class="headerlink" title="MapReduce 计算框架概要"></a>MapReduce 计算框架概要</h1><p>上一期我们聊到 MapReduce 编程模型将大数据计算过程切分为 Map 和 Reduce 两个阶段。<br>先复习一下，在 Map 阶段为每个数据块分配一个 Map 计算任务，然后将所有 map 输出的 Key 进行合并。<br>相同的 Key 及其对应的 Value 发送给同一个 Reduce 任务去处理。通过这两个阶段，工程师只需要遵循 MapReduce 编程模型就可以开发出复杂的大数据计算程序。</p><p>我们已经对 MapReduce 有一个值观的认识并知道它的工作原理：将输入拆分成多份小输出并行处理。<br>现在是时候来鸟瞰一下 MapReduce 的整体系统架构和数据流的走向了。</p><p>在一个经典的Hadoop集群中，MapReduce 运行在 HDFS 和 YARN 之上。</p><p>HDFS 是一个<strong>分布式文件系统</strong>，它提供了将大数据文件以分散和高可用的形式存放在大规模节点的集群中的功能，详细的介绍可以参考我<a href="https://shouneng.website/2020/09/29/hdfs-architecture-and-explaination-of-design/">之前的文章</a>。</p><p>YARN 则是一个比较抽象的概念。我们姑且先将它理解为一组后台进程，目的是为了做两件事情：</p><ul><li>资源管理（CPU、内存等计算资源）</li><li>作业调度、监视</li></ul><p>这两件事情其实是 Hadoop 作为分布式系统来看的核心功能，我认为这也是管理基础设施（服务器的计算资源和进程）的分布式系统共同的核心目的。<br>说得通俗一点就是管理进程和分配资源的分布式系统。</p><h2 id="Hadoop-1-vs-Hadoop-2"><a href="#Hadoop-1-vs-Hadoop-2" class="headerlink" title="Hadoop 1 vs Hadoop 2"></a>Hadoop 1 vs Hadoop 2</h2><p>Hadoop 现在的最新大版本是 3.x，我们需要知道 1.x 和 2.x 之间在基础组件上是有重大区别的。</p><p>最主要的区别就是新增了 YARN 这个基础组件。我们应该知道，一切的中间层都是为了解耦。</p><blockquote><p>计算机科学中的每个问题都可以用一间接层解决<br>(All problems in computer science can be solved by another level of indirection.)</p><p>by: David John Wheeler - 他是世界上第一个计算机科学博士。</p></blockquote><p>YARN 的基本思想是将<strong>资源管理</strong>和<strong>作业调度/监视</strong>的功能拆分为单独的守护程序。<br>这里，我只简单地提及一下 Hadoop 1.x 和 2.x 的区别，之后在学习 YARN 的时候，再详细讲它的架构和工作方式。</p><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>首先，我们来看一下 MapReduce 相关的术语(glossary)。</p><ul><li><p><em>作业(job)</em></p><p>MapReduce作业是客户端要执行的工作单元：它由输入数据，MapReduce程序和配置信息组成。</p></li><li><p><em>任务(task)</em></p><p>有两种类型：map任务和reduce任务。这些任务是使用 YARN 来调度的，并在群集中的节点上运行。如果任务失败，它将自动重新安排为在其他节点上运行。</p></li></ul><blockquote><p>注意：Hadoop 1.x 没有 YARN，所以任务的调度是集成在 MapReduce 框架内的。<br>并且由于从 Hadoop 2.x 开始，除了支持 MapReduce，还有 Spark 这样的新的计算框架，为了将编程模型和调度引擎解耦，YARN 才应运而生。</p></blockquote><ul><li><p><em>输入数据分片(input split)</em></p><p>Hadoop将MapReduce作业的输入分为固定大小的片段，称为输入数据分片或分片。 Hadoop为每个分片创建一个map任务，该任务为分片中的每个记录运行用户定义（编写）的map函数。</p></li><li><p><em>记录(record)</em></p><p>在 MapReduce 中，有2个和输入数据相关的概念：ImputFormat和RecordReader。我们应该先从宏观上理解。<br>对于操作系统机而言，一切数据都是以字节为单位来存储的，通过各种编码来转换并应用于文本、音频、视频等数据格式。<br>HDFS实现了一套数据类型的输入规范，比如一行原始数据文本可能保存在多个数据块中，HDFS将封装好的数据类型接口提供给我们来使用。就像我们在Python中调用file()系统方法来调用OS的接口以打开一个本地文件一样。<br>常见的InputFormat有FileInputFormat，以及其子类TextInputFormat。<br>MapReduce 框架依赖 InputFormat 来处理输入的数据。<br>你可以类比成这个场景：当一个压缩文件太大了，你的单块磁盘装不下，你就需要将这个压缩文件拆分成多个分割文件。<br>比如：example.rar分割成example.part1.rar, example.part2.rar,…<br>我们在处理这些压缩文件的分割文件时，需要指定压缩格式，这个压缩格式就类似于MapReduce中的InputFormat。<br>通常，RecordReader会转换输入数据块提供的输入的面向字节的视图，并将面向记录的形式呈现给Mapper实现以进行处理。<br>在TextInputFormat前提下，一个记录就是经过RecordReader处理后的一行文本，以<code>\n</code>为分隔符。<br>值得注意的是，原始文本数据存储在HDFS中时，不一定是在同一个数据块中的，所以这是TextInputFormat实现的逻辑。</p></li><li><p><em>数据局部性优化(data locality optimization)</em></p><p>在一个HDFS节点上保存着数据，Hadoop会尽力在该节点上运行map任务，因为这样不会使用宝贵的集群网络带宽。</p></li><li><p><em>分区(partition)</em></p><p>这里的分区指map任务输出的中间产物，是在交给reducer消费前聚合的产物。请参考<strong>图例 2-4</strong>。</p></li><li><p><em>组合器函数(Combiner Functions)</em></p><p>这是用户自定义的，作用于map任务的输出，并且是在”Shuffle and Sort”的过程中被执行的，更准确地说，由于这个是为了减少跨界点的网络带宽使用量，所以是在shuffle的过程中，在map任务本地节点上使用本地的中间产出物执行的。</p></li></ul><h2 id="数据流向"><a href="#数据流向" class="headerlink" title="数据流向"></a>数据流向</h2><p>现在我们知道，MapReduce的map任务会读取节点上的数据块，根据数据块、map任务所在的节点位置，可以推导出以下3种分部情况，如图例2-2：</p><p><img src="https://i.ibb.co/1nQjGPB/mapreduce-input-block-topology.png" alt="MapReduce任务和数据的拓扑关系"></p><p><strong><em>图例 2-2. Data-local (a), rack-local (b), and off-rack (c) map tasks</em></strong></p><p>map任务的输出是MapReduce作业的中间产物，是给reduce任务使用的，reduce任务一完成就没用了，所以不会保存在HDFS中，只会保存在本地文件系统中。<br>如果一个reduce任务在消费map的中间产物过程中，map所在的节点失败了，那Hadoop会自动重新找一个节点再运行一次该map任务。</p><p>reduce任务不会用到本地文件系统，通常来说一个reduce任务的输入是所有mapper的输出。所以运行reduce任务通常肯定会消耗集群中网络带宽，但是也只会和普通的HDFS层面的管道写入操作消耗相同的带宽。</p><p>单个reduce任务的整个数据流向如图例 2-3：</p><p><img src="https://i.ibb.co/b3Q9Yxs/mapreduce-single-reduce-data-flow.png" alt="MapReduce单个reduce情况的数据流向"></p><p><strong><em>图例 2-3. MapReduce data flow with a single reduce task</em></strong></p><p><strong>reduce任务的数量不是靠输入数据的大小来决定的，而是有单独指定的方式。之后我会阐述如何选择reduce任务的数量。</strong></p><p>在有多个reducer的情况下，map任务会将它们的输出进行分区处理，每个分区将被一个reducer来消费。<br>在每个分区中可能有多个<code>&lt;Key, Value&gt;</code>，并且相同的key只会在同一个分区中出现。</p><p>一个有着多个reduce任务的数据流向如图例 2-4：</p><p><img src="https://i.ibb.co/xgcwP19/mapreduce-multiple-reduce-data-flow.png" alt="MapReduce多个reduce情况的数据流向"></p><p><strong><em>图例 2-4. MapReduce data flow with multiple reduce tasks</em></strong></p><p>在图中的map和reduce任务之间的生成中间产物的阶段，我们习惯称之为”Shuffle and Sort”，实际上比上面的2个图例要复杂很多，对这个过程的调优将会对作业的执行时间产生巨大的影响。<br>之后我也许会记录这个过程的细节。</p><p>最后，还有一种是只有0个reduce任务的情况。<br>这种情况就是你的业务需求不需要执行”shuffle”阶段的操作。这个情况的例子包括使用<code>NLineInputFormat</code>这个InputFormat时的用例。后续有机会我也会记录。<br>这种情况下，会消耗带宽的操作只有map任务输出到HDFS时（参考图例 2-5），产生的跨节点的操作(repliacation)。</p><h2 id="组合器函数-Combiner-Functions"><a href="#组合器函数-Combiner-Functions" class="headerlink" title="组合器函数(Combiner Functions)"></a>组合器函数(Combiner Functions)</h2><p>许多MapReduce作业受群集上可用带宽的限制，因此需要最大程度地减少在map和reduce任务之间传输的数据。<br>Hadoop允许用户指定要在map输出上运行的组合器函数，并且组合器函数的输出构成了reduce函数的输入。<br>因为合并器功能是一种优化，所以Hadoop不能保证它会为特定的map输出记录调用多少次（如果有的话）。<br>换句话说，调用组合器函数0，1或多次应从reducer产生相同的输出。</p><p><img src="https://i.ibb.co/0q7QKbP/mapreduce-only-map-data-flow.png" alt="MapReduce只有map的情况的数据流向"></p><p><strong><em>图例 2-5. MapReduce data flow with no reduce tasks</em></strong></p><p>组合器函数的约定限制了可以使用的函数的类型。最好用一个例子说明。<br>假设以<a href="https://shouneng.website/2020/10/06/mapreduce-programming-model/#toc-heading-9">最高温度</a>为例，通过两个map处理了1950年的读数（因为它们位于不同的输入数据分片中）。想象一下第一个map产生的输出：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">(1950, 0)(1950, 20)(1950, 10)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第二个产生：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">(1950, 25)(1950, 15)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>reduce函数将接受到所有的value形成的列表作为输入：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">(1950, [0, 20, 10, 25, 15])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">(1950, 25)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>因为25是列表中的最大值。我们可以使用组合器函数，就像reduce函数一样，为每个map输出找到最高温度。然后reduce被调用时会接受到的输入就像以下这样：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">(1950, [20, 25])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并将产生与以前相同的输出。更简洁地说，在这个案例下，我们可以按以下的方式解释对于温度value的函数调用：</p><pre class="line-numbers language-platin-text"><code class="language-platin-text">max(0, 20, 10, 25, 15) = max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并非所有函数都具有此属性。例如，如果我们计算一个平均气温，我们不能使用mean函数来作为我们的组合其函数，因为：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">mean(0, 20, 10, 25, 15) = 14<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>但是：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">mean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>合并器函数不能替代reduce函数。（就如上面的例子一样，有些业务必须处理来自不同的map输出的相同的key所对应的value）<br>但这可以帮助减少map和reduce之间的数据交换量，仅出于这个原因，始终值得考虑是否可以在MapReduce作业中使用合并器函数。</p><blockquote><p>类似上面的max函数一样，具有此属性的函数称为<strong><em>可交换(commutative)</em></strong>的和<strong><em>关联的(associative)</em></strong>。<br>有的时候也被称为<strong><em>分布式的</em></strong>，例如Jim Gray等人的<br>“Data Cube: A Relational Aggregation Operator Generalizing Group-By,<br>Cross-Tab, and Sub-Totals”，February1995.</p></blockquote><h2 id="指定一个组合器函数"><a href="#指定一个组合器函数" class="headerlink" title="指定一个组合器函数"></a>指定一个组合器函数</h2><p>我们回到上一篇文章中的<a href="https://shouneng.website/2020/10/06/mapreduce-programming-model/#toc-heading-9">Java MapReduce程序</a>中去，组合器函数是使用<strong>Reducer</strong>类来定义的，并且对于这个应用程序来说，它的实现和<strong>MaxTemperatureReducer</strong>中的reduce函数一样。<br>唯一的变化是我们需要在Job上设置组合器的类（如 示例 2-6）。</p><p><strong><em>示例 2-6. 在天气数据集中查找最高温的应用，使用组合器函数来提高效率</em></strong></p><hr><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MaxTemperatureWithCombiner</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>length <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      System<span class="token punctuation">.</span>err<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Usage: MaxTemperatureWithCombiner &lt;input path> "</span> <span class="token operator">+</span>          <span class="token string">"&lt;output path>"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    Job job <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Job</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>MaxTemperatureWithCombiner<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setJobName</span><span class="token punctuation">(</span><span class="token string">"Max temperature"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    FileInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>MaxTemperatureMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong><code>job.setCombinerClass(MaxTemperatureReducer.class);</code></strong></p><pre class="line-numbers language-java"><code class="language-java">    job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>MaxTemperatureReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>NOTE: 对比<a href="https://shouneng.website/2020/10/06/mapreduce-programming-model/#toc-heading-9">Java MapReduce程序</a>中的<strong><em>示例 2-5</em></strong>，其实只是多了<code>job.setCombinerClass(MaxTemperatureReducer.class);</code>这一行。</p></blockquote><h2 id="Hadoop流-streaming"><a href="#Hadoop流-streaming" class="headerlink" title="Hadoop流(streaming)"></a>Hadoop流(streaming)</h2><p>Hadoop提供了MapReduce的API，该API允许您编写使用Java以外的其他语言来编写map和reduce函数。</p><p>Hadoop Streaming使用Unix标准流作为Hadoop与您的程序之间的接口，因此您可以使用可以读取标准输入并写入标准输出的任何语言来编写MapReduce程序。</p><p>流自然是适合文本处理的。Map输入数据通过标准输入传递到map函数，该map函数逐行对其进行处理并将行写入标准输出。<br>Map的输出的每一行为&lt;Key, Value&gt;对，以单个制表符为分隔符。<br>reduce函数的输入采用相同的格式（制表符分隔的&lt;Key, Value&gt;对），通过标准输入传入。<br>reduce函数从标准输入读取行，框架保证以Key来对行进行排序，并将其结果写入标准输出。</p><p>让我们通过重写MapReduce程序来说明这一点，该程序可在Streaming中按年份查找最高温度。</p><h3 id="Ruby"><a href="#Ruby" class="headerlink" title="Ruby"></a>Ruby</h3><p>map函数可以用Ruby表示，如示例 2-7所示。</p><p><strong><em>示例 2-7. 用Ruby写的用来查找最高温的Map函数</em></strong></p><hr><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token comment" spellcheck="true">#!/usr/bin/env ruby</span><span class="token constant">STDIN</span><span class="token punctuation">.</span>each_line <span class="token keyword">do</span> <span class="token operator">|</span>line<span class="token operator">|</span>  val <span class="token operator">=</span> line  year<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> q <span class="token operator">=</span> val<span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> val<span class="token punctuation">[</span><span class="token number">87</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> val<span class="token punctuation">[</span><span class="token number">92</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>  puts <span class="token string">"<span class="token interpolation"><span class="token delimiter tag">#{</span>year<span class="token delimiter tag">}</span></span>\t<span class="token interpolation"><span class="token delimiter tag">#{</span>temp<span class="token delimiter tag">}</span></span>"</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>temp <span class="token operator">!=</span> <span class="token string">"+9999"</span> <span class="token operator">&amp;&amp;</span> q <span class="token operator">=</span><span class="token operator">~</span> <span class="token regex">/[01459]/</span><span class="token punctuation">)</span><span class="token keyword">end</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>该程序通过对来自STDIN（IO类型的全局常量）中的每一行执行一个块(block)来遍历标准输入中的行。<br>该块从每个输入行中提取相关字段，如果温度有效，则将以制表符<code>\ t</code>分隔的年份和温度写入标准输出（使用puts）。</p><p><strong>注意</strong></p><p><strong>值得一提的是Streaming与Java MapReduce API之间的设计差异。<br>Java API的map函数旨在一次处理一个记录。<br>MapReduce框架为输入中的每个记录调用Mapper（类）上的map()方法，而通过流式处理，map程序可以决定如何处理输入–例如，由于你可以控制标准输入的数据，因此一次可以轻松读取和处理多行。<br>用户的Java map实现是“推送”记录，但通过在Mapper的实例变量中累积前几行，仍然可以考虑一次累计多行。<br>在这种情况下，您需要实现close()方法，以便知道何时读取了最后一条记录，从而可以完成对最后一组行的处理。</strong></p><p>由于脚本仅在标准输入和输出上运行，因此无需使用Hadoop就可以轻松测试脚本，只需使用Unix管道即可：</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">cat</span> input/ncdc/sample.txt <span class="token operator">|</span> ch02-mr-intro/src/main/ruby/max_temperature_map.rb1950    +00001950    +00221950    -00111949    +01111949    +0078<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>示例2-8</strong>中显示的reduce函数稍微复杂一点。</p><p><strong><em>示例2-8 用Ruby来计算出最高温度的reduce函数</em></strong></p><hr><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token comment" spellcheck="true">#!/usr/bin/env ruby</span>last_key<span class="token punctuation">,</span> max_val <span class="token operator">=</span> <span class="token keyword">nil</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1000000</span><span class="token constant">STDIN</span><span class="token punctuation">.</span>each_line <span class="token keyword">do</span> <span class="token operator">|</span>line<span class="token operator">|</span>  key<span class="token punctuation">,</span> val <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span>  <span class="token keyword">if</span> last_key <span class="token operator">&amp;&amp;</span> last_key <span class="token operator">!=</span> key    puts <span class="token string">"<span class="token interpolation"><span class="token delimiter tag">#{</span>last_key<span class="token delimiter tag">}</span></span>\t<span class="token interpolation"><span class="token delimiter tag">#{</span>max_val<span class="token delimiter tag">}</span></span>"</span>    last_key<span class="token punctuation">,</span> max_val <span class="token operator">=</span> key<span class="token punctuation">,</span> val<span class="token punctuation">.</span>to_i  <span class="token keyword">else</span>    last_key<span class="token punctuation">,</span> max_val <span class="token operator">=</span> key<span class="token punctuation">,</span> <span class="token punctuation">[</span>max_val<span class="token punctuation">,</span> val<span class="token punctuation">.</span>to_i<span class="token punctuation">]</span><span class="token punctuation">.</span>max  <span class="token keyword">end</span><span class="token keyword">end</span>puts <span class="token string">"<span class="token interpolation"><span class="token delimiter tag">#{</span>last_key<span class="token delimiter tag">}</span></span>\t<span class="token interpolation"><span class="token delimiter tag">#{</span>max_val<span class="token delimiter tag">}</span></span>"</span> <span class="token keyword">if</span> last_key<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同样，程序在标准输入的行上进行迭代，但是这次我们在处理每个key group时必须存储一些状态。<br>在这种情况下，key是年份，并且我们存储最后看到的key和到目前为止看到的最高温度(value)。<br>MapReduce框架可确保key是有序的，因此我们知道，<strong>如果一个key与前一个key不同，我们已经到了一组新的<code>&lt;Key, Value&gt;</code>（在遍历过程中）</strong>。</p><blockquote><p>Note: 上面这段话用示意图来说明比较好。<br>假设在Ruby的map阶段，输出如下：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">1950    +00001949    +01111950    +00221950    -00111949    +00781948    +01281948    +0108<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>那么在经过Hadoop流传给Ruby的reduce时，输入是已经被排序了的：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">1948    +01281948    +01081949    +01111949    +00781950    +00001950    +00221950    -0011<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><p>与Java API相反，在Java API中为每个键组提供了迭代器，在Streaming中，您必须在程序中查找键组边界。<br>对于每一行，我们拉出键和值。<br>然后，如果我们刚刚完成了一组（last_key &amp;&amp; last_key！= key），则在开始处理新键的最高温度之前，我们就写出该键和该组的最高温度（由制表符分隔）。<br>如果我们还没有完成一组处理，那么只需更新当前键对应的最高温度即可。<br>这个程序的最后一行代码确保为输入中的最后一个键组输出处理完成的结果。</p><p>现在我们可以使用Unix管道（相当于<a href="https://i.ibb.co/6DLL6w7/hdp-strm-diag.png" target="_blank" rel="noopener">图例2-1</a>中所示的Unix管道）模拟整个MapReduce管道：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">%</span> cat input<span class="token operator">/</span>ncdc<span class="token operator">/</span>sample<span class="token punctuation">.</span>txt <span class="token operator">|</span> \  ch02<span class="token operator">-</span>mr<span class="token operator">-</span>intro<span class="token operator">/</span>src<span class="token operator">/</span>main<span class="token operator">/</span>ruby<span class="token operator">/</span>max_temperature_map<span class="token punctuation">.</span>rb <span class="token operator">|</span> \  sort <span class="token operator">|</span> ch02<span class="token operator">-</span>mr<span class="token operator">-</span>intro<span class="token operator">/</span>src<span class="token operator">/</span>main<span class="token operator">/</span>ruby<span class="token operator">/</span>max_temperature_reduce<span class="token punctuation">.</span>rb<span class="token number">1949</span>    <span class="token number">111</span><span class="token number">1950</span>    <span class="token number">22</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出与Java程序的输出相同，因此下一步是使用Hadoop本身运行它。</p><p>Hadoop命令行工具里并没有Streaming选项。取而代之的是，您可以通过jar选项来指定Streaming JAR文件来使用流功能。<br>Streaming程序的选项指定输入和输出路径以及map和reduce脚本。<br>它看起来像这样：</p><pre class="line-numbers language-bash"><code class="language-bash">% hadoop jar <span class="token variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-*.jar \  -input input/ncdc/sample.txt \  -output output \  -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \  -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在集群上的大型数据集上运行时，我们应使用<code>-combiner</code>选项设置组合器：</p><pre class="line-numbers language-bash"><code class="language-bash">% hadoop jar <span class="token variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-*.jar \  -files ch02-mr-intro/src/main/ruby/max_temperature_map.rb,\ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \  -input input/ncdc/all \  -output output \  -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \  -combiner ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \  -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>还请注意<code>-files</code>的使用，它会帮我们我们把本地脚本传送到集群上各个节点，以运行Streaming程序。</p><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>现在我们再来看一个Python版本的例子，因为我比较熟悉Python，所以看起来也比较亲切~</p><blockquote><p>Python社区有一个叫<a href="http://klbostee.github.io/dumbo/" target="_blank" rel="noopener">Dumbo</a>的第三方包，更加Pythonic以及更易于使用。</p></blockquote><p>map脚本在<strong>示例 2-9</strong>中，reduce脚本在<strong>示例 2-10中</strong>。</p><p><strong><em>示例 2-9. Python实现的计算最高温的map函数</em></strong></p><hr><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/env python</span><span class="token keyword">import</span> re<span class="token keyword">import</span> sys<span class="token keyword">for</span> line <span class="token keyword">in</span> sys<span class="token punctuation">.</span>stdin<span class="token punctuation">:</span>  val <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>year<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> q<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span>val<span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">,</span> val<span class="token punctuation">[</span><span class="token number">87</span><span class="token punctuation">:</span><span class="token number">92</span><span class="token punctuation">]</span><span class="token punctuation">,</span> val<span class="token punctuation">[</span><span class="token number">92</span><span class="token punctuation">:</span><span class="token number">93</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span>temp <span class="token operator">!=</span> <span class="token string">"+9999"</span> <span class="token operator">and</span> re<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">"[01459]"</span><span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">"%s\t%s"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>year<span class="token punctuation">,</span> temp<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong><em>示例 2-10。 Python实现的计算最高温的reduce函数</em></strong></p><hr><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/env python</span><span class="token keyword">import</span> sys<span class="token punctuation">(</span>last_key<span class="token punctuation">,</span> max_val<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token operator">-</span>sys<span class="token punctuation">.</span>maxint<span class="token punctuation">)</span><span class="token keyword">for</span> line <span class="token keyword">in</span> sys<span class="token punctuation">.</span>stdin<span class="token punctuation">:</span>  <span class="token punctuation">(</span>key<span class="token punctuation">,</span> val<span class="token punctuation">)</span> <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span>  <span class="token keyword">if</span> last_key <span class="token operator">and</span> last_key <span class="token operator">!=</span> key<span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">"%s\t%s"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>last_key<span class="token punctuation">,</span> max_val<span class="token punctuation">)</span>    <span class="token punctuation">(</span>last_key<span class="token punctuation">,</span> max_val<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> int<span class="token punctuation">(</span>val<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token punctuation">(</span>last_key<span class="token punctuation">,</span> max_val<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> max<span class="token punctuation">(</span>max_val<span class="token punctuation">,</span> int<span class="token punctuation">(</span>val<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> last_key<span class="token punctuation">:</span>  <span class="token keyword">print</span> <span class="token string">"%s\t%s"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>last_key<span class="token punctuation">,</span> max_val<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们可以像在Ruby中一样测试程序并运行作业。例如，运行测试：</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">cat</span> input/ncdc/sample.txt <span class="token operator">|</span> \  ch02-mr-intro/src/main/python/max_temperature_map.py <span class="token operator">|</span> \  <span class="token function">sort</span> <span class="token operator">|</span> ch02-mr-intro/src/main/python/max_temperature_reduce.py1949    1111950    22<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="学而思"><a href="#学而思" class="headerlink" title="学而思"></a>学而思</h2><h3 id="如何实现Java-MapReduce的WordCount"><a href="#如何实现Java-MapReduce的WordCount" class="headerlink" title="如何实现Java MapReduce的WordCount"></a>如何实现Java MapReduce的WordCount</h3><p>在上一篇记录MapReduce的编程模型的文章中，开头我记录了一个Python单机版本wordcount例子。<br>看完今天这篇文章的内容，我想您应该可以利用所学到的概念来推测出如果要用Java来实现WordCount的MapReduce程序，应该怎么写了。</p><p>正所谓学而不思则罔，虽然我已经完全不记得Java的语法以及特性，不过我们IT从业者应该具备抽象地思维能力，所以，大胆地基于现有的知识才推论吧！</p><p>首先我们应该想到，WordCount是要累计地计算出现的单词去重后，每个单词一共有多少个。<br>假设我们的原始数据是一个巨大的文本文件，并且是以<code>\n</code>分了行的。<br>那么map函数的输入应该是<code>&lt;Offset, LineText&gt;</code>，输出则是<code>&lt;Word, 1&gt;</code><br>，此处map函数只需要对LineText进行以下处理–用<code>空格</code>或者<code>\t</code>来分割LineText，每遇到一个Word就输出<code>&lt;Word, 1&gt;</code>，<br>所以有许多个数据块，每个数据块都会输出许多相同的<code>&lt;Word, 1&gt;</code>。</p><p>在shuffle过程中，应该将map输出的<code>&lt;Word, 1&gt;</code>合并成<code>&lt;Word, [1,1,...]&gt;</code>。<br>所以交给reduce的是来自所有数据块产出的<code>&lt;Word, [1,1,...]&gt;</code>，且Word是唯一的。</p><p>然后就交给reduce函数处理，reduce将来自map的<code>&lt;Word, [1,1,...]&gt;</code>处理，输出为<code>&lt;Word, Count&gt;</code>。</p><p>我画个图来说明，会更加直观，照葫芦画瓢就行啦~</p><p><img src="https://i.ibb.co/4ZYSpNt/java-mapreduce-wordcout.png" alt="Java实现WordCount的MapReduce程序示意图"></p><h3 id="延申的思考"><a href="#延申的思考" class="headerlink" title="延申的思考"></a>延申的思考</h3><ol><li><p>公有云上的Hadoop集群为什么使用对象存储？</p><p>通过学习HDFS和MapReduce，我们知道了为了加快作业的处理速度，我们应该尽可能的将map任务调度到拥有所需数据块的节点上，<br>也就是说Hadoop的作业调度系统应该是有机架感知能力(Rack Awareness)的。<br>那么现在在公有云上搭建Hadoop集群时，我们的数据是否还存放在本地磁盘呢？显然不是的，现在的公有云Hadoop产品，默认的存储都倾向于使用对象存储，这就要消耗集群内的网络带宽。<br>在公有云内，是否使用对象存储服务的性价比已经远超于使用本地磁盘呢？<br>要知道现在我们创建云主机依然可以选择使用本地磁盘，而不使用默认的云磁盘。</p></li><li><p>更加复杂的编程实现</p><p>为了加快作业的速度，我们是否可以在多个节点上同时对一个数据块的多个副本进行map任务？<br>比如<strong>block 0</strong>有3个副本，分别存放在3个节点上，对于<strong>block 0</strong>，节点1-3上执行的map任务分别从不同的偏移位置开始处理数据，等等的复杂一点的实现。<br>是否在MapReduce框架中已经存在对应的接口。</p></li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>MapReduce既是编程模型又是计算框架，尽管对于我们来说只需要实现map和reduce函数就能完成业务，但是前提是要对MapReduce编程框架的内部机制有清晰的概念。<br>总体来说，MapReduce最神奇的地方在于shuffle和sort，这是框架可以帮我们完成的。<br>反过来说，就是如何实现一个分布式的任务调度系统，并且要基于并行处理数据的编程模型来设计。</p><p>本文主要参考：</p><ul><li><strong>极客时间专栏 - <a href="https://time.geekbang.org/column/intro/133" target="_blank" rel="noopener">李智慧 - 从0开始学大数据</a></strong></li><li><strong><a href="https://learning.oreilly.com/library/view/hadoop-the-definitive/9781491901687/" target="_blank" rel="noopener">Hadoop: The Definitive Guide, 4th Edition - Chapter2 - MapReduce</a></strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce Programming Model</title>
      <link href="2020/10/06/mapreduce-programming-model/"/>
      <url>2020/10/06/mapreduce-programming-model/</url>
      
        <content type="html"><![CDATA[<h1 id="MapReduce-Programming-Model"><a href="#MapReduce-Programming-Model" class="headerlink" title="MapReduce Programming Model"></a>MapReduce Programming Model</h1><p>这是我的 <code>Hadoop</code> 学习之旅的第二站，之前我分享了关于 <a href="https://shouneng.website/2020/09/29/hdfs-architecture-and-explaination-of-design/"><code>HDFS</code> 的架构和设计</a>。<br>现在要来看一下如何利用 <code>HDFS</code> 来实现大数据业务程序。</p><p>在 <code>Hadoop</code> 问世之前，分布式计算也是存在的，但是没有通用的解决方案，只能专门处理某一类计算。<br>这大概是跟<strong>不使用任何Web框架和系统库来开发Web程序</strong>的感觉差不多。</p><p><strong><em><code>MapReduce</code> 既是一个编程模型，又是一个计算框架。</em></strong></p><p>作为编程模型来说，<code>MapReduce</code> 并不是一个“神奇”的东西。但是，作为<strong>大数据计算框架</strong>来说，它的出现使得开发大数据应用的门槛降低了许多，就跟Web框架的应用一个道理。</p><p><strong>大数据计算框架解决的是什么呢？</strong></p><blockquote><p>简单来说，就是：</p><ul><li>如何基于分布式存储分配计算资源</li><li>如何调度计算任务</li></ul></blockquote><p>今天这篇文章先介绍一下作为编程模型的 <code>MapReduce</code>。计算框架的介绍放在下一篇。</p><h2 id="MarReduce-编程模型"><a href="#MarReduce-编程模型" class="headerlink" title="MarReduce - 编程模型"></a>MarReduce - 编程模型</h2><p>由于我们的数据是存放在分布式文件系统中，自然不能用传统的编程模型来完成任务了。<br>接下来我用3个代码例子来解释传统编程模型和分布式模型的区别，以经典的<strong>WordCount</strong>程序为例。</p><h3 id="Example-1-Python语言单机版WordCount"><a href="#Example-1-Python语言单机版WordCount" class="headerlink" title="Example 1: Python语言单机版WordCount"></a>Example 1: Python语言单机版WordCount</h3><p>我们可以创建一个 <code>Hash Table</code>，然后遍历文本中的每一个单词。<br>如果在 <code>Hash Table</code> 中存在，就将 key（单词）的 value <code>+1</code>；<br>否则将单词作为 key 添加到 <code>Hash Table</code> 中。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># %%</span><span class="token comment" spellcheck="true"># 文本前期处理</span>strlList <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Hello WorldBye WorldHello WorldBye World"""</span>strlList <span class="token operator">=</span> strlList<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span>countDict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># %%</span><span class="token comment" spellcheck="true"># 如果字典里有该单词则加1，否则添加入字典</span><span class="token keyword">for</span> w <span class="token keyword">in</span> strlList<span class="token punctuation">:</span>    <span class="token keyword">if</span> w <span class="token keyword">in</span> countDict<span class="token punctuation">:</span>        countDict<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        countDict<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token comment" spellcheck="true"># %%</span><span class="token keyword">print</span><span class="token punctuation">(</span>countDict<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># {'hello': 2, 'world': 4, 'bye': 2}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果文本很长，我们担心不能把文本一次性加载到内存中，也可以使用 <code>Generator</code> 特性来完成迭代操作。但是这样将会花费很长的<strong>时间</strong>。这是第一个问题。<br>我们自然会想到可以通过多进程的方式来增加处理速度，对文本设置多个偏移量，并行地处理。</p><p>但是数据量大到一个机器无法装得下的情况下，就要考虑如何将数据分散地存放到一个集群中了。有一个经典的比喻：</p><blockquote><p>如果有一块巨大的石头拉不动，你不会找一头巨型牛来拉，而会去找一群牛来一齐拉。</p></blockquote><p>那么如何将数据<strong>分散地</strong>存放在一个集群中，并尽可能快地处理这些数据呢，这是第二个问题。</p><p>试想一下，如果让你来设计一个并行处理分散在多个服务器上的数据的系统，你会考虑哪些问题？<br>明显地，这需要结合数据源来设计，因为数据源(HDFS)是采用分布式设计的，所以像上面这样的单机版逻辑就不满足需求了。</p><h3 id="Example-2-Shell脚本的MapReduce"><a href="#Example-2-Shell脚本的MapReduce" class="headerlink" title="Example 2: Shell脚本的MapReduce"></a>Example 2: Shell脚本的MapReduce</h3><p>现在我们来看一个复杂的，不用大数据框架来完成的MapReduce模型的计算任务。这是一个基于 Shell 脚本实现的程序。</p><p>这个示例来自这本书 - <strong><em>Hadoop: The Definitive Guide, 4th Edition</em></strong></p><p>以下的示例，我们编写一个挖掘天气数据的程序。气象传感器每小时在全球许多地方收集数据，并收集大量的日志数据，这是使用MapReduce进行分析的理想选择，因为我们要处理所有数据，并且数据是半结构化(semi-structured)且记录导向的(record-oriented)。</p><h4 id="资料格式"><a href="#资料格式" class="headerlink" title="资料格式"></a>资料格式</h4><p>我们将使用的数据来自<a href="http://www.ncdc.noaa.gov/" target="_blank" rel="noopener">国家气候数据中心</a>(National Climatic Data Center)(NCDC)。<br>数据是使用行导向的(line-oriented)ASCII格式存储的，其中每一行都是一条记录。该格式包含一组丰富的气象元素,其中许多是<strong>可选的</strong>或者具有可变的数据长度。<br>为简单起见，我们关注一些始终存在且宽度固定的基本元素，例如<strong>温度</strong>。</p><p><strong>示例2-1</strong>显示了一个示例行，其中标注了一些显著的字段。该行已经被分为多行以显示每个字段；在实际的文件中，这些字典打包在同一行，没有分行。</p><p><strong><em>示例2-1 NCDC记录的格式</em></strong></p><hr><pre class="line-numbers language-plain-text"><code class="language-plain-text">0057332130   # USAF weather station identifier99999    # WBAN weather station identifier19500101 # observation date0300     # observation time4+51317   # latitude (degrees x 1000)+028783  # longitude (degrees x 1000)FM-12+0171    # elevation (meters)99999V020320      # wind direction (degrees)1        # quality codeN0072100450    # sky ceiling height (meters)1        # quality codeCN010000   # visibility distance (meters)1        # quality codeN9-0128    # air temperature (degrees Celsius x 10)1        # quality code-0139    # dew point temperature (degrees Celsius x 10)1        # quality code10268    # atmospheric pressure (hectopascals x 10)1        # quality code<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><p>NOTE: 在一个文件中应该保存着多行这样的数据，我们可以看到这样一行数据中有气象站的代号、日期、时间、温度等重要信息。这一行信息的时间单位可能是一天，也可能是几个小时。</p></blockquote><p>数据文件按日期和气象站来组织。从1901年到2001年，每年都有一个目录，每个目录都包含每个气象站的压缩文件以及该年的读书。例如，以下是1990年的第一个条目：</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">ls</span> raw/1990 <span class="token operator">|</span> <span class="token function">head</span>010010-99999-1990.gz010014-99999-1990.gz010015-99999-1990.gz010016-99999-1990.gz010017-99999-1990.gz010030-99999-1990.gz010040-99999-1990.gz010080-99999-1990.gz010100-99999-1990.gz010150-99999-1990.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>NOTE: 例如 <code>010010-99999-1990.gz</code> 这个文件中应该是包含一个文件，该文件中包含多条<strong>示例2-1</strong>那样的数据。<br><strong>99999</strong> 是某个气象站的编号。<br>由上面的例子推测，1990年，编号为<strong>99999</strong>的气象站可能产生很多个上面那样的<code>gz</code>压缩文件，其中每个<code>gz</code>的内容为多条<strong>示例2-1</strong>的数据。</p></blockquote><p>有数万个气象站，因此整个数据集由大量的相对小的文件组成。通常来说，处理较少数量的相对较大的文件更容易，更高效，因此我们先对数据做了预处理，以便将每年的读数合并为一个文件。</p><h4 id="资料预处理"><a href="#资料预处理" class="headerlink" title="资料预处理"></a>资料预处理</h4><p>由于上述的原因，我们需要先对这些大量的小文件先进行预处理，我们想将一年的数据压缩成一个文件。<br>我们使用一个剧有<code>map</code>函数的程序来做这件事，不需要<code>reduce</code>函数，因为我们不需要并行地合并，<code>map</code>函数可以并行执行所有文件处理。</p><p>这段处理过程已经使用了Hadoop的MapReduce框架，所以在此就省略了，感兴趣的可以点击这里查看这个程序的源码：<br><a href="https://gist.github.com/aruruka/11715d8ef121f1473c6540f713261758" target="_blank" rel="noopener">MapReduce - streming接口示例</a></p><p>处理完成后的文件列表是这样的：</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">ls</span> -1 all/1901.tar.bz21902.tar.bz21903.tar.bz2<span class="token punctuation">..</span>.2000.tar.bz2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个<code>tar</code>文件都包含所有气象站的当年的读数的文件，并使用<code>gzip</code>压缩。（也就是说<code>1901.tar.bz2</code>中保存的还是压缩文件，所以这个<code>1901.tar.bz2</code>本身的<code>bzip2</code>压缩是多余的）</p><pre class="line-numbers language-bash"><code class="language-bash">% <span class="token function">tar</span> jxf 1901.tar.bz2% <span class="token function">ls</span> 1901 <span class="token operator">|</span> <span class="token function">head</span>029070-99999-1901.gz029500-99999-1901.gz029600-99999-1901.gz029720-99999-1901.gz029810-99999-1901.gz227070-99999-1901.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="使用Unix工具分析数据"><a href="#使用Unix工具分析数据" class="headerlink" title="使用Unix工具分析数据"></a>使用Unix工具分析数据</h4><p><strong>现在我们的需求是计算每一年中全球的最高气温。</strong></p><p>看看用 shell 脚本要如何处理：</p><p>我们使用处理行导向的经典工具 - <code>awk</code>。<br>参照<strong>示例2-2</strong></p><p><strong><em>示例2-2 一个从NCDC气象记录中逐年查找最高记录温度的程序</em></strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#!/usr/bin/env bash</span><span class="token keyword">for</span> year <span class="token keyword">in</span> all/*<span class="token keyword">do</span>  <span class="token keyword">echo</span> -ne <span class="token variable"><span class="token variable">`</span><span class="token function">basename</span> $year .gz<span class="token variable">`</span></span><span class="token string">"\t"</span>  gunzip -c <span class="token variable">$year</span> <span class="token operator">|</span> \    <span class="token function">awk</span> <span class="token string">'{ temp = substr(<span class="token variable">$0</span>, 88, 5) + 0;           q = substr(<span class="token variable">$0</span>, 93, 1);           if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp > max) max = temp }         END { print max }'</span><span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>该脚本循环遍历压缩的<code>year</code>文件，首先打印年份，然后使用<code>awk</code>处理每个文件。<br><code>awk</code>脚本从数据提取两个字段：<strong>空气温度</strong>(air temperature)和<strong>质量代码</strong>(quality code)。<br>空气温度值通过加0变成整数。<br>接下来，进行测试以查看温度是否有效（9999这个值表示NCDC数据集中的缺失值）以及质量代码表示读数是可信的还是错误的。<br>如果读数正常，则将该值与到目前为止看到的最大值进行比较，如果找到新的最大值，则将更新该最大值。<br><code>END</code>代码块将在<code>awk</code>处理完所有行后被执行，并输出最大值。</p><p>脚本运行之后看起来像这样：</p><pre class="line-numbers language-bash"><code class="language-bash">% ./max_temperature.sh1901 3171902 2441903 2891904 2561905 283<span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>源文件中的温度值放大了10倍，因此得出1901年的最高温度为31.7°C（本世纪初读数很少，所以这是合理的）。<br>计算这一个世纪每年的最高温，这个程序完全运行在一台<strong>EC2 High-CPU Extra Large</strong>实例上，一次运行了42分钟。</p><p>为了加快处理速度，我们需要并行运行程序的各个部分。从理论上讲，这很简单：我们可以使用机器上所有可用的硬件线程，在不同的过程中处理不同的年份。但是，这有一些问题。</p><p>首先，将<strong>工作</strong>分成相等大小的部分并不总是那么容易。在这种情况下，不同年份的文件大小会有很大差异，因此某些进程将比其他进程更早完成。即使他们接手进一步的工作，整个运行仍以最长的文件为主。尽管需要更多工作，但是更好的方法是将输入拆分为固定大小的块，并将每个块分配给一个进程。</p><blockquote><p>NOTE: 抽象地考虑，这里的<strong>工作</strong>有<strong>计算</strong>和<strong>存储</strong>两层含义。所以大数据处理在收集原始数据的时候就要和传统编程方式下不一样了。</p></blockquote><p>其次，将独立进程的结果合并可能需要进一步处理。在这种情况下，每年的结果与其他年份无关，可以通过合并所有结果并按年份排序来合并它们。如果使用固定大小的块方法，则组合会更加精细。对于此示例，特定年份的数据通常会分为几个块，每个块独立处理。我们将得出每个块的最高温度，因此最后一步是寻找每年这些最高温度中的最高温度。</p><blockquote><p>在资料预处理的阶段我们使用了Hadoop的Map功能，上述的shell脚本则是相当于一个Reduce功能的程序。</p></blockquote><p>第三，您仍然受到单台计算机处理能力的限制。如果您现有的处理器数量可以达到的最佳时间是20分钟，那就是这样了。您无法使其运行更快。此外，某些数据集超出了单台计算机的容量。当我们开始使用多台机器时，就会需要考虑许多其他因素了，主要是协调性和可靠性的范畴。谁负责整体工作？我们如何处理失败的进程？</p><p>因此，尽管并行处理是可行的，但在实践中却很棘手。使用像<code>Hadoop</code>这样的框架来解决这些问题是一个很好的选择。</p><blockquote><p>事实上，您会发现不得不开发一个像样的分布式系统来做这样的事。</p></blockquote><h3 id="Example-3-Java语言使用Hadoop的MapReduce"><a href="#Example-3-Java语言使用Hadoop的MapReduce" class="headerlink" title="Example 3: Java语言使用Hadoop的MapReduce"></a>Example 3: Java语言使用Hadoop的MapReduce</h3><p><code>Hadoop</code>在并行处理上有优势，要使用<code>Hadoop</code>，我们需要将查询(query)表示为<code>MapReduce</code>作业(job)。经过一些本地的小规模测试之后，我们将能够在一组机器上运行它。</p><h4 id="Map-and-Reduce"><a href="#Map-and-Reduce" class="headerlink" title="Map and Reduce"></a>Map and Reduce</h4><p><code>MapReduce</code>通过将处理分为两个阶段进行工作：Map阶段和Reduce阶段。<br>每个阶段都有键值对作为输入和输出，程序员可以选择其类型。程序员还需要指定用于Map和Reduce的函数 - map函数和reduce函数。</p><p>Map阶段的输入是原始NCDC数据。输入的数据是一个键值对(key-value pair)。<br>我们选择一种文本输入格式，该格式将数据集<br>(dataset)中的每一行作为文本类型，并作为值(value)处理。<br>键(key)是文件的开头到该行的开头的<strong>偏移量</strong>。</p><blockquote><p>NOTE: 文件开头的偏移量是<strong>0</strong>；第1行有<strong>120</strong>个字符，那么第二行的偏移量则是<strong>121</strong>；以此类推。</p></blockquote><p>我们的map函数很简单。我们提取年份和气温，因为这是我们唯一感兴趣的字段。在这种情况下，map函数只是数据准备阶段，以便reduce函数可以执行以下操作：<br>找出每年的最高温度。<br>map函数也是删除不良记录的好方法：在这里，我们可以过滤掉缺失的、可疑的或错误的温度。</p><p>为了可视化map的工作方式，请考虑以下输入数据提示示例（已经删除一些未使用的列以适配页面，用省略号表示）：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">0067011990999991950051507004...9999999N9+00001+99999999999...0043011990999991950051512004...9999999N9+00221+99999999999...0043011990999991950051518004...9999999N9-00111+99999999999...0043012650999991949032412004...0500001N9+01111+99999999999...0043012650999991949032418004...0500001N9+00781+99999999999...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行以键值对的形式呈现给map函数：</p><blockquote><p>(0, 006701199099999<strong><em>1950</em></strong>051507004 … 9999999N9 + 00001 + 99999999999…)<br>(106, 004301199099999<strong><em>1950</em></strong>051512004 … 9999999N9 + 00221 + 99999999999…)<br>(212, 004301199099999<strong><em>1950</em></strong>051518004 … 9999999N9- 00111 + 99999999999…)<br>(318, 004301265099999<strong><em>1949</em></strong>032412004 … 0500001N9 + 01111 + 99999999999…)<br>(424, 004301265099999<strong><em>1949</em></strong>032418004 … 0500001N9 + 00781 + 99999999999…)</p></blockquote><p>key是行号在文件中的偏移量，我们在map函数中将其忽略。map函数仅提取年份和气温（以粗体显示），并将其作为输出输出（温度值已解释为整数）：</p><pre class="line-numbers language-plain-text"><code class="language-plain-text">(1950, 0)  (1950, 22)  (1950, −11)  (1949, 111)  (1949, 78)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>map函数的输出在发送给reduce函数之前，由MapReduce框架处理。此处理会根据Key来对此key-value对进行排序和分组。因此，继续该示例，我们的reduce函数将会看到以下输入：</p><pre class="line-numbers language-platin-text"><code class="language-platin-text">(1949, [111, 78])(1950, [0, 22, −11])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>每年都会显示所有气温读数的列表，所有reduce函数现在要做的是遍历列表并获取最大读书：</p><pre class="line-numbers language-platin-text"><code class="language-platin-text">(1949, 111)(1950, 22)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这是最终输出：每年记录的最高全球温度。</p><blockquote><p>NOTE: map 函数的输入主要是一个 &lt;Key, Value&gt; 对，在这个例子里，Value 是要统计的所有文本中的一行数据，Key 在一般计算中都不会用到。</p></blockquote><p>整个数据流<strong>如图2-1所示</strong>。<br>该图的底部是一个Unix管道，它模仿了整个MapReduce流，当我们研究Hadoop Streaming时。以后研究Hadoop Streaming时，我们会在此看到。</p><p><img src="https://i.ibb.co/6DLL6w7/hdp-strm-diag.png" alt="模拟Hadoop数据流的示意图"><br>图2-1。MapReduce逻辑数据流</p><h4 id="JAVA-MAPREDUCE"><a href="#JAVA-MAPREDUCE" class="headerlink" title="JAVA MAPREDUCE"></a>JAVA MAPREDUCE</h4><p>上面我们看了MapReduce程序工作的机制，下一步是要通过代码表达它。我们需要三件事：一个map函数，一个reduce函数以及一些代码以运行这个job。map函数由Mapper类来表示，该类声明一个抽象map()方法。<strong>示例2-3</strong>显示了我们的map函数的实现。</p><p><strong><em>示例2-3 最高温度的Mapper示例</em></strong></p><hr><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>LongWritable<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Mapper<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MaxTemperatureMapper</span>    <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token operator">&lt;</span>LongWritable<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> IntWritable<span class="token operator">></span> <span class="token punctuation">{</span>  <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">int</span> MISSING <span class="token operator">=</span> <span class="token number">9999</span><span class="token punctuation">;</span>  <span class="token annotation punctuation">@Override</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span>      <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>    String line <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    String year <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> airTemperature<span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>line<span class="token punctuation">.</span><span class="token function">charAt</span><span class="token punctuation">(</span><span class="token number">87</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'+'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// parseInt doesn't like leading plus signs</span>      airTemperature <span class="token operator">=</span> Integer<span class="token punctuation">.</span><span class="token function">parseInt</span><span class="token punctuation">(</span>line<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">92</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>      airTemperature <span class="token operator">=</span> Integer<span class="token punctuation">.</span><span class="token function">parseInt</span><span class="token punctuation">(</span>line<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">87</span><span class="token punctuation">,</span> <span class="token number">92</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    String quality <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">92</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>airTemperature <span class="token operator">!=</span> MISSING <span class="token operator">&amp;&amp;</span> quality<span class="token punctuation">.</span><span class="token function">matches</span><span class="token punctuation">(</span><span class="token string">"[01459]"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span>year<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span>airTemperature<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Mapper类是一个通用类型(Generic Type)，具有四个形式参数以指定输入键(input key)、输入值(input<br>value)、输出键(output key)和输出值(output value)的类型。</p><p>对于本示例，输入键是一个长整数偏移量，输入值是一行文本，输出键是年份，输出值是气温（整数）。<br>Hadoop不使用内置的Java类型，而是提供了自己的一组基本类型。这些类型针对网络序列化进行了优化。这些类型位于<code>org.apache.hadoop.io</code>这个软件包。<br>在这里，我们使用LongWritable，其对应于一个Java Long，Text(如Java String)，和IntWritable（如Java Integer）。</p><p>该map()方法传递了一个键和一个值。我们将Text包含输入行的值转换为Java String，然后使用其substring()方法提取我们感兴趣的列。</p><p>该map()方法还提供了一个Context实例，用于将输出写入到reduce()。在这个例子中，我们将年份(year)输出为一个Text对象（因为我们只将它当作一个key），然后我们用IntWritable来包装。<br>我们只有在温度值不为空并且其品质代码(quality code)表示气温读书为OK时才会输出一个记录。</p><p>与map函数相似，reduce函数使用Reducer类来实现，如下面的<strong>示例2-4</strong>。</p><p><strong><em>示例2-4 计算最高温度的Reducer示例</em></strong></p><hr><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Reducer<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MaxTemperatureReducer</span>    <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token operator">&lt;</span>Text<span class="token punctuation">,</span> IntWritable<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> IntWritable<span class="token operator">></span> <span class="token punctuation">{</span>  <span class="token annotation punctuation">@Override</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>IntWritable<span class="token operator">></span> values<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span>      <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>    <span class="token keyword">int</span> maxValue <span class="token operator">=</span> Integer<span class="token punctuation">.</span>MIN_VALUE<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>IntWritable value <span class="token operator">:</span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>      maxValue <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">max</span><span class="token punctuation">(</span>maxValue<span class="token punctuation">,</span> value<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span>maxValue<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同样，四个形式参数用于指定输入和输出类型，这一次是reduce函数。reduce函数的输入类型必须与map函数的输出类型匹配：Text和IntWritale。<br>在这个示例中，reduce函数的输出类型是Text和IntWritable，代表了年份和其最高温度，这是通过迭代温度并将每个温度与迄今为止找到的最高记录进行比较而得出的。</p><p>第三段代码运行这个MapReduce作业（请参见<strong>示例2-5</strong>）。</p><p><strong><em>示例2-5 在天气数据集中查找最高温的应用</em></strong></p><hr><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Job<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>FileInputFormat<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span>FileOutputFormat<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MaxTemperature</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>length <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      System<span class="token punctuation">.</span>err<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Usage: MaxTemperature &lt;input path> &lt;output path>"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    Job job <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Job</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>MaxTemperature<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setJobName</span><span class="token punctuation">(</span><span class="token string">"Max temperature"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    FileInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>MaxTemperatureMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>MaxTemperatureReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们需要遵循Job对象的规范来实现这个作业，使您可以控制作业的运行方式。当我们在Hadoop集群上运行此作业时，我们会将代码打包到一个JAR文件中（Hadoop将在集群中分发该文件）。<br>无需显示指定JAR文件的名称，我们可以在Job的setJarByClass()方法中传递一个类，Hadoop将通过查找包含此类的JAR文件来使用该类来找到相关的JAR文件。</p><p>构造Job对象后，我们指定输入和输出路径。通过调用FileInputFormat上的静态的addInputPath()方法，来指定输入路径，并且它可以是一个单个的文件，或者一个目录（这种情况下，输入将会由该目录的所有文件组成），或者是一个文件模式（正则表达式）。<br>顾名思义，addInputPath()可以被调用多次来添加多个路径。</p><p>输出路径（只有一个）是通过调用FileOutputFormat上的静态的setOutputPath()方法来指定的。<br>它指定了reduce函数输出文件的目录。<br>在运行作业之前，该目录应该存在，因为Hadoop会抱怨而不运行作业。此预防措施是为了防止数据丢失（意外地用一个作业的输出覆盖另一个长作业的输出会很烦人）。</p><p>接下来，我们通过setMapperClass()和setReducerClass()方法来指定map和reduce的类型。</p><p>该setOutputKeyClass()和setOutputValueClass()方法为reduce函数控制输出类型，并且必须和Reduce类的产出相匹配。<br>map输出类型默认为相同的类型，因此，如果mapper生成与reducer相同的类型，则不需要设置它们（就像我们的例子一样）。<br>但是，如果它们不同，则必须使用setMapOutputKeyClass()和setMapOutputValueClass()方法设置map输出类型。</p><p>输入类型是通过输入格式来控制的，由于我们使用的是默认的TextInputFormat，因此尚未明确设置输入格式。</p><p>在设置了定义map和reduce函数的类之后，我们就可以运行该作业了。在waitForCompletion()对方法Job提交作业并等待它完成。该方法的单个参数是指示是否生成详细输出的标志。如果为true，则作业将有关其进度的信息写入控制台。</p><p>waitForCompletion()方法的返回值是一个布尔值，指示成功（true）或失败（false），我们将其转换为程序的退出代码0或1。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要参考：</p><ul><li><strong>极客时间专栏 - <a href="https://time.geekbang.org/column/intro/133" target="_blank" rel="noopener">李智慧 - 从0开始学大数据</a></strong></li><li><strong><a href="https://learning.oreilly.com/library/view/hadoop-the-definitive/9781491901687/" target="_blank" rel="noopener">Hadoop: The Definitive Guide, 4th Edition - Chapter2 - MapReduce</a></strong></li></ul><p>今天我们学习了MapReduce编程模型。这个模型既简单又强大，简单是因为它只包含Map和Reduce两个过程，强大之处在于它可以实现大数据领域几乎所有的计算需求。这也正是MapReduce这个模型令人着迷的地方。</p><p>以图2-1的这个查找最高温的程序为例，我们做的就是编写map函数，截取每行(key)中固定位置的字段作为输出给reduce的key（年份）；截取温度（value）输出给reduce。<br>然后MapReduce框架会在数据输出给reduce之前，先帮我们进行一次reduce，也就是图中shuffle的阶段。<br>这个阶段会将相同的年份的value合并，原本的value是一个IntWritable，合并后会变为<code>Iterable&lt;IntWritable&gt;</code>这么一个数组。<br>所以我们在reduce中需要实现业务逻辑就是遍历这个数组，取最大值。<br>可见，许多和key相关的操作，都是框架帮我们完成的，我们需要对MapReduce编程模型先加以了解，就可以按照它的规范完成自己的业务逻辑。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS Architecture and Explaination of Design</title>
      <link href="2020/09/29/hdfs-architecture-and-explaination-of-design/"/>
      <url>2020/09/29/hdfs-architecture-and-explaination-of-design/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS-架构以及解释其设计"><a href="#HDFS-架构以及解释其设计" class="headerlink" title="HDFS 架构以及解释其设计"></a>HDFS 架构以及解释其设计</h1><p><strong><code>HDFS</code> 也许不是最好的大数据存储技术，但依然是最重要的大数据存储技术。</strong><br>为什么呢？<br>在整个大数据体系里面，最宝贵、最难以代替的资产就是数据，大数据所有的一切都要围绕数据展开。HDFS存储着宝贵的数据资产，各种新的算法、框架要想得到人们的广泛使用，必须支持HDFS才能获取已经存储在里面的数据。</p><h2 id="HDFS-架构图"><a href="#HDFS-架构图" class="headerlink" title="HDFS 架构图"></a>HDFS 架构图</h2><p>我们都知道 <code>RAID</code> 是在多个磁盘上进行文件存储及并行读写，将 <code>RAID</code> 的设计理念扩大到整个分布式服务器集群,就产生了分布式文件系统， <code>Hadoop</code> 分布式文件系统的核心原理就是如此。</p><p><code>HDFS</code> 是在一个大规模分布式服务器集群上，对数据进行分片后进行并行读写及冗余存储。<br><code>HDFS</code> 的设计目标是管理数以千计的服务器、数以万计的磁盘，将这么大的规模的计算资源当作一个单一的存储系统进行管理，对应用程序提供数以 PB 计的存储容量，让应用程序像使用普通文件系统一样存储大规模的文件数据。</p><p>下面我们来看一下 <code>HDFS</code> 的架构图：<br><code>HDFS</code> 架构图<br><img src="https://i.ibb.co/0jKqBMR/HDFS-architecture.png" alt="HDFS_architecture" border="0"></p><p>上图是 HDFS 的架构图，从图中你可以看到 HDFS 的关键组件有两个，一个是 DataNode，一个是 NameNode。</p><p>在实践中，HDFS 集群的 DataNode 服务器会有很多台，一般在几百台到几千台这样的规模，每台服务器配有数块磁盘，整个集群的存储容量大概在几 PB 到数百 PB。</p><h2 id="HDFS-的高可用设计"><a href="#HDFS-的高可用设计" class="headerlink" title="HDFS 的高可用设计"></a>HDFS 的高可用设计</h2><p>在设计一个高可用系统时，我们需要从微观到宏观地去考虑采取什么措施，这里的措施就是如何容错(failover)。<br>When we consider how to design a high-available system, we need to think of what corresponding mesures to take from the micro to the macro perspective.  </p><ol><li><p>数据存储故障容错</p><p>这里面主要涉及：  </p><ul><li><code>Block</code></li><li><code>checksum</code></li></ul></li><li><p>磁盘故障容错</p><p>这里面主要涉及：  </p><ul><li><code>BlockID</code></li><li><code>NameNode</code> 保存了 <code>metadata</code> ，<code>metadata</code> 包括了文件名和文件的 <code>BlockID</code>，以及分散在什么位置(哪个 <code>DataNode</code>，哪块盘上)</li></ul></li><li><p>DataNode 故障容错</p><p>这里面主要涉及：</p><ul><li><code>DataNode</code> 和 <code>NameNode</code> 之间保持心跳</li><li>数据分散在不同的 <code>DataNode</code> 上，metadata 保存在 <code>NameNode</code> 上</li></ul></li><li><p>NameNode 故障容错</p><p> 这里面主要涉及：  </p><ul><li><code>Zookeeper</code>（分布式锁原理）</li><li><code>Active NameNode</code> 和 <code>Standby NameNode</code></li><li><code>Shared Edits</code></li><li>ANN 维持 <code>znode锁</code>，SNN 获取 <code>znode锁</code></li><li><code>DataNode</code> 向 ANN 和 SNN 同事发送心跳，只有 ANN 能返回控制信息</li></ul></li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>我们小结一下，看看 <code>HDFS</code> 是如何通过大规模分布式服务器集群实现数据的大容量、高速、高可靠的存储、访问的。</p><ol><li>文件数据以数据块(<code>Block</code>)的方式进行切分，数据块可以存储在集群中任意的 <code>DataNode</code> 上。所以 <code>HDFS</code> 存储的文件可以非常大，一个文件理论上可以占据整个 <code>HDFS</code> 服务器集群上的所有磁盘，实现了大容量存储。</li><li><code>HDFS</code> 一般的访问模式是通过 <code>MapReduce</code> 程序在计算时读取，<code>MapReduce</code> 对输入数据进行分片读取，<strong>通常一个分片就是一个数据块</strong> <sup id="a1"><a href="#f1">1</a></sup>，每个数据块分配一个计算进程，这样就可以同时启动很多进程对一个 <code>HDFS</code> 文件的多个数据块进行并发访问，从而实现数据的告诉访问。关于 <code>MapReduce</code> 的具体处理过程，我以后再详细记录。</li><li>DataNode 存储的数据块会进行复制，使每个数据块在集群里有多个备份，保证了数据的可靠性，并通过一系列的故障容错手段实现 <code>HDFS</code> 系统中主要组件的高可用，进而保证数据和整个系统的高可用。</li></ol><h2 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h2><p>这里有一个关于 <code>HDFS</code> 的漫画，可以作为工作机制的概括：<br><a href="https://kdocs.cn/l/stD7mMmVQgja" target="_blank" rel="noopener">金山文档 - HDFS_comic.pdf</a><br>缩略图：<br><img src="https://i.ibb.co/XYwnVtp/HDFS-comic.png" alt="HDFS comic" border="0"></p><p><b id="f1">1</b> Mapreduce中有分片(fragment)概念，和HDFS的数据块(Block)概念不一样。 <a href="#a1">↩</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用官方 CloudFormation 模板部署 EKS</title>
      <link href="2020/06/09/shi-yong-guan-fang-cloudformation-mo-ban-bu-shu-eks/"/>
      <url>2020/06/09/shi-yong-guan-fang-cloudformation-mo-ban-bu-shu-eks/</url>
      
        <content type="html"><![CDATA[<h1 id="AWS-Elastic-Kubernetes-Service-EKS-快速入门-Quick-Start-的指南"><a href="#AWS-Elastic-Kubernetes-Service-EKS-快速入门-Quick-Start-的指南" class="headerlink" title="AWS Elastic Kubernetes Service(EKS) 快速入门(Quick Start)的指南"></a>AWS Elastic Kubernetes Service(EKS) 快速入门(Quick Start)的指南</h1><p>科技的进步一直在改变生产方式，软件工程的方法论近年来的其中一个重要主题是DevOps，公有云自然也是全面拥抱这个概念的。<br>公有云上有种类繁多的各种服务，DevOps类的服务自然也是值得重点关注的对象。<br>“最佳实践”这个关键词，我想大家都经常听说，许多做得好的软件服务商都会提供关于自家产品的最佳实践相关的文档。</p><p>那什么是AWS 快速入门(Quick Start)呢？</p><blockquote><p>AWS Quick Start 是一种适用于 AWS 云中关键工作负载的自动化参考部署。<br>每个 Quick Start 都使用有关安全性和可用性的 AWS 最佳实践来启动、配置和运行在 AWS 上部署特定工作负载所需的 AWS 计算、网络、存储和其他服务。</p><p>Quick Start 是一种加速器，可将数百个手动程序简化为几个步骤，其特点是速度快，成本低且可定制，功能齐全，且专为生产而设计。</p><p>Quick Start 包括：</p><ul><li>适用于部署的参考架构</li><li>可用于自动化和配置部署的 AWS CloudFormation 模板（JSON 或 YAML 脚本）</li><li>详细说明了具体架构和安装启用问题，且提供了自定义部署说明的部署指南</li></ul><p>Quick Start 还包括一些集成，这些集成扩展了 Amazon Connect 提供的基于云的联系中心功能与 APN 合作伙伴提供的关键服务和解决方案，用于客户关系管理 (CRM)、人力资源优化 (WFO)、分析、统一通信 (UC) 及其他使用案例。</p></blockquote><p>所以简单来说，<code>Quick Start</code>就是AWS对于自身认为重要的服务，提供的符合最佳实践和集成了DevOps工具的服务。<br>举个例子，你想在亚马孙上创建一个符合最佳实践的VPC（虚拟私有网络），也可以找到相应的Quick Start来“一键创建”。<br>我认为，对于公有云的使用者来说，Quick Start的意义，除了是方便用户去创建、初始化服务之外，更加是学习使用公有云服务的一个最佳途径之一。<br>因为Quick Start中往往使用了具有实际生产可用的示例配置，以及以<strong>代码</strong>的形式来展现出这些配置项，对于用户来说就非常清晰，有助于更好地理解架构的逻辑。</p><p><strong>这里要额外提一点，AWS提供了AWS CloudFormation以便用户去“编排”云服务，对应地在开源方案中也有一个叫Terraform的编排工具，阿里云就是其支持者之一，当然AWS上也可以使用Terraform。</strong></p><p>相对于AWS闭环生态，阿里云的策略看起来是拥抱开原方案。<br>开源云服务编排工具 – Terraform，可以了解一下。<br>也是声明式API。<br><a href="https://www.terraform.io/" target="_blank" rel="noopener">https://www.terraform.io/</a></p><p>阿里云官方文档中有Terraform专区：<br><a href="https://help.aliyun.com/document_detail/146291.html?spm=a2c4g.11174283.3.8.19ec11e90z3tae" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/146291.html?spm=a2c4g.11174283.3.8.19ec11e90z3tae</a></p><p>AWS上也可以使用，参考可以在这里找：<br><a href="https://www.terraform.io/docs/providers/aws/index.html" target="_blank" rel="noopener">https://www.terraform.io/docs/providers/aws/index.html</a></p><h2 id="AWS-EKS-Quick-Start-的部署内容"><a href="#AWS-EKS-Quick-Start-的部署内容" class="headerlink" title="AWS EKS Quick Start 的部署内容"></a>AWS EKS Quick Start 的部署内容</h2><p>使用 Quick Start 自动设置的 Amazon EKS环境，包括下列内容：  </p><ul><li>一个跨三个可用区的高度可用的架构。</li><li>一个根据 AWS 最佳实践配置有公有子网和私有子网的 Virtual Private Cloud(VPC)，用于在 AWS 上为您提供您自己的虚拟网络。</li><li>在公共子网中设置托管 NAT 网关，以允许对私有子网中的资源进行出站互联网访问。</li><li>在一个公共子网内的 Auto Scailing 组中设置 Linux 堡垒主机，以允许对私有子网中的Amazon Elastic Compute Cloud(Amazon EC2)实例进行入站安全Shell(SSH)访问。堡垒主机还配置有用于管理 Kubernetes 集群的 Kubernetes kubectl 命令行界面。</li><li>提供 Kubernetes 控制平面的 Amazon EKS 集群。</li><li>在私有子网中设置一组 Kubernetes 节点。</li></ul><p>上述内容摘自<a href="https://aws.amazon.com/cn/quickstart/architecture/amazon-eks/" target="_blank" rel="noopener">官网简介</a></p><h2 id="关于-EKS-的-CloudFormation-模板解析"><a href="#关于-EKS-的-CloudFormation-模板解析" class="headerlink" title="关于 EKS 的 CloudFormation 模板解析"></a>关于 EKS 的 CloudFormation 模板解析</h2><p>AWS EKS Quick Start使用这个<a href="https://s3.amazonaws.com/aws-quickstart/quickstart-amazon-eks/templates/amazon-eks-master.template.yaml" target="_blank" rel="noopener">CloudFormation模板</a>来新建VPC并初始化EKS。<br>下面是我对于此模板内容的解析：  </p><p><a href="../cloudFormation/amazon-eks-master.template.yaml">EKS CloudFormation 模板解析</a></p><h3 id="模板中涉及到-VPC-相关的配置"><a href="#模板中涉及到-VPC-相关的配置" class="headerlink" title="模板中涉及到 VPC 相关的配置"></a>模板中涉及到 VPC 相关的配置</h3><ul><li>可用区1的私有子网默认CIDR为：<strong>10.0.0.0/19</strong>，也就是 <code>10.0.0.1 ~ 10.0.31.254</code> 这 8190 个 IP。  </li><li>可用区2的私有子网默认CIDR为：<strong>10.0.32.0/19</strong>，也就是 <code>10.0.32.1 ~ 10.0.63.254</code> 这 8190 个 IP。  </li><li>可用区3的私有子网默认CIDR为：<strong>10.0.64.0/19</strong>，也就是 <code>10.0.64.1 ~ 10.0.95.254</code> 这 8190 个 IP。  </li></ul><hr><ul><li>可用区1的公有子网默认CIDR为：<strong>10.0.128.0/20</strong>，也就是 <code>10.0.128.1 ~ 10.0.143.254</code> 这 4094 个 IP。  </li><li>可用区2的公有子网默认CIDR为：<strong>10.0.144.0/20</strong>，也就是 <code>10.0.144.1 ~ 10.0.159.254</code> 这 4094 个 IP。  </li><li>可用区3的公有子网默认CIDR为：<strong>10.0.160.0/20</strong>，也就是 <code>10.0.160.1 ~ 10.0.175.254</code> 这 4094 个 IP。  </li></ul><hr><ul><li>VPCCIDR：默认值为<code>10.0.0.0/16</code>，也就是<code>10.0.0.1 ~ 10.0.255.254</code> 这 65534 个 IP。</li><li>NumberOfAZs： 默认值为<code>3</code>。</li></ul><h3 id="模板中涉及到访问-EKS-相关的配置"><a href="#模板中涉及到访问-EKS-相关的配置" class="headerlink" title="模板中涉及到访问 EKS 相关的配置"></a>模板中涉及到访问 EKS 相关的配置</h3><ul><li><p>RemoteAccessCIDR</p><blockquote><p>允许这个CIDR访问EKS中的node，可以设置为包含VPC的所有网段的CIDR。</p></blockquote></li><li><p>EKSPublicAccessCIDRs</p><blockquote><p>允许这个CIDR访问EKS的API，这个CIDR必须是公网的。默认为0.0.0.0/0。<br>由于Kubernetes API一般是基于双向TLS认证的，所以基本上开放给公网也没有什么问题。</p></blockquote></li><li><p>EKSPublicAccessEndpoint</p><blockquote><p>是否开启暴露给公网的EKS API Gateway。相当于创建一个load balancer，分配一个公网IP，这个load balancer绑定了Kubernetes的API Service。</p></blockquote></li><li><p>EKSPrivateAccessEndpoint</p><blockquote><p>是否开启暴露给VPC的EKS API Gateway。相当于原生Kubernetes初始化后的kubernetes这个Service。</p></blockquote></li><li><p>AdditionalEKSAdminUserArn</p><blockquote><p>这个IAM user拥有EKS的admin权限。<br>注意： EKS 只是使用 IAM 验证</p></blockquote></li><li><p>AdditionalEKSAdminRoleArn</p><blockquote><p>这个IAM role拥有EKS的admin权限。</p></blockquote></li><li><p>ProvisionBastionHost</p><blockquote><p>是否自动创建堡垒机。</p></blockquote></li></ul><h3 id="模板中涉及到-EKS-control-plain-相关的配置"><a href="#模板中涉及到-EKS-control-plain-相关的配置" class="headerlink" title="模板中涉及到 EKS control plain 相关的配置"></a>模板中涉及到 EKS control plain 相关的配置</h3><ul><li><p>KubernetesVersion</p><blockquote><p>默认为”1.16”，可选值为：”1.13”, “1.14”, “1.15”, “1.16”。</p></blockquote></li></ul><h3 id="模板中涉及到-EKS-node-相关的配置"><a href="#模板中涉及到-EKS-node-相关的配置" class="headerlink" title="模板中涉及到 EKS node 相关的配置"></a>模板中涉及到 EKS node 相关的配置</h3><ul><li><p>NodeInstanceType</p><blockquote><p>这些<code>实例类型(t3.micro, t3.small等)</code>可以作为 node 的实例类型可选项。</p></blockquote></li><li><p>NumberOfNodes</p><blockquote><p>初始化时 node 的数量，注意：默认是3，也就是 HA 架构下 control plane 的 node 数量。</p></blockquote></li><li><p>MaxNumberOfNodes</p><blockquote><p>集群 node 的最大数量。</p></blockquote></li><li><p>ManagedNodeGroup</p><blockquote><p>这个<code>Managed Node Group</code>在后台利用<code>EC2 Auto Scailing group</code> 功能。建议开启此功能，此功能是使用<code>EKS Cluster Autoscaler</code>的前提。<br>而<code>EKS Cluster Autoscaler</code>是<code>HPA</code>的隐性前提，因为当Node不够的时候如果不会自动加入新的Node，则HPA也无法完成。</p></blockquote></li><li><p>ClusterAutoScaler</p><blockquote><p>这个功能结合上面的 ManagedNodeGroup 来实现 EKS 集群 Node 的水平扩展。<br>但是不知道为什么这里设置成不能两个功能都开启，可能是因为Auto Scaler的子template中有关于Managed Node Group的操作。</p></blockquote></li></ul><h3 id="模板中涉及到监控相关的配置"><a href="#模板中涉及到监控相关的配置" class="headerlink" title="模板中涉及到监控相关的配置"></a>模板中涉及到监控相关的配置</h3><ul><li><p>MonitoringStack</p><blockquote><p>目前只有 <code>Prometheus + Grafana</code> 这一个选项。</p></blockquote></li></ul><h3 id="模板中涉及到-EKS-其他-addons-和-AWS-其他服务集成相关的配置"><a href="#模板中涉及到-EKS-其他-addons-和-AWS-其他服务集成相关的配置" class="headerlink" title="模板中涉及到 EKS 其他 addons 和 AWS 其他服务集成相关的配置"></a>模板中涉及到 EKS 其他 addons 和 AWS 其他服务集成相关的配置</h3><ul><li><p>ALBIngressController</p><blockquote><p>是否自动部署一个ALB Ingress Controller(类似于Traefik)。</p></blockquote></li><li><p>EfsStorageClass</p><blockquote><p>是否自动创建一个 EFS 卷并创建 EFS StorageClass。</p></blockquote></li><li><p>FargateNamespaces</p><blockquote><p>Fargate 是 AWS 的 serverless 服务，以容器为载体。EKS 中可以设置专属于 Fargate 服务的 Namespace，适用于一些离线或批处理任务，比如 Jenkins 的构建任务。</p></blockquote></li></ul><h3 id="查看具体模板以及子模板内容"><a href="#查看具体模板以及子模板内容" class="headerlink" title="查看具体模板以及子模板内容"></a>查看具体模板以及子模板内容</h3><p>首先你应该打开CloudFormation的文档看看<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html" target="_blank" rel="noopener">模板内可填写的字段(field)</a>，和可以使用的<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html" target="_blank" rel="noopener">内置函数</a>、<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html" target="_blank" rel="noopener">best practice</a>。</p><p>然后可以浏览一下这里面的 template，这些就是 QuickStart EKS 所用到的 template 了。  <a href="https://github.com/aws-quickstart/quickstart-amazon-eks/tree/master/templates" target="_blank" rel="noopener">https://github.com/aws-quickstart/quickstart-amazon-eks/tree/master/templates</a><br>其中入口 tempalte 是 <code>amazon-eks-master.template.yaml</code>。</p><h2 id="EKS-QuickStart-部署记录"><a href="#EKS-QuickStart-部署记录" class="headerlink" title="EKS QuickStart 部署记录"></a>EKS QuickStart 部署记录</h2><p>按照 AWS EKS QuickStart 的 <a href="https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/deployment-steps.html" target="_blank" rel="noopener">Deployment steps</a> 的提示，一步一步做就好了。<br>以下的实例是新建一个 VPC 的实例，一个 Region 中默认允许创建 5 个 VPC，也可以修改这个 quota。<br>考虑到已经存在的 VPC 和 EKS 的 VPC 可能需要建立内网连接，所以在规划 EKS VPC 的 subnet 时，需要避免和已有的 VPC 的 CIDR 重叠。<br>关于 Parameters 和 Options 可以参考文档来作为修改的依据：<br><a href="https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/step-2.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/step-2.html</a></p><ol><li><p>可以看到，在 CloudFormation 上创建 Stack 的时候，Parameters 的顺序就与 Template 中 <code>Metadata.AWS::CloudFormation::Interface</code> 中定义的顺序一致。</p><p> 如下图：<br> <img src="EKS-QuickStart-Parameters-1.png" alt="EKS CloudFormation stack Web-UI"><br> <img src="EKS-QuickStart-Parameters-2.png" alt="EKS CloudFormation template interface"></p></li><li><p>在 CloudFormation Web-UI 中填写完 EKS 的模板中定义的 Parameters 和 Options之后，会出现一个确认页面，并且可以生成一个 URL 用来保存这次填写的信息，以便以后再次生成同一个配置集合的 Stack。</p><p> 如下（你得要登入AWS才能生效）：<br> <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/quickcreate?templateUrl=https%3A%2F%2Fs3.amazonaws.com%2Faws-quickstart%2Fquickstart-amazon-eks%2Ftemplates%2Famazon-eks-master.template.yaml&amp;stackName=Amazon-EKS&amp;param_ALBIngressController=Enabled&amp;param_AdditionalEKSAdminRoleArn=&amp;param_AdditionalEKSAdminUserArn=&amp;param_AvailabilityZones%5B%5D=us-east-1a&amp;param_AvailabilityZones%5B%5D=us-east-1b&amp;param_AvailabilityZones%5B%5D=us-east-1c&amp;param_ClusterAutoScaler=Disabled&amp;param_EKSClusterLoggingTypes=api%2Caudit%2Cauthenticator%2CcontrollerManager%2Cscheduler&amp;param_EKSEncryptSecrets=Enabled&amp;param_EKSEncryptSecretsKmsKeyArn=&amp;param_EKSPrivateAccessEndpoint=Enabled&amp;param_EKSPublicAccessCIDRs=0.0.0.0%2F0&amp;param_EKSPublicAccessEndpoint=Enabled&amp;param_EfsPerformanceMode=generalPurpose&amp;param_EfsProvisionedThroughputInMibps=0&amp;param_EfsStorageClass=Enabled&amp;param_EfsThroughputMode=bursting&amp;param_FargateLabels=fargate%3Dtrue&amp;param_FargateNamespaces=fargate&amp;param_KeyPairName=id_rsa_centos&amp;param_KubernetesVersion=1.16&amp;param_LambdaZipsBucketName=&amp;param_ManagedNodeGroup=yes&amp;param_ManagedNodeGroupAMIType=AL2_x86_64&amp;param_MangedNodeGroupLabel=&amp;param_MaxNumberOfNodes=3&amp;param_MonitoringStack=Prometheus%20%2B%20Grafana&amp;param_NodeGroupName=Default&amp;param_NodeInstanceType=t3.medium&amp;param_NodeVolumeSize=20&amp;param_NumberOfAZs=3&amp;param_NumberOfNodes=3&amp;param_PrivateSubnet1CIDR=10.0.0.0%2F19&amp;param_PrivateSubnet2CIDR=10.0.32.0%2F19&amp;param_PrivateSubnet3CIDR=10.0.64.0%2F19&amp;param_ProvisionBastionHost=Enabled&amp;param_PublicSubnet1CIDR=10.0.128.0%2F20&amp;param_PublicSubnet2CIDR=10.0.144.0%2F20&amp;param_PublicSubnet3CIDR=10.0.160.0%2F20&amp;param_QSS3BucketName=aws-quickstart&amp;param_QSS3BucketRegion=us-east-1&amp;param_QSS3KeyPrefix=quickstart-amazon-eks%2F&amp;param_RemoteAccessCIDR=10.0.0.0%2F16&amp;param_VPCCIDR=10.0.0.0%2F16" target="_blank" rel="noopener">https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/quickcreate?templateUrl=https%3A%2F%2Fs3.amazonaws.com%2Faws-quickstart%2Fquickstart-amazon-eks%2Ftemplates%2Famazon-eks-master.template.yaml&amp;stackName=Amazon-EKS&amp;param_ALBIngressController=Enabled&amp;param_AdditionalEKSAdminRoleArn=&amp;param_AdditionalEKSAdminUserArn=&amp;param_AvailabilityZones%5B%5D=us-east-1a&amp;param_AvailabilityZones%5B%5D=us-east-1b&amp;param_AvailabilityZones%5B%5D=us-east-1c&amp;param_ClusterAutoScaler=Disabled&amp;param_EKSClusterLoggingTypes=api%2Caudit%2Cauthenticator%2CcontrollerManager%2Cscheduler&amp;param_EKSEncryptSecrets=Enabled&amp;param_EKSEncryptSecretsKmsKeyArn=&amp;param_EKSPrivateAccessEndpoint=Enabled&amp;param_EKSPublicAccessCIDRs=0.0.0.0%2F0&amp;param_EKSPublicAccessEndpoint=Enabled&amp;param_EfsPerformanceMode=generalPurpose&amp;param_EfsProvisionedThroughputInMibps=0&amp;param_EfsStorageClass=Enabled&amp;param_EfsThroughputMode=bursting&amp;param_FargateLabels=fargate%3Dtrue&amp;param_FargateNamespaces=fargate&amp;param_KeyPairName=id_rsa_centos&amp;param_KubernetesVersion=1.16&amp;param_LambdaZipsBucketName=&amp;param_ManagedNodeGroup=yes&amp;param_ManagedNodeGroupAMIType=AL2_x86_64&amp;param_MangedNodeGroupLabel=&amp;param_MaxNumberOfNodes=3&amp;param_MonitoringStack=Prometheus%20%2B%20Grafana&amp;param_NodeGroupName=Default&amp;param_NodeInstanceType=t3.medium&amp;param_NodeVolumeSize=20&amp;param_NumberOfAZs=3&amp;param_NumberOfNodes=3&amp;param_PrivateSubnet1CIDR=10.0.0.0%2F19&amp;param_PrivateSubnet2CIDR=10.0.32.0%2F19&amp;param_PrivateSubnet3CIDR=10.0.64.0%2F19&amp;param_ProvisionBastionHost=Enabled&amp;param_PublicSubnet1CIDR=10.0.128.0%2F20&amp;param_PublicSubnet2CIDR=10.0.144.0%2F20&amp;param_PublicSubnet3CIDR=10.0.160.0%2F20&amp;param_QSS3BucketName=aws-quickstart&amp;param_QSS3BucketRegion=us-east-1&amp;param_QSS3KeyPrefix=quickstart-amazon-eks%2F&amp;param_RemoteAccessCIDR=10.0.0.0%2F16&amp;param_VPCCIDR=10.0.0.0%2F16</a></p></li><li><p>创建过程中</p><p> 如下图：<br> <img src="EKS-QuickStart-onProgress.png" alt="EKS QuickStart on progress"></p></li></ol><h2 id="EKS-QuickStart-注意事项"><a href="#EKS-QuickStart-注意事项" class="headerlink" title="EKS QuickStart 注意事项"></a>EKS QuickStart 注意事项</h2><p>当前时间为<code>2020-6-5</code>。<br>目前我发现以下问题：  </p><ol><li><p>EKS QuickStart 的入口 tempalte 中定义了 <code>Managed Node Group</code> 不能和 <code>AutoScaler</code> 同时启用，但是看 EKS 的官方文档，我目前的理解是其实 <code>Managed Node Group</code> 是 <code>AutoScaler</code> 的前提，可以关注此 <a href="https://github.com/aws-quickstart/quickstart-amazon-eks/issues/147" target="_blank" rel="noopener">issue</a>。</p></li><li><p>目前启用 <code>AutoScaler</code> 会在创建的过程中由于 AutoScaler 的 ImagePullBackoff 问题导致部署失败，所以最好是手动部署比较好，可以参考 EKS 的官方文档，然后修改镜像 repo。</p><blockquote><p>这个问题是由于 <a href="https://github.com/aws-quickstart/quickstart-amazon-eks/blob/33474c09a6ccf9e3e647758d6a3cd8513e304c54/templates/amazon-eks-cluster-autoscaler.template.yaml#L31" target="_blank" rel="noopener">AutoScaler 的 template</a> 中的 <code>Mappings</code> 部分中，定义了“如果 EKS 的版本是 1.16 则拉取 AutoScaler 的 ImageTag 为 v1.16.8 ”，由于当下(2020-6-9)，AutoScaler 的这个 Tag 拉取不了，导致了此问题。<br>今天刚刚由此 <a href="https://github.com/aws-quickstart/quickstart-amazon-eks/commit/e692b93d9b3edd321ced5a48fbe26e94ba3de90d" target="_blank" rel="noopener">commit</a> 修复了，将 AutoScaler 的 ImageTag 改成了 v1.16.5。</p></blockquote></li></ol>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> EKS </tag>
            
            <tag> CloudFormation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quick guide to deploying and using Traefik 2.0 on Kubernetes</title>
      <link href="2020/04/08/quick-guide-to-deploying-and-using-traefik-2-0-on-kubernetes/"/>
      <url>2020/04/08/quick-guide-to-deploying-and-using-traefik-2-0-on-kubernetes/</url>
      
        <content type="html"><![CDATA[<h1 id="在Kubernetes中部署和使用Traefik-2-0的快速指南"><a href="#在Kubernetes中部署和使用Traefik-2-0的快速指南" class="headerlink" title="在Kubernetes中部署和使用Traefik 2.0的快速指南"></a>在Kubernetes中部署和使用Traefik 2.0的快速指南</h1><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>上一次我安装Traefik时还在使用Traefik 1.7版本，前天我这里有个项目搞容器化，部署<strong>边缘节点</strong>时，选型Ingress Controller的时候又去看了下Traefik，发现2.0版本功能强大地让人惊讶，果断选择了Traefik。</p><blockquote><p>关于边缘节点的概念，可以参考我之前总结的<a href="https://shouneng.website/2019/12/31/how-to-setup-traefik-as-ingress-controller-to-expose-service-from-kubernetes/">一篇博客</a>。</p></blockquote><p>本文旨在为同行提供Traefik 2.0踩坑经验的分享。</p><h3 id="本文内容"><a href="#本文内容" class="headerlink" title="本文内容"></a>本文内容</h3><ol><li>在Kubernetes中使用Helm安装Traefik的选项说明</li><li>关于配置<a href="https://shouneng.website/2019/12/31/how-to-setup-traefik-as-ingress-controller-to-expose-service-from-kubernetes/">边缘节点</a>的说明</li><li>使用Traefik 2.0的CRD – IngressRoute为一个HTTP和WebSocket后端服务配置路由的说明</li><li>Traefik 2.0功能以及使用场景简单总结</li></ol><p>阅读本文需要有以下前提:</p><ul><li>Kubernetes的<a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">基本概念</a>要清晰</li><li><a href="https://helm.sh/docs/" target="_blank" rel="noopener">Helm</a>的组件逻辑结构(Repo、Chart、release、template等)，基本使用等经验</li></ul><h4 id="在Kubernetes中使用Helm安装Traefik的选项说明"><a href="#在Kubernetes中使用Helm安装Traefik的选项说明" class="headerlink" title="在Kubernetes中使用Helm安装Traefik的选项说明"></a>在Kubernetes中使用Helm安装Traefik的选项说明</h4><p>在k8s中安装Traefik，一般使用官方的<a href="https://docs.traefik.io/getting-started/install-traefik/#use-the-helm-chart" target="_blank" rel="noopener">Helm chart</a>，就可以了。<br>官方的例子中没有对选项有什么解释，所以我先把chart下载下来看看有啥内容。</p><pre class="line-numbers language-bash"><code class="language-bash">helm pull traefik/traefik --version 7.0.0<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>看过一遍之后，我也没啥想改的，除了为了把Traefik的Pod固定在指定的节点上，我打算使用<strong>Node Affinity</strong>属性之外，就没有改<code>values.yaml</code>中的其他部分了。</p><p>以下为Traefik-7.0.0的chart中我修改了部分。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">affinity</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># # This example pod anti-affinity forces the scheduler to put traefik pods</span><span class="token comment" spellcheck="true"># # on nodes where no other traefik pods are scheduled.</span><span class="token comment" spellcheck="true"># # It should be used when hostNetwork: true to prevent port conflicts</span>  <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>    <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>      <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role          <span class="token key atrule">operator</span><span class="token punctuation">:</span> In          <span class="token key atrule">values</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> ingress<span class="token punctuation">-</span>controller        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>controller          <span class="token key atrule">operator</span><span class="token punctuation">:</span> In          <span class="token key atrule">values</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> traefik  <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>    <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app          <span class="token key atrule">operator</span><span class="token punctuation">:</span> In          <span class="token key atrule">values</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> software<span class="token punctuation">-</span>traefik      <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> failure<span class="token punctuation">-</span>domain.beta.kubernetes.io/zone  <span class="token comment" spellcheck="true"># This example node affinity "forces" the scheduler to put traefik pods</span>  <span class="token comment" spellcheck="true"># on nodes with specified lables.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后使用以下的一段命令来部署了Traefik。</p><ul><li><p>我打算将<code>ip-10-20-1-95</code>和<code>ip-10-20-1-96</code>作为<a href="https://shouneng.website/2019/12/31/how-to-setup-traefik-as-ingress-controller-to-expose-service-from-kubernetes/">边缘节点</a>，所以打上label</p><pre class="line-numbers language-bash"><code class="language-bash">  <span class="token comment" spellcheck="true"># Specify traefik node label, to be used with node affinity</span>  kubectl label node ip-10-20-1-95 ip-10-20-1-96 node-role<span class="token operator">=</span>ingress-controller ingress-controller<span class="token operator">=</span>traefik<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>用<code>helm template</code>命令渲染一下模板，看看最终生成的manifest长啥样？</p><pre class="line-numbers language-bash"><code class="language-bash">  <span class="token function">cd</span> ~/helm/charts/traefik  <span class="token function">mkdir</span> -pv dry-run  <span class="token comment" spellcheck="true"># Render Helm chart into dry-run to check final manifests</span>  <span class="token comment" spellcheck="true"># In dev environment, we can use Traefik dashboard.</span>  <span class="token comment" spellcheck="true"># Can use a fixed IP for Traefik service</span>  helm template -n traefik-v2 --dry-run --dependency-update --output-dir dry-run traefik-v2-r2 ./traefik-7.0.0-customized/traefik \  --set nameOverride<span class="token operator">=</span>software-traefik \  --set image.name<span class="token operator">=</span><span class="token string">'harbor.YOUR-DOMAIN.com.cn/library/traefik'</span> \  --set image.tag<span class="token operator">=</span><span class="token string">'2.2.0'</span> \  --set deployment.replicas<span class="token operator">=</span>2 \  --set ports.traefik.hostPort<span class="token operator">=</span>9000 \  --set ports.web.hostPort<span class="token operator">=</span>8000 \  --set ports.websecure.hostPort<span class="token operator">=</span>8443 \  --set additionalArguments<span class="token operator">=</span><span class="token string">"{--accesslog=true}"</span> \  --set service.type<span class="token operator">=</span>ClusterIP \  --set service.spec.clusterIP<span class="token operator">=</span><span class="token string">'10.20.211.200'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>正式安装Traefik</p><pre class="line-numbers language-bash"><code class="language-bash">  <span class="token comment" spellcheck="true"># Install Traefik</span>  helm upgrade --history-max 10 --atomic --install --namespace traefik-v2 traefik-v2 ./traefik-7.0.0-customized/traefik/ \  --set nameOverride<span class="token operator">=</span>software-traefik \  --set image.name<span class="token operator">=</span><span class="token string">'harbor.YOUR-DOMAIN.com.cn/library/traefik'</span> \  --set image.tag<span class="token operator">=</span><span class="token string">'2.2.0'</span> \  --set deployment.replicas<span class="token operator">=</span>2 \  --set ports.traefik.hostPort<span class="token operator">=</span>9000 \  --set ports.web.hostPort<span class="token operator">=</span>8000 \  --set ports.websecure.hostPort<span class="token operator">=</span>8443 \  --set additionalArguments<span class="token operator">=</span><span class="token string">"{--accesslog=true}"</span> \  --set service.type<span class="token operator">=</span>ClusterIP \  --set service.spec.clusterIP<span class="token operator">=</span><span class="token string">'10.20.211.200'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="关于配置边缘节点的说明"><a href="#关于配置边缘节点的说明" class="headerlink" title="关于配置边缘节点的说明"></a>关于配置边缘节点的说明</h4><p>首先解释下什么叫<a href="https://shouneng.website/2019/12/31/how-to-setup-traefik-as-ingress-controller-to-expose-service-from-kubernetes/">边缘节点</a>（Edge Node），所谓的边缘节点即集群内部用来向集群外暴露服务能力的节点，集群外部的服务通过该节点来调用集群内部的服务，边缘节点是集群内外交流的一个Endpoint。</p><p>以上面部分部署时的配置来说，就是把Traefik部署的node通过Node Affinity固定在<code>ip-10-20-1-95</code>和<code>ip-10-20-1-96</code>这2个节点上了。</p><p><code>Pod</code>以<code>hostPort</code>方式运行，所以集群外可以通过<code>NODE_IP:PORT</code>访问到Traefik。</p><p>在外部秩序配置一个4层的load balancing，后端指向<code>ip-10-20-1-95</code>和<code>ip-10-20-1-96</code>这2个节点的Traefik的端口就可以了。</p><h4 id="使用Traefik-2-0的CRD-–-IngressRoute为一个HTTP和WebSocket后端服务配置路由的说明"><a href="#使用Traefik-2-0的CRD-–-IngressRoute为一个HTTP和WebSocket后端服务配置路由的说明" class="headerlink" title="使用Traefik 2.0的CRD – IngressRoute为一个HTTP和WebSocket后端服务配置路由的说明"></a>使用Traefik 2.0的CRD – IngressRoute为一个HTTP和WebSocket后端服务配置路由的说明</h4><ol><li><p>创建用于TLS证书的secret，注意secret中的文件名必须为固定值</p><p> 注意Traefik的IngressRoute必须和TLS的secret在同一namespace下才能获取到到secret。</p><pre class="line-numbers language-bash"><code class="language-bash"> <span class="token comment" spellcheck="true"># Create secret for Traffic ingressRoute to use</span> <span class="token comment" spellcheck="true"># Secret must be in the same namespace with ingressRoute</span> <span class="token comment" spellcheck="true"># The filename inside the secret must be 'tls.crt' and 'tls.key'</span> cat<span class="token operator">&lt;&lt;</span>-<span class="token string">'EOF'</span> <span class="token operator">|</span> kubectl apply -f - apiVersion: v1 kind: Secret metadata:   name: secret-name   namespace: namespace-name   <span class="token comment" spellcheck="true"># `type` is not needed to be specified</span>   <span class="token comment" spellcheck="true"># type: kubernetes.io/tls</span> data:   tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0<span class="token punctuation">..</span>.   tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVk<span class="token punctuation">..</span>. EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> 可选项，TLSStore可以定义一个默认的secret。</p><pre class="line-numbers language-bash"><code class="language-bash"> <span class="token comment" spellcheck="true"># This is optional, if you only use one cert, e.g. "*.example.com", you can use TLSStore to define a default TLS secret to be used in ingressRoute</span> cat<span class="token operator">&lt;&lt;</span>-<span class="token string">'EOF'</span> <span class="token operator">|</span> kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata:   name: tlsstore-name   namespace: namespace-name spec:   defaultCertificate:     secretName: secret-name EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>创建IngressRoute，用来配置Traefik中实际的路由规则。顺带吐槽以下，Traefik官网上关于配置的说明还是不够详细。</p><pre class="line-numbers language-bash"><code class="language-bash"> cat<span class="token operator">&lt;&lt;</span>-<span class="token string">'EOF'</span> <span class="token operator">|</span> kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata:   name: svc-your-app-http   namespace: namespace-name spec:   entryPoints:     - web   routes:     - match: Host<span class="token punctuation">(</span><span class="token variable"><span class="token variable">`</span>YOUR-APP.YOUR-DOMAIN.com.cn<span class="token variable">`</span></span><span class="token punctuation">)</span>       middlewares:         - name: svc-YOUR-APP-redirectscheme-to-https       kind: Rule       services:         - name: YOUR-APP           port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata:   name: svc-your-app-https   namespace: namespace-name spec:   entryPoints:     - websecure   routes:     - match: Host<span class="token punctuation">(</span><span class="token variable"><span class="token variable">`</span>YOUR-APP.YOUR-DOMAIN.com.cn<span class="token variable">`</span></span><span class="token punctuation">)</span>       middlewares:         - name: svc-YOUR-APP-headers       kind: Rule       services:         - name: YOUR-APP           port: 80   tls:     secretName: secret-name     domains:     - main: YOUR-DOMAIN.com.cn       sans:       - YOUR-APP.YOUR-DOMAIN.com.cn     <span class="token comment" spellcheck="true"># certResolver: default</span>     <span class="token comment" spellcheck="true"># options: {}</span> --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata:   name: svc-YOUR-APP-redirectscheme-to-https   namespace: namespace-name spec:   redirectScheme:     scheme: https     permanent: <span class="token boolean">true</span> --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata:   name: svc-YOUR-APP-headers spec:   headers:     customRequestHeaders:       X-Forwarded-Proto: <span class="token string">"https"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h3 id="Traefik-2-0功能以及使用场景简单总结"><a href="#Traefik-2-0功能以及使用场景简单总结" class="headerlink" title="Traefik 2.0功能以及使用场景简单总结"></a>Traefik 2.0功能以及使用场景简单总结</h3><p>Traefik 2.0 新增了万众期待的TCP、UDP代理，而且还可以配置相当灵活的转发规则，比如同一个<code>Host(app.example.com)</code>下，<code>/dashboard/</code>这个<code>path</code>转发到<code>web-ui</code>这个<code>Service</code>，而<code>/api/</code>转发到<code>api</code>这个Service。</p><p>还有各种内置<code>Middleware</code>可以使用，比如对请求头的操作、重定向、截取path、basicauth等。<br>总之就是一个功能强大的Application Load Balancing和Network Load Balancing了。</p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Traefik </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Change Rook Ceph filestore</title>
      <link href="2020/03/23/change-rook-ceph-filestore/"/>
      <url>2020/03/23/change-rook-ceph-filestore/</url>
      
        <content type="html"><![CDATA[<h1 id="Backgroud-reason"><a href="#Backgroud-reason" class="headerlink" title="Backgroud reason"></a>Backgroud reason</h1><p>After deploying Rook Ceph by the default configuration, I realized that the filestore was set to <code>filestore</code>, and I found the official suggestion as follows:</p><blockquote><p>The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.</p></blockquote><p>So, that’s because I use directory as OSD backend storage, so storeType is set to filestore.</p><p>And at the first time I created a block pool with <code>Replicated</code> settings, but I want to change to <code>Erasure Coded</code> settings for more storage capacity, because I’m using ceph in on-premises environment, not having much resources.</p><h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><h2 id="Check-if-any-resource-is-using-ceph-as-storage-backend"><a href="#Check-if-any-resource-is-using-ceph-as-storage-backend" class="headerlink" title="Check if any resource is using ceph as storage backend"></a>Check if any resource is using ceph as storage backend</h2><p>Check if pvc exist.</p><pre class="line-numbers language-bash"><code class="language-bash">$ kubectl get pvc --all-namespacesNAMESPACE   NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGEdefault     mysql-pv-claim   Bound    pvc-88499d1c-e910-4535-b97b-f9be5c3ea579   20Gi       RWO            rook-ceph-block   3d18hdefault     wp-pv-claim      Bound    pvc-bbd0cc4c-e5e6-4b83-b357-4368ee69728a   20Gi       RWO            rook-ceph-block   3d17h<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>Delete reources using pvc.</p><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token function">cd</span> manifest/apps/example/stateful/wordpress_ceph$ kubectl delete -f ./<span class="token function">service</span> <span class="token string">"wordpress-mysql"</span> deletedpersistentvolumeclaim <span class="token string">"mysql-pv-claim"</span> deleteddeployment.apps <span class="token string">"wordpress-mysql"</span> deleted<span class="token function">service</span> <span class="token string">"wordpress"</span> deletedpersistentvolumeclaim <span class="token string">"wp-pv-claim"</span> deleteddeployment.apps <span class="token string">"wordpress"</span> deleted<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Change-configuration-of-cluster-yaml"><a href="#Change-configuration-of-cluster-yaml" class="headerlink" title="Change configuration of cluster.yaml"></a>Change configuration of <code>cluster.yaml</code></h2><p>Change configuration of <code>cluster.yaml</code> <a href="https://github.com/rook/rook/blob/release-1.2/cluster/examples/kubernetes/ceph/cluster.yaml" target="_blank" rel="noopener">ceph cluster</a></p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">storage</span><span class="token punctuation">:</span>  <span class="token key atrule">useAllNodes</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">useAllDevices</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">deviceFilter</span><span class="token punctuation">:</span> <span class="token string">'^sd.'</span>   <span class="token comment" spellcheck="true"># The default settings are commented.</span>  <span class="token key atrule">config</span><span class="token punctuation">:</span>    <span class="token key atrule">storeType</span><span class="token punctuation">:</span> bluestore <span class="token comment" spellcheck="true"># The default settings are commented.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Maintenance-the-dedicated-nodes-for-ceph"><a href="#Maintenance-the-dedicated-nodes-for-ceph" class="headerlink" title="Maintenance the dedicated nodes for ceph"></a>Maintenance the dedicated nodes for ceph</h2><p>I’ve tainted these nodes to be dedicated nodes for Ceph.</p><ul><li>datateam-rookceph-01</li><li>datateam-rookceph-02</li><li>datateam-rookceph-03</li></ul><pre class="line-numbers language-bash"><code class="language-bash">$ kubectl cordon datateam-rookceph-01 <span class="token operator">&amp;&amp;</span> \   kubectl cordon datateam-rookceph-02 <span class="token operator">&amp;&amp;</span> \   kubectl cordon datateam-rookceph-03<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash">$ kubectl drain datateam-rookceph-01 <span class="token operator">&amp;&amp;</span> \   kubectl drain datateam-rookceph-02 <span class="token operator">&amp;&amp;</span> \   kubectl drain datateam-rookceph-03<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash">$ kubectl uncordon datateam-rookceph-01 <span class="token operator">&amp;&amp;</span> \   kubectl uncordon datateam-rookceph-02 <span class="token operator">&amp;&amp;</span> \   kubectl uncordon datateam-rookceph-03<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>I’ve upgrade kernel and enable BBR on dedicated nodes for Rook Ceph. Reference: <a href="https://www.vultr.com/docs/how-to-deploy-google-bbr-on-centos-7" target="_blank" rel="noopener">BBR</a></p><p>I’ve cleaned up the disks of the data on the nodes of Rook Ceph and increased the capacity.</p><h2 id="Reapply-the-Rook-Ceph-cluster-manifest"><a href="#Reapply-the-Rook-Ceph-cluster-manifest" class="headerlink" title="Reapply the Rook Ceph cluster manifest"></a>Reapply the Rook Ceph cluster manifest</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cd</span> manifest/apps/rook/kubectl apply cluster.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Observe that there are no abnormal objects in the Ceph namespace.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@datateam-k8s-control-plane-01 rook<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># kubectl get all -n rook-ceph</span>NAME                                                                 READY   STATUS             RESTARTS   AGE<span class="token punctuation">..</span>. Omitted here <span class="token punctuation">..</span>.pod/rook-ceph-mgr-a-5d4765dfb-l9clk                                  0/1     CrashLoopBackOff   7          20hpod/rook-ceph-mon-a-6864d58cc7-gkt4c                                 1/1     Running            0          20hpod/rook-ceph-mon-b-65fbf9b96c-k792m                                 1/1     Running            0          20hpod/rook-ceph-mon-c-54984d88b6-cbcz8                                 1/1     Running            0          20hpod/rook-ceph-operator-648d574f5c-m6452                              1/1     Running            0          2dpod/rook-ceph-osd-0-6f966cc46b-wg8gq                                 0/1     CrashLoopBackOff   7          20hpod/rook-ceph-osd-1-757cc69bc9-k6qbs                                 0/1     CrashLoopBackOff   7          20hpod/rook-ceph-osd-2-57b88cd98f-zkfp7                                 0/1     CrashLoopBackOff   7          20hNAME                                                            READY   UP-TO-DATE   AVAILABLE   AGE<span class="token punctuation">..</span>. Omitted here <span class="token punctuation">..</span>.deployment.apps/rook-ceph-mgr-a                                 0/1     1            0           25ddeployment.apps/rook-ceph-mon-a                                 1/1     1            1           25ddeployment.apps/rook-ceph-mon-b                                 1/1     1            1           25ddeployment.apps/rook-ceph-mon-c                                 1/1     1            1           25ddeployment.apps/rook-ceph-operator                              1/1     1            1           25ddeployment.apps/rook-ceph-osd-0                                 0/1     1            0           25ddeployment.apps/rook-ceph-osd-1                                 0/1     1            0           25ddeployment.apps/rook-ceph-osd-2                                 0/1     1            0           25ddeployment.apps/rook-ceph-tools                                 1/1     1            1           24d<span class="token punctuation">..</span>. Omitted here <span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>So, as we can see, ceph-osd does not start as expect.</p><pre class="line-numbers language-bash"><code class="language-bash">kubectl logs -f --tail<span class="token operator">=</span>100 pod/rook-ceph-mgr-a-5d4765dfb-l9clk -n rook-ceph<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Tear-down-Rook-Ceph"><a href="#Tear-down-Rook-Ceph" class="headerlink" title="Tear down Rook Ceph"></a>Tear down Rook Ceph</h3><p>After some investigation, I found that the Rook Operator can identify the nodes that need to deploy Ceph Cluster, which is in line with the expectations of taint and tolerance.</p><p>But somehow, the initialization process was not performed. I suspect that the device I prepared for blueStore was not recognized, but the setting of deviceFilter looks fine.</p><p>So I decided to tear down the entire Rook and redeploy it to see if it succeeded.<br>Here is the instruction of <a href="https://rook.github.io/docs/rook/v1.2/ceph-teardown.html" target="_blank" rel="noopener">tear down</a>.</p><p>One of the steps is very important. This involves cleaning up the partition table, and is Ceph’s standard for using disks as available devices.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cd</span> <span class="token variable">$HOME</span> <span class="token operator">&amp;&amp;</span> <span class="token function">mkdir</span> -pv hack <span class="token operator">&amp;&amp;</span> <span class="token function">cd</span> hackDISK<span class="token operator">=</span><span class="token string">"/dev/sdb"</span><span class="token function">tee</span> <span class="token variable">$HOME</span>/hack/reset_disk_usable_ceph.sh <span class="token operator">&lt;&lt;</span>-EOF<span class="token comment" spellcheck="true">#!/usr/bin/env bash</span>DISK<span class="token operator">=</span><span class="token string">"<span class="token variable">$DISK</span>"</span><span class="token comment" spellcheck="true"># Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)</span><span class="token comment" spellcheck="true"># You will have to run this step for all disks.</span>sgdisk --zap-all <span class="token variable">$DISK</span><span class="token comment" spellcheck="true"># These steps only have to be run once on each node</span><span class="token comment" spellcheck="true"># If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.</span><span class="token function">ls</span> /dev/mapper/ceph-* <span class="token operator">|</span> <span class="token function">xargs</span> -I% -- dmsetup remove %<span class="token comment" spellcheck="true"># ceph-volume setup can leave ceph-&lt;UUID> directories in /dev (unnecessary clutter)</span><span class="token function">rm</span> -rf /dev/ceph-*EOFsh ./reset_disk_usable_ceph.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Recreate-a-cluster-with-bluestore-as-storeType-backend"><a href="#Recreate-a-cluster-with-bluestore-as-storeType-backend" class="headerlink" title="Recreate a cluster with bluestore as storeType backend"></a>Recreate a cluster with bluestore as storeType backend</h3><p>Just use these yaml files.<br>Modification between these files and official example is image location and block storage devicefilter, nothing big deal.</p><p>Yaml files is located here: <a href="https://github.com/aruruka/laboratory/tree/master/k8s/manifest/apps/rook/ceph" target="_blank" rel="noopener">Rook-Ceph-blueStore</a>.</p><blockquote><p>In the “csi / rbd” directory, there are files with the words “ec” and “replicated”, respectively.<br>They represent block storage of two algorithms, <code>Erasure Coded</code> and <code>Replicated</code>.</p></blockquote><p><strong>Notice:</strong></p><blockquote><p>After apply the <code>storageclass-ec_customized.yaml</code>, we shall notice that the default PG(Placement Group) number of <code>ec-data-pool</code> is 8, this is too little and can see warn log on Ceph Dashboard.</p></blockquote><p>Here is an official tool and guidance of <a href="https://ceph.com/pgcalc/" target="_blank" rel="noopener">PG calculating</a>.<br>Take my example <a href="https://github.com/aruruka/laboratory/blob/master/k8s/manifest/apps/rook/ceph/csi/rbd/storageclass_1.2_ec_customized.yaml" target="_blank" rel="noopener">here</a>, it should be <code>128</code> PGs.</p><p>After recreating the cluster with blueStore as storeType backend, I use this command to verify the cluster status:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Get OSD Pods</span><span class="token comment" spellcheck="true"># This uses the example/default cluster name "rook-ceph"</span>OSD_PODS<span class="token operator">=</span><span class="token punctuation">$(</span>kubectl -n rook-ceph get pods -l \  app<span class="token operator">=</span>rook-ceph-osd,rook_cluster<span class="token operator">=</span>rook-ceph -o jsonpath<span class="token operator">=</span><span class="token string">'{.items[*].metadata.name}'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Find node and drive associations from OSD pods</span><span class="token keyword">for</span> pod <span class="token keyword">in</span> <span class="token variable"><span class="token variable">$(</span><span class="token keyword">echo</span> $<span class="token punctuation">{</span>OSD_PODS<span class="token punctuation">}</span><span class="token variable">)</span></span><span class="token keyword">do</span> <span class="token keyword">echo</span> <span class="token string">"Pod:  <span class="token variable">${pod}</span>"</span> <span class="token keyword">echo</span> <span class="token string">"Node: <span class="token variable"><span class="token variable">$(</span>kubectl -n rook-ceph get pod $<span class="token punctuation">{</span>pod<span class="token punctuation">}</span> -o jsonpath<span class="token operator">=</span>'<span class="token punctuation">{</span>.spec.nodeName<span class="token punctuation">}</span>'<span class="token variable">)</span></span>"</span> kubectl -n rook-ceph <span class="token function">exec</span> <span class="token variable">${pod}</span> -- sh -c <span class="token string">'\  for i in /var/lib/rook/osd*; do    [ -f <span class="token variable">${i}</span>/ready ] || continue    echo -ne "-<span class="token variable"><span class="token variable">$(</span><span class="token function">basename</span> $<span class="token punctuation">{</span>i<span class="token punctuation">}</span><span class="token variable">)</span></span> "    echo <span class="token variable"><span class="token variable">$(</span>lsblk -n -o NAME,SIZE $<span class="token punctuation">{</span>i<span class="token punctuation">}</span>/block 2<span class="token operator">></span> /dev/null <span class="token operator">||</span> \    findmnt -n -v -o SOURCE,SIZE -T $<span class="token punctuation">{</span>i<span class="token punctuation">}</span><span class="token variable">)</span></span> <span class="token variable"><span class="token variable">$(</span><span class="token function">cat</span> $<span class="token punctuation">{</span>i<span class="token punctuation">}</span>/type<span class="token variable">)</span></span>  done | sort -V  echo'</span><span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Nothing show up. :sweat_smile:</p><p>But I can see blueStore is enabled via Dashboard.</p><p><img src="osd_bluestore.png" alt="bluestore enabled" title="See, bluestore."></p><h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><p>As far as the latest(Release 1.2) version is concerned.<br>It is not supported to deploy more than one Ceph cluster in the Rook Cluster.<br>And when I tried to deploy another Ceph cluster in a new namespace, it turns up error of the CRD created by <code>common.yaml</code> and some Ceph plugin would need to be configured in deepth. I mean not only need to change the name of the namespace.<br>So, if you are not fimiliar with Ceph itself, you better plan early on what nodes to use to deploy Ceph, the size of the nodes, and how to expand the cluster.<br>Like my settings in the manifest, I use taints and node affinity to restrict Ceph deployed on specific nodes and use <code>/dev/[^sdb-i]</code> as bluestore devices.<br>I can add nodes or devices on nodes to adjust the scale of cluster.<br>And changing the PG number and <code>mon</code> amount may be also needed, it depends on the further research of utilization. </p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph Performance Testing</title>
      <link href="2020/03/23/ceph-performance-testing/"/>
      <url>2020/03/23/ceph-performance-testing/</url>
      
        <content type="html"><![CDATA[<h1 id="Performance-test-on-Erasure-Coded-Block-Storage"><a href="#Performance-test-on-Erasure-Coded-Block-Storage" class="headerlink" title="Performance test on Erasure Coded Block Storage"></a>Performance test on Erasure Coded Block Storage</h1><h2 id="Create-a-EC-pool"><a href="#Create-a-EC-pool" class="headerlink" title="Create a EC pool"></a>Create a EC pool</h2><p>I use this <a href="https://github.com/aruruka/laboratory/blob/master/k8s/manifest/apps/rook/ceph/csi/rbd/storageclass_1.2_ec_customized.yaml" target="_blank" rel="noopener">manifest</a> to create a EC pool.</p><pre class="line-numbers language-bash"><code class="language-bash">kubectl apply -f storageclass-ec_customized.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="Create-a-PVC"><a href="#Create-a-PVC" class="headerlink" title="Create a PVC"></a>Create a PVC</h2><p>Then create a PVC by this <a href="https://github.com/aruruka/laboratory/blob/master/k8s/manifest/apps/rook/ceph/test-pvc.yaml" target="_blank" rel="noopener">manifest</a>.</p><pre class="line-numbers language-bash"><code class="language-bash">kubectl apply -f test-pvc.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="Use-PVC-in-Deployment"><a href="#Use-PVC-in-Deployment" class="headerlink" title="Use PVC in Deployment"></a>Use PVC in Deployment</h2><p>Then redeploy the <a href="https://github.com/aruruka/laboratory/blob/master/k8s/manifest/apps/rook/ceph/toolbox_1.2_customized.yaml" target="_blank" rel="noopener">Toolbox</a>, mounting a volume provided by the PVC.</p><pre class="line-numbers language-bash"><code class="language-bash">kubectl apply -f toolbox_1.2_customized.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="Use-fio-to-test-the-EC-block-storage"><a href="#Use-fio-to-test-the-EC-block-storage" class="headerlink" title="Use fio to test the EC block storage"></a>Use <code>fio</code> to test the EC block storage</h2><p>Then you can enter the toolbox and use <code>fio</code> tool to test the performance.</p><blockquote><p><code>fio</code> could be simply installed within the pod.</p></blockquote><pre class="line-numbers language-bash"><code class="language-bash">kubectl -n rook-ceph <span class="token function">exec</span> -it <span class="token punctuation">$(</span>kubectl -n rook-ceph get pod -l <span class="token string">"app=rook-ceph-tools"</span> -o jsonpath<span class="token operator">=</span><span class="token string">'{.items[0].metadata.name}'</span><span class="token punctuation">)</span> <span class="token function">bash</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">touch</span> /tmp/ceph-rbd-ec-volume/testfio -filename<span class="token operator">=</span>/tmp/ceph-rbd-ec-volume/test -direct<span class="token operator">=</span>1 -iodepth<span class="token operator">=</span>128 -rw<span class="token operator">=</span>randrw -ioengine<span class="token operator">=</span>libaio -bs<span class="token operator">=</span>4k -size<span class="token operator">=</span>1G -numjobs<span class="token operator">=</span>8 -runtime<span class="token operator">=</span>100 -group_reporting -name<span class="token operator">=</span>Rand_Write_Testing<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Here is the result.</p><pre><code>[root@ip-10-30-0-205 ceph-rbd-ec-volume]# fio -filename=/tmp/ceph-rbd-ec-volume/test -direct=1 -iodepth=128 -rw=randrw -ioengine=libaio -bs=4k -size=1G -numjobs=8 -runtime=100 -group_reporting -name=Rand_Write_TestingRand_Write_Testing: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128...fio-3.7Starting 8 processesRand_Write_Testing: Laying out IO file (1 file / 1024MiB)Jobs: 8 (f=8): [m(8)][100.0%][r=1052KiB/s,w=956KiB/s][r=263,w=239 IOPS][eta 00m:00s]Rand_Write_Testing: (groupid=0, jobs=8): err= 0: pid=686: Thu Mar 19 11:55:54 2020   read: IOPS=490, BW=1961KiB/s (2008kB/s)(193MiB/100883msec)    slat (usec): min=2, max=1296.3k, avg=8108.82, stdev=34428.80    clat (msec): min=5, max=4560, avg=890.89, stdev=614.82     lat (msec): min=5, max=4560, avg=899.00, stdev=619.60    clat percentiles (msec):     |  1.00th=[  155],  5.00th=[  211], 10.00th=[  255], 20.00th=[  376],     | 30.00th=[  485], 40.00th=[  584], 50.00th=[  709], 60.00th=[  885],     | 70.00th=[ 1116], 80.00th=[ 1385], 90.00th=[ 1720], 95.00th=[ 2056],     | 99.00th=[ 2970], 99.50th=[ 3171], 99.90th=[ 3540], 99.95th=[ 3708],     | 99.99th=[ 4245]   bw (  KiB/s): min=    8, max= 1224, per=12.76%, avg=250.06, stdev=189.74, samples=1568   iops        : min=    2, max=  306, avg=62.47, stdev=47.44, samples=1568  write: IOPS=493, BW=1973KiB/s (2021kB/s)(194MiB/100883msec)    slat (usec): min=3, max=1285.1k, avg=8024.51, stdev=34210.52    clat (msec): min=26, max=5560, avg=1172.19, stdev=783.87     lat (msec): min=26, max=5560, avg=1180.22, stdev=788.17    clat percentiles (msec):     |  1.00th=[  207],  5.00th=[  279], 10.00th=[  347], 20.00th=[  493],     | 30.00th=[  651], 40.00th=[  785], 50.00th=[  953], 60.00th=[ 1183],     | 70.00th=[ 1469], 80.00th=[ 1804], 90.00th=[ 2232], 95.00th=[ 2735],     | 99.00th=[ 3574], 99.50th=[ 3842], 99.90th=[ 4329], 99.95th=[ 4597],     | 99.99th=[ 5134]   bw (  KiB/s): min=    7, max= 1280, per=12.70%, avg=250.67, stdev=191.16, samples=1569   iops        : min=    1, max=  320, avg=62.62, stdev=47.79, samples=1569  lat (msec)   : 10=0.01%, 20=0.02%, 50=0.01%, 100=0.06%, 250=6.04%  lat (msec)   : 500=19.84%, 750=19.18%, 1000=13.61%  cpu          : usr=0.06%, sys=0.21%, ctx=23443, majf=0, minf=100  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.3%, &gt;=64=99.5%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%     issued rwts: total=49451,49772,0,0 short=0,0,0,0 dropped=0,0,0,0     latency   : target=0, window=0, percentile=100.00%, depth=128Run status group 0 (all jobs):   READ: bw=1961KiB/s (2008kB/s), 1961KiB/s-1961KiB/s (2008kB/s-2008kB/s), io=193MiB (203MB), run=100883-100883msec  WRITE: bw=1973KiB/s (2021kB/s), 1973KiB/s-1973KiB/s (2021kB/s-2021kB/s), io=194MiB (204MB), run=100883-100883msecDisk stats (read/write):  rbd0: ios=49447/49721, merge=1/44, ticks=5840992/19554655, in_queue=25345828, util=33.23%</code></pre><h1 id="Performance-test-on-Replicated-Block-Storage"><a href="#Performance-test-on-Replicated-Block-Storage" class="headerlink" title="Performance test on Replicated Block Storage"></a>Performance test on Replicated Block Storage</h1><h2 id="Create-a-Replicated-pool"><a href="#Create-a-Replicated-pool" class="headerlink" title="Create a Replicated pool"></a>Create a Replicated pool</h2><p>I use this <a href="https://github.com/aruruka/laboratory/blob/master/k8s/manifest/apps/rook/ceph/csi/rbd/storageclass_1.2_replicated_customized.yaml" target="_blank" rel="noopener">manifest</a> to create the Replicated block pool.</p><pre class="line-numbers language-bash"><code class="language-bash">kubectl apply -f storageclass_1.2_replicated_customized.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="Create-a-PVC-and-use-it-in-Deployment-with-the-same-method-mentioned-above"><a href="#Create-a-PVC-and-use-it-in-Deployment-with-the-same-method-mentioned-above" class="headerlink" title="Create a PVC and use it in Deployment with the same method mentioned above."></a>Create a PVC and use it in Deployment with the same method mentioned above.</h2><blockquote><p>Change the name of PVC and StorageClass accordingly.</p></blockquote><pre class="line-numbers language-bash"><code class="language-bash">kubectl apply -f test-pvc.yamlkubectl apply -f toolbox_1.2_customized.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="Use-fio-to-test-the-Replicated-Block-storage"><a href="#Use-fio-to-test-the-Replicated-Block-storage" class="headerlink" title="Use fio to test the Replicated Block storage"></a>Use <code>fio</code> to test the Replicated Block storage</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@ip-10-30-0-174 /<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># touch /tmp/ceph_rbd_replicated_test_volume/replicated_block_test.tmp</span><span class="token punctuation">[</span>root@ip-10-30-0-174 /<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># which fio</span>/usr/bin/which: no fio <span class="token keyword">in</span> <span class="token punctuation">(</span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin<span class="token punctuation">)</span><span class="token punctuation">[</span>root@ip-10-30-0-174 /<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># yum install fio</span>Failed to <span class="token keyword">set</span> locale, defaulting to CLoaded plugins: fastestmirror, ovlDetermining fastest mirrors<span class="token punctuation">..</span>.omitted here<span class="token punctuation">..</span>.Installed:  fio.x86_64 0:3.7-1.el7Dependency Installed:  daxctl-libs.x86_64 0:64.1-2.el7    libpmem.x86_64 0:1.5.1-2.1.el7    libpmemblk.x86_64 0:1.5.1-2.1.el7    ndctl-libs.x86_64 0:64.1-2.el7    numactl-libs.x86_64 0:2.0.12-3.el7_7.1Complete<span class="token operator">!</span><span class="token punctuation">[</span>root@ip-10-30-0-174 ceph_rbd_replicated_test_volume<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># fio -filename=/tmp/ceph_rbd_replicated_test_volume/replicated_block_test.tmp -direct=1 -iodepth=128 -rw=randrw -ioengine=libaio -bs=4k -size=8G -numjobs=8 -runtime=100 -group_reporting -name=Rand_Write_Testing</span>Rand_Write_Testing: <span class="token punctuation">(</span>g<span class="token operator">=</span>0<span class="token punctuation">)</span>: rw<span class="token operator">=</span>randrw, bs<span class="token operator">=</span><span class="token punctuation">(</span>R<span class="token punctuation">)</span> 4096B-4096B, <span class="token punctuation">(</span>W<span class="token punctuation">)</span> 4096B-4096B, <span class="token punctuation">(</span>T<span class="token punctuation">)</span> 4096B-4096B, ioengine<span class="token operator">=</span>libaio, iodepth<span class="token operator">=</span>128<span class="token punctuation">..</span>.fio-3.7Starting 8 processesRand_Write_Testing: Laying out IO <span class="token function">file</span> <span class="token punctuation">(</span>1 <span class="token function">file</span> / 8192MiB<span class="token punctuation">)</span>Jobs: 8 <span class="token punctuation">(</span>f<span class="token operator">=</span>8<span class="token punctuation">)</span>: <span class="token punctuation">[</span>m<span class="token punctuation">(</span>8<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>100.0%<span class="token punctuation">]</span><span class="token punctuation">[</span>r<span class="token operator">=</span>4504KiB/s,w<span class="token operator">=</span>4536KiB/s<span class="token punctuation">]</span><span class="token punctuation">[</span>r<span class="token operator">=</span>1126,w<span class="token operator">=</span>1134 IOPS<span class="token punctuation">]</span><span class="token punctuation">[</span>eta 00m:00s<span class="token punctuation">]</span>Rand_Write_Testing: <span class="token punctuation">(</span>groupid<span class="token operator">=</span>0, jobs<span class="token operator">=</span>8<span class="token punctuation">)</span>: err<span class="token operator">=</span> 0: pid<span class="token operator">=</span>2058: Mon Mar 23 08:13:04 2020   read: IOPS<span class="token operator">=</span>890, BW<span class="token operator">=</span>3560KiB/s <span class="token punctuation">(</span>3646kB/s<span class="token punctuation">)</span><span class="token punctuation">(</span>348MiB/100169msec<span class="token punctuation">)</span>    slat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>3, max<span class="token operator">=</span>759415, avg<span class="token operator">=</span>4397.68, stdev<span class="token operator">=</span>19203.03    clat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>: min<span class="token operator">=</span>2, max<span class="token operator">=</span>3702, avg<span class="token operator">=</span>500.83, stdev<span class="token operator">=</span>322.63     lat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>: min<span class="token operator">=</span>2, max<span class="token operator">=</span>3719, avg<span class="token operator">=</span>505.22, stdev<span class="token operator">=</span>324.59    clat percentiles <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>:     <span class="token operator">|</span>  1.00th<span class="token operator">=</span><span class="token punctuation">[</span>  167<span class="token punctuation">]</span>,  5.00th<span class="token operator">=</span><span class="token punctuation">[</span>  222<span class="token punctuation">]</span>, 10.00th<span class="token operator">=</span><span class="token punctuation">[</span>  255<span class="token punctuation">]</span>, 20.00th<span class="token operator">=</span><span class="token punctuation">[</span>  305<span class="token punctuation">]</span>,     <span class="token operator">|</span> 30.00th<span class="token operator">=</span><span class="token punctuation">[</span>  338<span class="token punctuation">]</span>, 40.00th<span class="token operator">=</span><span class="token punctuation">[</span>  372<span class="token punctuation">]</span>, 50.00th<span class="token operator">=</span><span class="token punctuation">[</span>  405<span class="token punctuation">]</span>, 60.00th<span class="token operator">=</span><span class="token punctuation">[</span>  447<span class="token punctuation">]</span>,     <span class="token operator">|</span> 70.00th<span class="token operator">=</span><span class="token punctuation">[</span>  506<span class="token punctuation">]</span>, 80.00th<span class="token operator">=</span><span class="token punctuation">[</span>  609<span class="token punctuation">]</span>, 90.00th<span class="token operator">=</span><span class="token punctuation">[</span>  860<span class="token punctuation">]</span>, 95.00th<span class="token operator">=</span><span class="token punctuation">[</span> 1183<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.00th<span class="token operator">=</span><span class="token punctuation">[</span> 1770<span class="token punctuation">]</span>, 99.50th<span class="token operator">=</span><span class="token punctuation">[</span> 2072<span class="token punctuation">]</span>, 99.90th<span class="token operator">=</span><span class="token punctuation">[</span> 2769<span class="token punctuation">]</span>, 99.95th<span class="token operator">=</span><span class="token punctuation">[</span> 3071<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.99th<span class="token operator">=</span><span class="token punctuation">[</span> 3507<span class="token punctuation">]</span>   bw <span class="token punctuation">(</span>  KiB/s<span class="token punctuation">)</span>: min<span class="token operator">=</span>    8, max<span class="token operator">=</span> 1120, per<span class="token operator">=</span>12.52%, avg<span class="token operator">=</span>445.55, stdev<span class="token operator">=</span>211.44, samples<span class="token operator">=</span>1592   iops        <span class="token keyword">:</span> min<span class="token operator">=</span>    2, max<span class="token operator">=</span>  280, avg<span class="token operator">=</span>111.33, stdev<span class="token operator">=</span>52.87, samples<span class="token operator">=</span>1592  write: IOPS<span class="token operator">=</span>890, BW<span class="token operator">=</span>3562KiB/s <span class="token punctuation">(</span>3647kB/s<span class="token punctuation">)</span><span class="token punctuation">(</span>348MiB/100169msec<span class="token punctuation">)</span>    slat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>5, max<span class="token operator">=</span>857972, avg<span class="token operator">=</span>4564.23, stdev<span class="token operator">=</span>19536.89    clat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>: min<span class="token operator">=</span>4, max<span class="token operator">=</span>4155, avg<span class="token operator">=</span>638.70, stdev<span class="token operator">=</span>385.95     lat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>: min<span class="token operator">=</span>4, max<span class="token operator">=</span>4155, avg<span class="token operator">=</span>643.27, stdev<span class="token operator">=</span>387.54    clat percentiles <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>:     <span class="token operator">|</span>  1.00th<span class="token operator">=</span><span class="token punctuation">[</span>  211<span class="token punctuation">]</span>,  5.00th<span class="token operator">=</span><span class="token punctuation">[</span>  279<span class="token punctuation">]</span>, 10.00th<span class="token operator">=</span><span class="token punctuation">[</span>  326<span class="token punctuation">]</span>, 20.00th<span class="token operator">=</span><span class="token punctuation">[</span>  384<span class="token punctuation">]</span>,     <span class="token operator">|</span> 30.00th<span class="token operator">=</span><span class="token punctuation">[</span>  430<span class="token punctuation">]</span>, 40.00th<span class="token operator">=</span><span class="token punctuation">[</span>  481<span class="token punctuation">]</span>, 50.00th<span class="token operator">=</span><span class="token punctuation">[</span>  531<span class="token punctuation">]</span>, 60.00th<span class="token operator">=</span><span class="token punctuation">[</span>  592<span class="token punctuation">]</span>,     <span class="token operator">|</span> 70.00th<span class="token operator">=</span><span class="token punctuation">[</span>  667<span class="token punctuation">]</span>, 80.00th<span class="token operator">=</span><span class="token punctuation">[</span>  793<span class="token punctuation">]</span>, 90.00th<span class="token operator">=</span><span class="token punctuation">[</span> 1099<span class="token punctuation">]</span>, 95.00th<span class="token operator">=</span><span class="token punctuation">[</span> 1452<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.00th<span class="token operator">=</span><span class="token punctuation">[</span> 2165<span class="token punctuation">]</span>, 99.50th<span class="token operator">=</span><span class="token punctuation">[</span> 2500<span class="token punctuation">]</span>, 99.90th<span class="token operator">=</span><span class="token punctuation">[</span> 3138<span class="token punctuation">]</span>, 99.95th<span class="token operator">=</span><span class="token punctuation">[</span> 3373<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.99th<span class="token operator">=</span><span class="token punctuation">[</span> 3742<span class="token punctuation">]</span>   bw <span class="token punctuation">(</span>  KiB/s<span class="token punctuation">)</span>: min<span class="token operator">=</span>    7, max<span class="token operator">=</span> 1008, per<span class="token operator">=</span>12.51%, avg<span class="token operator">=</span>445.52, stdev<span class="token operator">=</span>204.63, samples<span class="token operator">=</span>1591   iops        <span class="token keyword">:</span> min<span class="token operator">=</span>    1, max<span class="token operator">=</span>  252, avg<span class="token operator">=</span>111.32, stdev<span class="token operator">=</span>51.17, samples<span class="token operator">=</span>1591  lat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>   <span class="token keyword">:</span> 4<span class="token operator">=</span>0.01%, 10<span class="token operator">=</span>0.01%, 20<span class="token operator">=</span>0.01%, 50<span class="token operator">=</span>0.01%, 100<span class="token operator">=</span>0.06%  lat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>   <span class="token keyword">:</span> 250<span class="token operator">=</span>5.86%, 500<span class="token operator">=</span>50.73%, 750<span class="token operator">=</span>25.42%, 1000<span class="token operator">=</span>8.22%  cpu          <span class="token keyword">:</span> usr<span class="token operator">=</span>0.10%, sys<span class="token operator">=</span>0.34%, ctx<span class="token operator">=</span>47997, majf<span class="token operator">=</span>0, minf<span class="token operator">=</span>262  IO depths    <span class="token keyword">:</span> 1<span class="token operator">=</span>0.1%, 2<span class="token operator">=</span>0.1%, 4<span class="token operator">=</span>0.1%, 8<span class="token operator">=</span>0.1%, 16<span class="token operator">=</span>0.1%, 32<span class="token operator">=</span>0.1%, <span class="token operator">>=</span>64<span class="token operator">=</span>99.7%     submit    <span class="token keyword">:</span> 0<span class="token operator">=</span>0.0%, 4<span class="token operator">=</span>100.0%, 8<span class="token operator">=</span>0.0%, 16<span class="token operator">=</span>0.0%, 32<span class="token operator">=</span>0.0%, 64<span class="token operator">=</span>0.0%, <span class="token operator">>=</span>64<span class="token operator">=</span>0.0%     complete  <span class="token keyword">:</span> 0<span class="token operator">=</span>0.0%, 4<span class="token operator">=</span>100.0%, 8<span class="token operator">=</span>0.0%, 16<span class="token operator">=</span>0.0%, 32<span class="token operator">=</span>0.0%, 64<span class="token operator">=</span>0.0%, <span class="token operator">>=</span>64<span class="token operator">=</span>0.1%     issued rwts: total<span class="token operator">=</span>89153,89189,0,0 short<span class="token operator">=</span>0,0,0,0 dropped<span class="token operator">=</span>0,0,0,0     latency   <span class="token keyword">:</span> target<span class="token operator">=</span>0, window<span class="token operator">=</span>0, percentile<span class="token operator">=</span>100.00%, depth<span class="token operator">=</span>128Run status group 0 <span class="token punctuation">(</span>all jobs<span class="token punctuation">)</span>:   READ: bw<span class="token operator">=</span>3560KiB/s <span class="token punctuation">(</span>3646kB/s<span class="token punctuation">)</span>, 3560KiB/s-3560KiB/s <span class="token punctuation">(</span>3646kB/s-3646kB/s<span class="token punctuation">)</span>, io<span class="token operator">=</span>348MiB <span class="token punctuation">(</span>365MB<span class="token punctuation">)</span>, run<span class="token operator">=</span>100169-100169msec  WRITE: bw<span class="token operator">=</span>3562KiB/s <span class="token punctuation">(</span>3647kB/s<span class="token punctuation">)</span>, 3562KiB/s-3562KiB/s <span class="token punctuation">(</span>3647kB/s-3647kB/s<span class="token punctuation">)</span>, io<span class="token operator">=</span>348MiB <span class="token punctuation">(</span>365MB<span class="token punctuation">)</span>, run<span class="token operator">=</span>100169-100169msecDisk stats <span class="token punctuation">(</span>read/write<span class="token punctuation">)</span>:  rbd0: ios<span class="token operator">=</span>89146/89153, merge<span class="token operator">=</span>1/16, ticks<span class="token operator">=</span>6813188/18485213, in_queue<span class="token operator">=</span>12813419, util<span class="token operator">=</span>100.00%<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Performance-test-on-host"><a href="#Performance-test-on-host" class="headerlink" title="Performance test on host"></a>Performance test on host</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@ip-10-30-0-174 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># df -Th /data</span>Filesystem     Type  Size  Used Avail Use% Mounted on/dev/sdb1      xfs    16G  2.8G   14G  18% /data<span class="token punctuation">[</span>root@ip-10-30-0-174 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># fio -filename=/data/xfs_fio_test.tmp -direct=1 -iodepth=128 -rw=randrw -ioengine=libaio -bs=4k -size=8G -numjobs=8 -runtime=100 -group_reporting -name=Rand_Write_Testing</span>Rand_Write_Testing: <span class="token punctuation">(</span>g<span class="token operator">=</span>0<span class="token punctuation">)</span>: rw<span class="token operator">=</span>randrw, bs<span class="token operator">=</span><span class="token punctuation">(</span>R<span class="token punctuation">)</span> 4096B-4096B, <span class="token punctuation">(</span>W<span class="token punctuation">)</span> 4096B-4096B, <span class="token punctuation">(</span>T<span class="token punctuation">)</span> 4096B-4096B, ioengine<span class="token operator">=</span>libaio, iodepth<span class="token operator">=</span>128<span class="token punctuation">..</span>.fio-3.7Starting 8 processesRand_Write_Testing: Laying out IO <span class="token function">file</span> <span class="token punctuation">(</span>1 <span class="token function">file</span> / 8192MiB<span class="token punctuation">)</span>Jobs: 8 <span class="token punctuation">(</span>f<span class="token operator">=</span>8<span class="token punctuation">)</span>: <span class="token punctuation">[</span>m<span class="token punctuation">(</span>8<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>100.0%<span class="token punctuation">]</span><span class="token punctuation">[</span>r<span class="token operator">=</span>13.4MiB/s,w<span class="token operator">=</span>13.4MiB/s<span class="token punctuation">]</span><span class="token punctuation">[</span>r<span class="token operator">=</span>3434,w<span class="token operator">=</span>3432 IOPS<span class="token punctuation">]</span><span class="token punctuation">[</span>eta 00m:00s<span class="token punctuation">]</span>Rand_Write_Testing: <span class="token punctuation">(</span>groupid<span class="token operator">=</span>0, jobs<span class="token operator">=</span>8<span class="token punctuation">)</span>: err<span class="token operator">=</span> 0: pid<span class="token operator">=</span>11598: Mon Mar 23 16:05:17 2020   read: IOPS<span class="token operator">=</span>3361, BW<span class="token operator">=</span>13.1MiB/s <span class="token punctuation">(</span>13.8MB/s<span class="token punctuation">)</span><span class="token punctuation">(</span>1315MiB/100153msec<span class="token punctuation">)</span>    slat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>3, max<span class="token operator">=</span>402473, avg<span class="token operator">=</span>1075.48, stdev<span class="token operator">=</span>6513.03    clat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>925, max<span class="token operator">=</span>1970.6k, avg<span class="token operator">=</span>171791.88, stdev<span class="token operator">=</span>108593.36     lat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>931, max<span class="token operator">=</span>1970.7k, avg<span class="token operator">=</span>172867.62, stdev<span class="token operator">=</span>108897.86    clat percentiles <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>:     <span class="token operator">|</span>  1.00th<span class="token operator">=</span><span class="token punctuation">[</span>   51<span class="token punctuation">]</span>,  5.00th<span class="token operator">=</span><span class="token punctuation">[</span>   66<span class="token punctuation">]</span>, 10.00th<span class="token operator">=</span><span class="token punctuation">[</span>   79<span class="token punctuation">]</span>, 20.00th<span class="token operator">=</span><span class="token punctuation">[</span>   96<span class="token punctuation">]</span>,     <span class="token operator">|</span> 30.00th<span class="token operator">=</span><span class="token punctuation">[</span>  111<span class="token punctuation">]</span>, 40.00th<span class="token operator">=</span><span class="token punctuation">[</span>  126<span class="token punctuation">]</span>, 50.00th<span class="token operator">=</span><span class="token punctuation">[</span>  142<span class="token punctuation">]</span>, 60.00th<span class="token operator">=</span><span class="token punctuation">[</span>  163<span class="token punctuation">]</span>,     <span class="token operator">|</span> 70.00th<span class="token operator">=</span><span class="token punctuation">[</span>  188<span class="token punctuation">]</span>, 80.00th<span class="token operator">=</span><span class="token punctuation">[</span>  230<span class="token punctuation">]</span>, 90.00th<span class="token operator">=</span><span class="token punctuation">[</span>  300<span class="token punctuation">]</span>, 95.00th<span class="token operator">=</span><span class="token punctuation">[</span>  372<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.00th<span class="token operator">=</span><span class="token punctuation">[</span>  592<span class="token punctuation">]</span>, 99.50th<span class="token operator">=</span><span class="token punctuation">[</span>  718<span class="token punctuation">]</span>, 99.90th<span class="token operator">=</span><span class="token punctuation">[</span>  953<span class="token punctuation">]</span>, 99.95th<span class="token operator">=</span><span class="token punctuation">[</span> 1045<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.99th<span class="token operator">=</span><span class="token punctuation">[</span> 1250<span class="token punctuation">]</span>   bw <span class="token punctuation">(</span>  KiB/s<span class="token punctuation">)</span>: min<span class="token operator">=</span>   56, max<span class="token operator">=</span> 3688, per<span class="token operator">=</span>12.50%, avg<span class="token operator">=</span>1680.24, stdev<span class="token operator">=</span>550.35, samples<span class="token operator">=</span>1600   iops        <span class="token keyword">:</span> min<span class="token operator">=</span>   14, max<span class="token operator">=</span>  922, avg<span class="token operator">=</span>420.04, stdev<span class="token operator">=</span>137.58, samples<span class="token operator">=</span>1600  write: IOPS<span class="token operator">=</span>3354, BW<span class="token operator">=</span>13.1MiB/s <span class="token punctuation">(</span>13.7MB/s<span class="token punctuation">)</span><span class="token punctuation">(</span>1312MiB/100153msec<span class="token punctuation">)</span>    slat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>4, max<span class="token operator">=</span>456740, avg<span class="token operator">=</span>1297.14, stdev<span class="token operator">=</span>7098.63    clat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>871, max<span class="token operator">=</span>777913, avg<span class="token operator">=</span>130293.71, stdev<span class="token operator">=</span>64597.51     lat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>: min<span class="token operator">=</span>877, max<span class="token operator">=</span>778115, avg<span class="token operator">=</span>131591.15, stdev<span class="token operator">=</span>65089.13    clat percentiles <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>:     <span class="token operator">|</span>  1.00th<span class="token operator">=</span><span class="token punctuation">[</span>   46<span class="token punctuation">]</span>,  5.00th<span class="token operator">=</span><span class="token punctuation">[</span>   60<span class="token punctuation">]</span>, 10.00th<span class="token operator">=</span><span class="token punctuation">[</span>   69<span class="token punctuation">]</span>, 20.00th<span class="token operator">=</span><span class="token punctuation">[</span>   83<span class="token punctuation">]</span>,     <span class="token operator">|</span> 30.00th<span class="token operator">=</span><span class="token punctuation">[</span>   94<span class="token punctuation">]</span>, 40.00th<span class="token operator">=</span><span class="token punctuation">[</span>  105<span class="token punctuation">]</span>, 50.00th<span class="token operator">=</span><span class="token punctuation">[</span>  115<span class="token punctuation">]</span>, 60.00th<span class="token operator">=</span><span class="token punctuation">[</span>  127<span class="token punctuation">]</span>,     <span class="token operator">|</span> 70.00th<span class="token operator">=</span><span class="token punctuation">[</span>  142<span class="token punctuation">]</span>, 80.00th<span class="token operator">=</span><span class="token punctuation">[</span>  167<span class="token punctuation">]</span>, 90.00th<span class="token operator">=</span><span class="token punctuation">[</span>  215<span class="token punctuation">]</span>, 95.00th<span class="token operator">=</span><span class="token punctuation">[</span>  255<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.00th<span class="token operator">=</span><span class="token punctuation">[</span>  355<span class="token punctuation">]</span>, 99.50th<span class="token operator">=</span><span class="token punctuation">[</span>  380<span class="token punctuation">]</span>, 99.90th<span class="token operator">=</span><span class="token punctuation">[</span>  609<span class="token punctuation">]</span>, 99.95th<span class="token operator">=</span><span class="token punctuation">[</span>  676<span class="token punctuation">]</span>,     <span class="token operator">|</span> 99.99th<span class="token operator">=</span><span class="token punctuation">[</span>  751<span class="token punctuation">]</span>   bw <span class="token punctuation">(</span>  KiB/s<span class="token punctuation">)</span>: min<span class="token operator">=</span>   56, max<span class="token operator">=</span> 3848, per<span class="token operator">=</span>12.50%, avg<span class="token operator">=</span>1677.35, stdev<span class="token operator">=</span>558.50, samples<span class="token operator">=</span>1600   iops        <span class="token keyword">:</span> min<span class="token operator">=</span>   14, max<span class="token operator">=</span>  962, avg<span class="token operator">=</span>419.32, stdev<span class="token operator">=</span>139.62, samples<span class="token operator">=</span>1600  lat <span class="token punctuation">(</span>usec<span class="token punctuation">)</span>   <span class="token keyword">:</span> 1000<span class="token operator">=</span>0.01%  lat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>   <span class="token keyword">:</span> 2<span class="token operator">=</span>0.01%, 10<span class="token operator">=</span>0.01%, 20<span class="token operator">=</span>0.01%, 50<span class="token operator">=</span>1.35%, 100<span class="token operator">=</span>27.53%  lat <span class="token punctuation">(</span>msec<span class="token punctuation">)</span>   <span class="token keyword">:</span> 250<span class="token operator">=</span>60.19%, 500<span class="token operator">=</span>9.97%, 750<span class="token operator">=</span>0.72%, 1000<span class="token operator">=</span>0.18%  cpu          <span class="token keyword">:</span> usr<span class="token operator">=</span>0.29%, sys<span class="token operator">=</span>1.46%, ctx<span class="token operator">=</span>689469, majf<span class="token operator">=</span>0, minf<span class="token operator">=</span>267  IO depths    <span class="token keyword">:</span> 1<span class="token operator">=</span>0.1%, 2<span class="token operator">=</span>0.1%, 4<span class="token operator">=</span>0.1%, 8<span class="token operator">=</span>0.1%, 16<span class="token operator">=</span>0.1%, 32<span class="token operator">=</span>0.1%, <span class="token operator">>=</span>64<span class="token operator">=</span>99.9%     submit    <span class="token keyword">:</span> 0<span class="token operator">=</span>0.0%, 4<span class="token operator">=</span>100.0%, 8<span class="token operator">=</span>0.0%, 16<span class="token operator">=</span>0.0%, 32<span class="token operator">=</span>0.0%, 64<span class="token operator">=</span>0.0%, <span class="token operator">>=</span>64<span class="token operator">=</span>0.0%     complete  <span class="token keyword">:</span> 0<span class="token operator">=</span>0.0%, 4<span class="token operator">=</span>100.0%, 8<span class="token operator">=</span>0.0%, 16<span class="token operator">=</span>0.0%, 32<span class="token operator">=</span>0.0%, 64<span class="token operator">=</span>0.0%, <span class="token operator">>=</span>64<span class="token operator">=</span>0.1%     issued rwts: total<span class="token operator">=</span>336703,335963,0,0 short<span class="token operator">=</span>0,0,0,0 dropped<span class="token operator">=</span>0,0,0,0     latency   <span class="token keyword">:</span> target<span class="token operator">=</span>0, window<span class="token operator">=</span>0, percentile<span class="token operator">=</span>100.00%, depth<span class="token operator">=</span>128Run status group 0 <span class="token punctuation">(</span>all jobs<span class="token punctuation">)</span>:   READ: bw<span class="token operator">=</span>13.1MiB/s <span class="token punctuation">(</span>13.8MB/s<span class="token punctuation">)</span>, 13.1MiB/s-13.1MiB/s <span class="token punctuation">(</span>13.8MB/s-13.8MB/s<span class="token punctuation">)</span>, io<span class="token operator">=</span>1315MiB <span class="token punctuation">(</span>1379MB<span class="token punctuation">)</span>, run<span class="token operator">=</span>100153-100153msec  WRITE: bw<span class="token operator">=</span>13.1MiB/s <span class="token punctuation">(</span>13.7MB/s<span class="token punctuation">)</span>, 13.1MiB/s-13.1MiB/s <span class="token punctuation">(</span>13.7MB/s-13.7MB/s<span class="token punctuation">)</span>, io<span class="token operator">=</span>1312MiB <span class="token punctuation">(</span>1376MB<span class="token punctuation">)</span>, run<span class="token operator">=</span>100153-100153msecDisk stats <span class="token punctuation">(</span>read/write<span class="token punctuation">)</span>:  sdb: ios<span class="token operator">=</span>336432/335776, merge<span class="token operator">=</span>0/1, ticks<span class="token operator">=</span>14384038/863203, in_queue<span class="token operator">=</span>15265429, util<span class="token operator">=</span>100.00%<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Test-Pod-network’s-bandwidth"><a href="#Test-Pod-network’s-bandwidth" class="headerlink" title="Test Pod network’s bandwidth"></a>Test Pod network’s bandwidth</h1><h2 id="Create-first-pod-to-serve-on-port-3390"><a href="#Create-first-pod-to-serve-on-port-3390" class="headerlink" title="Create first pod to serve on port 3390"></a>Create first pod to serve on port 3390</h2><pre class="line-numbers language-bash"><code class="language-bash">cat<span class="token operator">&lt;&lt;</span>EOF <span class="token operator">|</span> kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: busybox  namespace: defaultspec:  containers:  - name: busybox    image: harbor.sunvalley.com.cn/library/centos:zhangguanzhang    command:      - <span class="token function">sleep</span>      - <span class="token string">"3600"</span>    imagePullPolicy: IfNotPresent  restartPolicy: AlwaysEOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash">kubectl <span class="token function">exec</span> -ti busybox <span class="token function">bash</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@busybox /<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># iperf3 -s -p 3390</span>-----------------------------------------------------------Server listening on 3390-----------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Create-second-pod-to-send-packets-to-the-first-pod"><a href="#Create-second-pod-to-send-packets-to-the-first-pod" class="headerlink" title="Create second pod to send packets to the first pod"></a>Create second pod to send packets to the first pod</h2><pre class="line-numbers language-bash"><code class="language-bash">cat<span class="token operator">&lt;&lt;</span>EOF <span class="token operator">|</span> kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: busybox2  namespace: defaultspec:  containers:  - name: busybox    image: harbor.sunvalley.com.cn/library/centos:zhangguanzhang    command:      - <span class="token function">sleep</span>      - <span class="token string">"3600"</span>    imagePullPolicy: IfNotPresent  restartPolicy: AlwaysEOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>Through the following command, we know that the IP of Pod1 is <strong>10.200.0.193</strong>.</p></blockquote><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@datateam-k8s-control-plane-01 ceph<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># kubectl get pod busybox -o wide</span>NAME      READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATESbusybox   1/1     Running   3          3h42m   10.200.0.193   ip-10-20-1-84   <span class="token operator">&lt;</span>none<span class="token operator">></span>           <span class="token operator">&lt;</span>none<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash">kubectl <span class="token function">exec</span> -ti busybox2 <span class="token function">bash</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@busybox2 /<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># iperf3 -c 10.200.0.193 -p 3390</span>Connecting to host 10.200.0.193, port 3390<span class="token punctuation">[</span>  4<span class="token punctuation">]</span> local 10.200.0.75 port 44990 connected to 10.200.0.193 port 3390<span class="token punctuation">[</span> ID<span class="token punctuation">]</span> Interval           Transfer     Bandwidth       Retr  Cwnd<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   0.00-1.00   sec   108 MBytes   909 Mbits/sec   59    180 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   1.00-2.00   sec   108 MBytes   905 Mbits/sec    8    279 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   2.00-3.00   sec   106 MBytes   892 Mbits/sec   25    214 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   3.00-4.00   sec   104 MBytes   875 Mbits/sec   23    221 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   4.00-5.00   sec   106 MBytes   886 Mbits/sec   48    160 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   5.00-6.00   sec   108 MBytes   902 Mbits/sec   24    247 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   6.00-7.00   sec   102 MBytes   860 Mbits/sec    9    192 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   7.00-8.00   sec   103 MBytes   866 Mbits/sec   25    308 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   8.00-9.00   sec   107 MBytes   901 Mbits/sec   85    203 KBytes<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   9.00-10.00  sec   106 MBytes   890 Mbits/sec   16    302 KBytes- - - - - - - - - - - - - - - - - - - - - - - - -<span class="token punctuation">[</span> ID<span class="token punctuation">]</span> Interval           Transfer     Bandwidth       Retr<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   0.00-10.00  sec  1.03 GBytes   889 Mbits/sec  322             sender<span class="token punctuation">[</span>  4<span class="token punctuation">]</span>   0.00-10.00  sec  1.03 GBytes   887 Mbits/sec                  receiveriperf Done.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ol><li><p>My environment spec</p><ul><li>Ceph cluster deployed in K8s.</li><li>Using Calico as CNI and enabled IPIP mode.</li><li>Using bluStore as storeType.</li><li>Backend hardware of Ceph block is HDD.</li></ul></li><li><p>The transfer speed of the Replicated block is about twice the speed of the EC block.</p><blockquote><p>Please see this comparison chart.</p></blockquote><p><img src="ceph_fio_contrast.png" alt="Ceph performance test"></p></li><li><p>The file system on the host is <strong>3.75</strong> times faster than the Replicated block.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About-Rook-Ceph</title>
      <link href="2020/03/18/about-rook-ceph/"/>
      <url>2020/03/18/about-rook-ceph/</url>
      
        <content type="html"><![CDATA[<h1 id="Rook-Ceph"><a href="#Rook-Ceph" class="headerlink" title="Rook Ceph"></a>Rook Ceph</h1><h2 id="Why-StatefulSets"><a href="#Why-StatefulSets" class="headerlink" title="Why StatefulSets"></a>Why StatefulSets</h2><p>So we all know there are several kinds of workloads controller in Kubernetes including <code>Deployments</code>, <code>StatufulSets</code>, <code>DaemonSet</code> – these three kinds are mostly common-used.<a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">^1</a></p><p>When we deploy stateful App in Kubernetes, we have to concern about that where is the storage. In the very basic way, it is located within the container, which means the when the Pod is gone, the storage is gone.</p><p>Take a glance at the picture below.</p><p><img src="Stateful_Apps_on_Kubernetes.png" alt="Stateful Apps on Kubernetes.png"></p><p>A StatefulSets is for us to deploy Apps which have persistence requirement to storage, which typically is like most of the cluster type Apps(e.g. database, cache, registry in microservices).</p><p><img src="Data_To_Container_Host_Mapping.png" alt="Data To Container Host Mapping"></p><p>The picture above explained why persistent storage is needed.</p><h2 id="About-Kubernetes-and-Ceph"><a href="#About-Kubernetes-and-Ceph" class="headerlink" title="About Kubernetes and Ceph"></a>About Kubernetes and Ceph</h2><h3 id="Ceph"><a href="#Ceph" class="headerlink" title="Ceph"></a>Ceph</h3><p>Ceph is an open-source distributed object, block, and file storage.<a href="https://www.stratoscale.com/blog/storage/introduction-to-ceph/" target="_blank" rel="noopener">^2</a></p><p>Today’s workloads and infrastructure require different data access methods (object, block, file) and Ceph supports all of them.<br>It is designed to be scalable and to have no single point of failure. It is open source software that runs on commodity, readily-available, hardware.</p><h3 id="Compatibility-of-Kubernetes-with-Ceph"><a href="#Compatibility-of-Kubernetes-with-Ceph" class="headerlink" title="Compatibility of Kubernetes with Ceph"></a>Compatibility of Kubernetes with Ceph</h3><p>Kubernetes supports CephFS and CephRBD as StorageClass provioner.<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner" target="_blank" rel="noopener">^3</a><br>While using CephFS as backend of PV(PersistentVolume), it is needed to have external provisioner installed.</p><h1 id="Use-Rook-to-Install-Ceph"><a href="#Use-Rook-to-Install-Ceph" class="headerlink" title="Use Rook to Install Ceph"></a>Use Rook to Install Ceph</h1><h3 id="About-Operator"><a href="#About-Operator" class="headerlink" title="About Operator"></a>About Operator</h3><p>While running complicated Apps, one of the best practice is to run an Operator.<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/" target="_blank" rel="noopener">^5</a><br>In short, an Operator is a combination of <code>Custom Resources</code><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">^7</a> and <code>Controller</code><a href="https://kubernetes.io/docs/concepts/architecture/controller/" target="_blank" rel="noopener">^8</a>.<br>Many middleware cloud provider has maintaining there own official <code>Operator</code> for Kubernetes, such as <a href="https://github.com/elastic/cloud-on-k8s" target="_blank" rel="noopener">Elastisearch</a>, <a href="https://github.com/mongodb/mongodb-enterprise-kubernetes" target="_blank" rel="noopener">MongoDB</a>, <a href="https://github.com/pingcap/tidb-operator" target="_blank" rel="noopener">TiDB</a>.<br>If you want to deploy a new stateful App in Kubernetes, start from looking for if an available <code>Operator</code> exists. Some useful collection links as below.</p><ul><li><a href="https://ramitsurana.github.io/awesome-kubernetes/#operators" target="_blank" rel="noopener">awesome-kubernetes</a></li><li><a href="https://github.com/operator-framework/awesome-operators" target="_blank" rel="noopener">awesome-operators</a></li><li><a href="https://kubedex.com/operators/" target="_blank" rel="noopener">kubedex - Operators</a></li></ul><h2 id="About-Rook"><a href="#About-Rook" class="headerlink" title="About Rook"></a>About Rook</h2><p>Rook is an open source cloud-native storage orchestrator for Kubernetes, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.<a href="https://github.com/rook/rook/blob/master/README.md#what-is-rook" target="_blank" rel="noopener">^4</a></p><p>At this stage, support for Ceph on Rook is already stable,<a href="https://github.com/rook/rook/blob/master/README.md#project-status" target="_blank" rel="noopener">^6</a> we can use it to manage Ceph cluster deployed in Kubernetes. And use it to provide persistent volume to afford StatefulSets Apps.</p><h2 id="Install-steps-of-Rook"><a href="#Install-steps-of-Rook" class="headerlink" title="Install steps of Rook"></a>Install steps of Rook</h2><p>At the moment the latest release version of Rook is “release-1.2”.<br>Go to the Githup repository of Rook, find <a href="https://github.com/rook/rook/tree/release-1.2/cluster/examples/kubernetes/ceph" target="_blank" rel="noopener">ceph’s manifest example</a>.</p><p>I have put the configured files in our repository, link: <a href="https://github.com/aruruka/laboratory/tree/master/k8s/manifest/apps/rook/ceph" target="_blank" rel="noopener">rook manifest</a></p><p><img src="rook_manifest.png" alt="Rook Manifest"></p><h3 id="Deploy-Rook-Operator"><a href="#Deploy-Rook-Operator" class="headerlink" title="Deploy Rook Operator"></a>Deploy Rook Operator</h3><p>Apply the manifest.</p><pre><code>$ kubectl apply -f common.yaml$ kubectl apply -f operator.yaml</code></pre><p>Then watch status of Pod.</p><pre><code>$ kubectl get pod -n rook-ceph</code></pre><h3 id="Create-Ceph-cluster"><a href="#Create-Ceph-cluster" class="headerlink" title="Create Ceph cluster"></a>Create Ceph cluster</h3><p>Now Rook Operator is running so we can continue to create Ceph cluster.</p><h4 id="Some-customized-configuration"><a href="#Some-customized-configuration" class="headerlink" title="Some customized configuration"></a>Some customized configuration</h4><p>Here are some places I’ve configured in <code>cluster.yaml</code>.</p><ol><li><p><code>.spec.placement</code></p><p>To use <code>Affinity</code> and <code>Toleration</code> to specify dedicated Nodes for deploying Ceph Cluster.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">placement</span><span class="token punctuation">:</span>    <span class="token key atrule">all</span><span class="token punctuation">:</span>      <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>        <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>          <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> role              <span class="token key atrule">operator</span><span class="token punctuation">:</span> In              <span class="token key atrule">values</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> storage<span class="token punctuation">-</span>node      <span class="token key atrule">podAffinity</span><span class="token punctuation">:</span>      <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> storage<span class="token punctuation">-</span>node        <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Also, we need to add some Nodes into Kubernetes cluster, and <code>label</code> and <code>taint</code> them right.</p><pre><code>$ kubectl get nodes -l role=storage-nodeNAME                   STATUS   ROLES    AGE    VERSIONdatateam-rookceph-01   Ready    &lt;none&gt;   7d5h   v1.17.1datateam-rookceph-02   Ready    &lt;none&gt;   7d5h   v1.17.1datateam-rookceph-03   Ready    &lt;none&gt;   7d5h   v1.17.1</code></pre><pre><code>$ kubectl get nodes -o go-template-file=&quot;${HOME}/go-template/kubectl_node_taint.tmpl&quot; | sed -rn &#39;1p;/rook/Ip&#39;Node                                               Taintdatateam-rookceph-01    storage-node=&lt;no value&gt;:NoExecutedatateam-rookceph-02    storage-node=&lt;no value&gt;:NoExecutedatateam-rookceph-03    storage-node=&lt;no value&gt;:NoExecute</code></pre></li><li><p><code>.spec.dataDirHostPath</code></p><p>This is the host dir for Ceph cluster storage, make sure every Node which is used to deploy Ceph has this directory.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">dataDirHostPath</span><span class="token punctuation">:</span> /data/rook_data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><code>.spec.storage.directories</code></p><p>This is the path for Ceph OSD storage.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">storage</span><span class="token punctuation">:</span>  <span class="token key atrule">directories</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /data/rook_data/filestore<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h4 id="Sample-output"><a href="#Sample-output" class="headerlink" title="Sample output"></a>Sample output</h4><pre><code>$ kubectl apply -f cluster.yaml</code></pre><p>Sample output after deploying Ceph cluster.</p><pre><code>$ kubectl get pods -n rook-cephNAME                                                             READY   STATUS      RESTARTS   AGEcsi-cephfsplugin-9bcdd                                           3/3     Running     0          6d19hcsi-cephfsplugin-jgkjx                                           3/3     Running     0          6d19hcsi-cephfsplugin-provisioner-7c75f6bf94-k4h4c                    5/5     Running     0          6d19hcsi-cephfsplugin-provisioner-7c75f6bf94-s9p5m                    5/5     Running     0          6d19hcsi-rbdplugin-ccw6t                                              3/3     Running     0          6d19hcsi-rbdplugin-jz5kh                                              3/3     Running     0          6d19hcsi-rbdplugin-provisioner-7954c9db75-jvjh7                       6/6     Running     0          6d19hcsi-rbdplugin-provisioner-7954c9db75-wv56c                       6/6     Running     0          6d19hrook-ceph-crashcollector-datateam-rookceph-01-5c489fbc7b-246vr   1/1     Running     0          7drook-ceph-crashcollector-datateam-rookceph-02-7b459d4f69-t7wdq   1/1     Running     0          7drook-ceph-crashcollector-datateam-rookceph-03-5d7578fdd6-lv9br   1/1     Running     0          7drook-ceph-mgr-a-5d4765dfb-p4tfr                                  1/1     Running     0          7drook-ceph-mon-a-6864d58cc7-8qtst                                 1/1     Running     0          7drook-ceph-mon-b-65fbf9b96c-v9llx                                 1/1     Running     0          7drook-ceph-mon-c-54984d88b6-kxxc9                                 1/1     Running     0          7drook-ceph-operator-648d574f5c-bw6cz                              1/1     Running     0          6d19hrook-ceph-osd-0-6f966cc46b-n2trl                                 1/1     Running     0          7drook-ceph-osd-1-757cc69bc9-nd2ml                                 1/1     Running     0          7drook-ceph-osd-2-57b88cd98f-bvkw4                                 1/1     Running     0          7drook-ceph-osd-prepare-datateam-rookceph-01-tcfwp                 0/1     Completed   0          6d19hrook-ceph-osd-prepare-datateam-rookceph-02-mqg4w                 0/1     Completed   0          6d19hrook-ceph-osd-prepare-datateam-rookceph-03-bchr5                 0/1     Completed   0          6d19hrook-ceph-tools-6cb8fcc6f9-42p69                                 1/1     Running     0          6d19hrook-discover-42h6t                                              1/1     Running     0          7d5hrook-discover-tghgv                                              1/1     Running     0          7d5h</code></pre><h3 id="Use-Rook-toolbox-to-verify-Ceph-cluster-status"><a href="#Use-Rook-toolbox-to-verify-Ceph-cluster-status" class="headerlink" title="Use Rook toolbox to verify Ceph cluster status"></a>Use Rook toolbox to verify Ceph cluster status</h3><p>To verify status of Ceph cluster, we could use the Rook toolbox.</p><p>P.S. The image of Rook Ceph is based on CentOS, so we can use yum to install tools on-demand and rebuild the image.</p><pre><code>kubectl apply -f toolbox.yaml</code></pre><p>And we can simply define a script to alias the <a href="https://github.com/aruruka/laboratory/blob/master/k8s/hack/ceph_profile_d.sh" target="_blank" rel="noopener"><code>ceph</code></a> command.</p><pre><code># /etc/profile.d/ceph_profile_d.shalias ceph=&#39;kubectl -n rook-ceph exec -i $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -- ceph&#39;alias rados=&#39;kubectl -n rook-ceph exec -i $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -- rados&#39;</code></pre><pre><code>$ ceph status  cluster:    id:     cf4fa4d1-0d37-44bd-8cb1-53d7304b233c    health: HEALTH_OK  services:    mon: 3 daemons, quorum a,b,c (age 7d)    mgr: a(active, since 6d)    osd: 3 osds: 3 up (since 7d), 3 in (since 7d)  data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   20 GiB used, 280 GiB / 300 GiB avail    pgs:</code></pre><pre><code>$ ceph osd status+----+----------------------+-------+-------+--------+---------+--------+---------+-----------+| id |         host         |  used | avail | wr ops | wr data | rd ops | rd data |   state   |+----+----------------------+-------+-------+--------+---------+--------+---------+-----------+| 0  | datateam-rookceph-02 | 6885M | 93.2G |    0   |     0   |    0   |     0   | exists,up || 1  | datateam-rookceph-03 | 6882M | 93.2G |    0   |     0   |    0   |     0   | exists,up || 2  | datateam-rookceph-01 | 6968M | 93.1G |    0   |     0   |    0   |     0   | exists,up |+----+----------------------+-------+-------+--------+---------+--------+---------+-----------+</code></pre><pre><code>$ ceph dfRAW STORAGE:    CLASS     SIZE        AVAIL       USED       RAW USED     %RAW USED    hdd       300 GiB     280 GiB     20 GiB       20 GiB          6.75    TOTAL     300 GiB     280 GiB     20 GiB       20 GiB          6.75POOLS:    POOL     ID     STORED     OBJECTS     USED     %USED     MAX AVAIL</code></pre><pre><code>$ rados dfPOOL_NAME USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS RD WR_OPS WR USED COMPR UNDER COMPRtotal_objects    0total_used       20 GiBtotal_avail      280 GiBtotal_space      300 GiB</code></pre><h3 id="Use-Ceph-Dashboard"><a href="#Use-Ceph-Dashboard" class="headerlink" title="Use Ceph Dashboard"></a>Use Ceph Dashboard</h3><p>In the <a href="https://github.com/aruruka/laboratory/blob/master/k8s/manifest/apps/rook/cluster.yaml" target="_blank" rel="noopener"><code>cluster.yaml</code></a> manifest, we could found <code>.spec.dashboard</code> config, defaultly enabled.</p><p>Let’s find the Service name of defaultly deployed Ceph Dashboard.</p><p>First we could deploy a busybox container in whichever Namespace and use nslookup to find out the FQDN of any service.</p><p>As below, we can see the cluster domain is <code>datateam.local</code>.</p><pre><code>$ kubectl exec -ti busybox -- nslookup kubernetesServer:         10.210.0.10Address:        10.210.0.10#53Name:   kubernetes.default.svc.datateam.localAddress: 10.210.0.1</code></pre><p>Then we find the service name of Ceph Dashboard.<br>As below, it’s <code>rook-ceph-mgr-dashboard</code>.</p><pre><code>$ kubectl get service -n rook-cephNAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGEcsi-cephfsplugin-metrics   ClusterIP   10.210.0.62    &lt;none&gt;        8080/TCP,8081/TCP   7d1hcsi-rbdplugin-metrics      ClusterIP   10.210.0.196   &lt;none&gt;        8080/TCP,8081/TCP   7d1hrook-ceph-mgr              ClusterIP   10.210.0.118   &lt;none&gt;        9283/TCP            7d1hrook-ceph-mgr-dashboard    ClusterIP   10.210.0.190   &lt;none&gt;        8443/TCP            7d1hrook-ceph-mon-a            ClusterIP   10.210.0.48    &lt;none&gt;        6789/TCP,3300/TCP   7d1hrook-ceph-mon-b            ClusterIP   10.210.0.223   &lt;none&gt;        6789/TCP,3300/TCP   7d1hrook-ceph-mon-c            ClusterIP   10.210.0.149   &lt;none&gt;        6789/TCP,3300/TCP   7d1h</code></pre><p>So we can access Ceph Dashboard by this URL – <a href="https://rook-ceph-mgr-dashboard.rook-ceph.svc.datateam.local:8443" target="_blank" rel="noopener">https://rook-ceph-mgr-dashboard.rook-ceph.svc.datateam.local:8443</a>.</p><p>By default, Rook will create a Configmap to store the <code>admin</code> user’s password.</p><pre><code>$ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;{[&#39;data&#39;][&#39;password&#39;]}&quot; | base64 --decode &amp;&amp; echoD17j9zPi3R</code></pre><p>Sample display as below.<br><img src="ceph_dashboard.png" alt="Ceph Dashboard"></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Calico Practicing-3 -- How To Configure Accessing Calico Endpoint From Outside of The Cluster</title>
      <link href="2020/01/19/calico-practicing-3-how-to-configure-accessing-calico-endpoint-from-outside-of-the-cluster/"/>
      <url>2020/01/19/calico-practicing-3-how-to-configure-accessing-calico-endpoint-from-outside-of-the-cluster/</url>
      
        <content type="html"><![CDATA[<h1 id="How-To-Configure-Accessing-Calico-Endpoint-From-Outside-of-The-Cluster"><a href="#How-To-Configure-Accessing-Calico-Endpoint-From-Outside-of-The-Cluster" class="headerlink" title="How To Configure Accessing Calico Endpoint From Outside of The Cluster"></a>How To Configure Accessing Calico Endpoint From Outside of The Cluster</h1><h2 id="This-article’s-purpose"><a href="#This-article’s-purpose" class="headerlink" title="This article’s purpose"></a>This article’s purpose</h2><p>Our software technology stack is mainly Java with micro-service frameworks(e.g. Spring Boot, Consul).<br>And we’re moving from VM to Kubernetes. Considering cases of putting consumer or/and registry conponents outside of the Kubernetes cluster, we have to make our network plane allows workload accessing Pod IP, Service IP from outside the Kubernetes cluster.</p><p>In previous article, I mentioned that the circumstance I’m facing is On-prem L2 networks.<br>Because there’s no one can handle the “<a href="https://docs.projectcalico.org/v3.11/networking/determine-best-networking#unencapsulated-peered-with-physical-infrastructure" target="_blank" rel="noopener">BGP peering with physical infrastructure</a>“.<br>I decided to use Calico node as gateway to access Calico’s network from outside the cluster.</p><h2 id="This-article-contains-procedures-of-below"><a href="#This-article-contains-procedures-of-below" class="headerlink" title="This article contains procedures of below"></a>This article contains procedures of below</h2><ol><li>Disable default node-to-node mesh</li><li>Configure node as Route Reflector</li><li>Configure IP in IP encapsulation for only cross subnet traffic</li><li>Add route rules on VM outside of Calico cluster</li><li>Summary and guess of optimization</li></ol><h1 id="Assets-List"><a href="#Assets-List" class="headerlink" title="Assets List"></a>Assets List</h1><table><thead><tr><th>Host Name</th><th>IP Address</th><th>Description</th><th>Subnet</th></tr></thead><tbody><tr><td>ip-10-20-1-81 ~ ip-10-20-1-83</td><td>10.20.1.81 ~ 10.20.1.83</td><td>Calico Route Reflector</td><td>10.20.0.0/23</td></tr><tr><td>ip-10-20-1-84 ~ ip-10-20-1-85</td><td>10.20.1.84 ~ 10.20.1.85</td><td>Calico Node</td><td>10.20.0.0/23</td></tr><tr><td>iot-brand-website-ravpower-middleware-dev</td><td>10.20.1.206</td><td>Outside of Calico cluster</td><td>10.20.0.0/23</td></tr><tr><td>Shounen-HP-Z</td><td>10.35.1.61</td><td>My Work PC</td><td>10.30.1.0/24</td></tr></tbody></table><p>Some CIDR infomation</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">cluster-cidr</span><span class="token punctuation">:</span> 10.244.0.0/16<span class="token key atrule">service-cidr</span><span class="token punctuation">:</span> 10.96.0.0/12<span class="token key atrule">podCIDR</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> 10.244.0.0/24  <span class="token punctuation">-</span> 10.244.1.0/24  <span class="token punctuation">-</span> 10.244.2.0/24  <span class="token punctuation">-</span> 10.244.3.0/24  <span class="token punctuation">-</span> 10.244.4.0/24<span class="token key atrule">ip-pool</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span>    <span class="token key atrule">default-ipv4-ippool</span><span class="token punctuation">:</span>      <span class="token key atrule">range</span><span class="token punctuation">:</span> 10.244.0.0/16      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span> all()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Disable-default-node-to-node-mesh"><a href="#Disable-default-node-to-node-mesh" class="headerlink" title="Disable default node-to-node mesh"></a>Disable default node-to-node mesh</h1><ol><li><p>Export the default BGPConfiguration</p><p> <code>calicoctl get bgpconfiguration default -o yaml --export &gt; ~/calico/configure_bgp_peering/bgpconfiguration_default.yaml</code></p></li><li><p>Modify the <code>nodeToNodeMeshEnabled</code> field to <code>false</code></p><pre class="line-numbers language-bash"><code class="language-bash"> calicoctl apply -f - <span class="token operator">&lt;&lt;</span> EOF  apiVersion: projectcalico.org/v3  kind: BGPConfiguration  metadata:    name: default  spec:    nodeToNodeMeshEnabled: <span class="token boolean">false</span>    asNumber: 64512 EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p>reference link: <a href="https://docs.projectcalico.org/v3.11/networking/bgp#disable-the-default-bgp-node-to-node-mesh" target="_blank" rel="noopener">Disable the default BGP node-to-node mesh</a></p><h1 id="Configure-node-as-Route-Reflector"><a href="#Configure-node-as-Route-Reflector" class="headerlink" title="Configure node as Route Reflector"></a>Configure node as Route Reflector</h1><ol><li><p>I have Kubernetes installed with Calico, these are my k8s nodes.</p><pre class="line-numbers language-bash"><code class="language-bash">$ kubectl get node -o wideNAME            STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIMEip-10-20-1-81   Ready    master   27d   v1.16.3   10.20.1.81    <span class="token operator">&lt;</span>none<span class="token operator">></span>        CentOS Linux 7 <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>   3.10.0-1062.9.1.el7.x86_64   docker://19.3.5ip-10-20-1-82   Ready    master   27d   v1.16.3   10.20.1.82    <span class="token operator">&lt;</span>none<span class="token operator">></span>        CentOS Linux 7 <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>   3.10.0-1062.9.1.el7.x86_64   docker://19.3.5ip-10-20-1-83   Ready    master   27d   v1.16.3   10.20.1.83    <span class="token operator">&lt;</span>none<span class="token operator">></span>        CentOS Linux 7 <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>   3.10.0-1062.9.1.el7.x86_64   docker://19.3.5ip-10-20-1-84   Ready    worker   27d   v1.16.3   10.20.1.84    <span class="token operator">&lt;</span>none<span class="token operator">></span>        CentOS Linux 7 <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>   3.10.0-1062.9.1.el7.x86_64   docker://18.9.9ip-10-20-1-85   Ready    worker   26d   v1.16.3   10.20.1.85    <span class="token operator">&lt;</span>none<span class="token operator">></span>        CentOS Linux 7 <span class="token punctuation">(</span>Core<span class="token punctuation">)</span>   3.10.0-1062.9.1.el7.x86_64   docker://19.3.5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>To configure a node to be a route reflector with cluster ID 244.0.0.1, run the following command.</p><p><strong><em>About routeReflectorClusterID:</em></strong><br>all the route reflector nodes should have the same cluster ID.<sup id="a1"><a href="#f1">Footnote-1</a></sup></p><pre class="line-numbers language-bash"><code class="language-bash">calicoctl patch node ip-10-20-1-81 -p <span class="token string">'{"spec": {"bgp": {"routeReflectorClusterID": "244.0.0.1"}}}'</span>calicoctl patch node ip-10-20-1-82 -p <span class="token string">'{"spec": {"bgp": {"routeReflectorClusterID": "244.0.0.1"}}}'</span>calicoctl patch node ip-10-20-1-83 -p <span class="token string">'{"spec": {"bgp": {"routeReflectorClusterID": "244.0.0.1"}}}'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>Label node(s) to indicate that it is a route reflector</p><p><strong><em><code>calicoctl</code> can also label a Calico node, but <code>kubectl get node --show-labels</code>would not show the label, but <code>calicoctl get node -o yaml</code> would show the label patched by <code>kubectl</code>.</em></strong></p><p><code>kubectl label node ip-10-20-1-81 ip-10-20-1-82 ip-10-20-1-83 route-reflector=true</code></p></li><li><p>Configure non route reflector node peer with route reflector and route reflectors peering with each other.</p><pre class="line-numbers language-bash"><code class="language-bash">cat<span class="token operator">&lt;&lt;</span>EOF <span class="token operator">|</span> calicoctl apply -f -kind: BGPPeerapiVersion: projectcalico.org/v3metadata:  name: peer-with-route-reflectorsspec:  nodeSelector: <span class="token string">"!has(route-reflector)"</span>  peerSelector: route-reflector <span class="token operator">==</span> <span class="token string">'true'</span>EOFcat<span class="token operator">&lt;&lt;</span>EOF <span class="token operator">|</span> calicoctl apply -f -kind: BGPPeerapiVersion: projectcalico.org/v3metadata:  name: rrs-to-rrsspec:  nodeSelector: route-reflector <span class="token operator">==</span> <span class="token string">'true'</span>  peerSelector: route-reflector <span class="token operator">==</span> <span class="token string">'true'</span>EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>After configuring, you should see route reflector’s node status as below</p><pre class="line-numbers language-bash"><code class="language-bash">$ def_inf<span class="token operator">=</span>`ip route <span class="token operator">|</span> <span class="token function">grep</span> -Ei default <span class="token operator">|</span> <span class="token function">awk</span> <span class="token string">'{print <span class="token variable">$5</span>}'</span>`$ <span class="token keyword">echo</span> <span class="token variable">$def_inf</span>ens192$ ip addr show <span class="token variable">$def_inf</span> <span class="token operator">|</span> <span class="token function">grep</span> inet <span class="token operator">|</span> <span class="token function">awk</span> <span class="token string">'{ print <span class="token variable">$2</span>; }'</span> <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/\/.*$//'</span>10.20.1.81$ calicoctl node statusCalico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+<span class="token operator">|</span> PEER ADDRESS <span class="token operator">|</span>   PEER TYPE   <span class="token operator">|</span> STATE <span class="token operator">|</span>  SINCE   <span class="token operator">|</span>    INFO     <span class="token operator">|</span>+--------------+---------------+-------+----------+-------------+<span class="token operator">|</span> 10.20.1.84   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:00:00 <span class="token operator">|</span> Established <span class="token operator">|</span><span class="token operator">|</span> 10.20.1.85   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:00:00 <span class="token operator">|</span> Established <span class="token operator">|</span><span class="token operator">|</span> 10.20.1.82   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:02:53 <span class="token operator">|</span> Established <span class="token operator">|</span><span class="token operator">|</span> 10.20.1.83   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:03:04 <span class="token operator">|</span> Established <span class="token operator">|</span>+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>After configuring, you should see node’s node status as below</p><pre class="line-numbers language-bash"><code class="language-bash">$ def_inf<span class="token operator">=</span>`ip route <span class="token operator">|</span> <span class="token function">grep</span> -Ei default <span class="token operator">|</span> <span class="token function">awk</span> <span class="token string">'{print <span class="token variable">$5</span>}'</span>`$ ip addr show <span class="token variable">$def_inf</span> <span class="token operator">|</span> <span class="token function">grep</span> inet <span class="token operator">|</span> <span class="token function">awk</span> <span class="token string">'{ print <span class="token variable">$2</span>; }'</span> <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/\/.*$//'</span>10.20.1.84$ calicoctl node statusCalico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+<span class="token operator">|</span> PEER ADDRESS <span class="token operator">|</span>   PEER TYPE   <span class="token operator">|</span> STATE <span class="token operator">|</span>  SINCE   <span class="token operator">|</span>    INFO     <span class="token operator">|</span>+--------------+---------------+-------+----------+-------------+<span class="token operator">|</span> 10.20.1.81   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:00:00 <span class="token operator">|</span> Established <span class="token operator">|</span><span class="token operator">|</span> 10.20.1.82   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:02:53 <span class="token operator">|</span> Established <span class="token operator">|</span><span class="token operator">|</span> 10.20.1.83   <span class="token operator">|</span> node specific <span class="token operator">|</span> up    <span class="token operator">|</span> 12:03:04 <span class="token operator">|</span> Established <span class="token operator">|</span>+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p>reference link:</p><ul><li><a href="https://docs.projectcalico.org/v3.11/networking/bgp#configure-a-node-to-act-as-a-route-reflector" target="_blank" rel="noopener">Configure a node to act as a route reflector</a></li><li><a href="https://docs.projectcalico.org/v3.11/getting-started/kubernetes/hardway/configure-bgp-peering" target="_blank" rel="noopener">Configure BGP peering</a></li></ul><h1 id="Configure-IP-in-IP-encapsulation-for-only-cross-subnet-traffic"><a href="#Configure-IP-in-IP-encapsulation-for-only-cross-subnet-traffic" class="headerlink" title="Configure IP in IP encapsulation for only cross subnet traffic"></a>Configure IP in IP encapsulation for only cross subnet traffic</h1><p>If you assume all of your nodes are in the same subnet, you can simply disable IPIP mode.<br>Otherwise you should configure BGP peering with your physical routers or use IPIP mode.<br>In order to accelerate the traffic, I configured encapsulation for cross subnet traffic only.</p><pre class="line-numbers language-bash"><code class="language-bash">$ cat<span class="token operator">&lt;&lt;</span>EOF <span class="token operator">|</span> calicoctl apply -f -apiVersion: projectcalico.org/v3kind: IPPoolmetadata:  name: default-ipv4-ippoolspec:  cidr: 10.244.0.0/16  ipipMode: CrossSubnet  natOutgoing: <span class="token boolean">true</span>EOF$ calicoctl get ipPool -o wideNAME                  CIDR            NAT    IPIPMODE      VXLANMODE   DISABLED   SELECTORdefault-ipv4-ippool   10.244.0.0/16   <span class="token boolean">true</span>   CrossSubnet   Never       <span class="token boolean">false</span>      all<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Add-route-rules-to-make-it-accessible-from-outside-of-cluster"><a href="#Add-route-rules-to-make-it-accessible-from-outside-of-cluster" class="headerlink" title="Add route rules to make it accessible from outside of cluster"></a>Add route rules to make it accessible from outside of cluster</h1><p>Since we are unable to configure BGP peering with physical routers or switches, I just googled for “Calico micro-service access outside cluster” and found out some articles recording that just simply add route rules to a server like below:</p><p><strong>Notice: After steps above, there’s some confilcts between the default Pod subnet and our on-premises infructure, so I reseted my cluster’s IP subnet.</strong></p><h2 id="The-global-route-rules"><a href="#The-global-route-rules" class="headerlink" title="The global route rules"></a>The global route rules</h2><p>Acctually, Calico will maintain a list of route rules like below.<br>Notice that the whole Pod subnet are splited into small IP blocks in Calico’s SDN routing system, depending on the increasement of workload(here, which is k8s Pod).</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@ip-10-20-1-84 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ip route show</span>default via 10.20.0.1 dev ens19210.20.0.0/23 dev ens192 proto kernel scope <span class="token function">link</span> src 10.20.1.84blackhole 10.200.0.192/26 proto bird10.200.0.193 dev cali00b03047dbe scope <span class="token function">link</span>10.200.1.0/26 via 10.20.1.81 dev ens192 proto bird10.200.1.64/26 via 10.20.1.82 dev ens192 proto bird10.200.1.128/26 via 10.20.1.83 dev ens192 proto bird10.200.1.192/26 via 10.20.1.85 dev ens192 proto bird169.254.0.0/16 dev ens192 scope <span class="token function">link</span> metric 1002172.17.0.0/16 dev docker0 proto kernel scope <span class="token function">link</span> src 172.17.0.1<span class="token punctuation">[</span>root@ip-10-20-1-84 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>I will just add the whole Pod subnet and service subnet rule, this is gonna trigger some “nexthop” phenomenon, I guess that means there’s packet loss problem.<br>But, for now, we just leave it behind.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ping 10.200.1.193</span>PING 10.200.1.193 <span class="token punctuation">(</span>10.200.1.193<span class="token punctuation">)</span> 56<span class="token punctuation">(</span>84<span class="token punctuation">)</span> bytes of data.64 bytes from 10.200.1.193: icmp_seq<span class="token operator">=</span>1 ttl<span class="token operator">=</span>63 time<span class="token operator">=</span>0.486 msFrom 10.20.1.84 icmp_seq<span class="token operator">=</span>2 Redirect Host<span class="token punctuation">(</span>New nexthop: 10.20.1.85<span class="token punctuation">)</span>From 10.20.1.84: icmp_seq<span class="token operator">=</span>2 Redirect Host<span class="token punctuation">(</span>New nexthop: 10.20.1.85<span class="token punctuation">)</span>64 bytes from 10.200.1.193: icmp_seq<span class="token operator">=</span>2 ttl<span class="token operator">=</span>63 time<span class="token operator">=</span>0.524 ms<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Here is the global route rules to add:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Pod IP</span>ip route add 10.200.0.0/23 via 10.20.1.84 <span class="token comment" spellcheck="true"># I've change the default Pod subnet from 10.244.0.0/16 to 10.200.0.0/23</span><span class="token comment" spellcheck="true"># Service IP</span>ip route add 10.210.0.0/24 via 10.20.1.84 <span class="token comment" spellcheck="true"># I've change the default Pod subnet from 10.96.0.0/12 to 10.210.0.0/24</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>After appending the route rules:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Initial route table outside cluster</span><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ip route show</span>default via 10.20.0.1 dev ens192 proto static metric 10010.20.0.0/23 dev ens192 proto kernel scope <span class="token function">link</span> src 10.20.1.222 metric 100172.17.0.0/16 dev docker0 proto kernel scope <span class="token function">link</span> src 172.17.0.1172.18.0.0/16 dev br-1f4086f25e8d proto kernel scope <span class="token function">link</span> src 172.18.0.1<span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Add route rule</span><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ip route add 10.200.0.0/23 via 10.20.1.84</span><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ip -4 -o addr</span>1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever2: ens192    inet 10.20.1.222/23 brd 10.20.1.255 scope global noprefixroute ens192\       valid_lft forever preferred_lft forever3: docker0    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\       valid_lft forever preferred_lft forever7151: br-1f4086f25e8d    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-1f4086f25e8d\       valid_lft forever preferred_lft forever<span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ip route show</span>default via 10.20.0.1 dev ens192 proto static metric 10010.20.0.0/23 dev ens192 proto kernel scope <span class="token function">link</span> src 10.20.1.222 metric 10010.200.0.0/23 via 10.20.1.84 dev ens192172.17.0.0/16 dev docker0 proto kernel scope <span class="token function">link</span> src 172.17.0.1172.18.0.0/16 dev br-1f4086f25e8d proto kernel scope <span class="token function">link</span> src 172.18.0.1<span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># After adding route rule, server outside can access Pod</span><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># curl -s 10.200.1.193</span><span class="token operator">&lt;</span><span class="token operator">!</span>DOCTYPE html<span class="token operator">></span><span class="token operator">&lt;</span>html<span class="token operator">></span><span class="token operator">&lt;</span>head<span class="token operator">></span><span class="token operator">&lt;</span>title<span class="token operator">></span>Welcome to nginx<span class="token operator">!</span><span class="token operator">&lt;</span>/title<span class="token operator">></span><span class="token operator">&lt;</span>style<span class="token operator">></span>    body <span class="token punctuation">{</span>        width: 35em<span class="token punctuation">;</span>        margin: 0 auto<span class="token punctuation">;</span>        font-family: Tahoma, Verdana, Arial, sans-serif<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token operator">&lt;</span>/style<span class="token operator">></span><span class="token operator">&lt;</span>/head<span class="token operator">></span><span class="token operator">&lt;</span>body<span class="token operator">></span><span class="token operator">&lt;</span>h1<span class="token operator">></span>Welcome to nginx<span class="token operator">!</span><span class="token operator">&lt;</span>/h1<span class="token operator">></span><span class="token operator">&lt;</span>p<span class="token operator">></span>If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.<span class="token operator">&lt;</span>/p<span class="token operator">></span><span class="token operator">&lt;</span>p<span class="token operator">></span>For online documentation and support please refer to<span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"http://nginx.org/"</span><span class="token operator">></span>nginx.org<span class="token operator">&lt;</span>/a<span class="token operator">></span>.<span class="token operator">&lt;</span>br/<span class="token operator">></span>Commercial support is available at<span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"http://nginx.com/"</span><span class="token operator">></span>nginx.com<span class="token operator">&lt;</span>/a<span class="token operator">></span>.<span class="token operator">&lt;</span>/p<span class="token operator">></span><span class="token operator">&lt;</span>p<span class="token operator">></span><span class="token operator">&lt;</span>em<span class="token operator">></span>Thank you <span class="token keyword">for</span> using nginx.<span class="token operator">&lt;</span>/em<span class="token operator">></span><span class="token operator">&lt;</span>/p<span class="token operator">></span><span class="token operator">&lt;</span>/body<span class="token operator">></span><span class="token operator">&lt;</span>/html<span class="token operator">></span><span class="token punctuation">[</span>root@harbor ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># This is the Pod I just accessed</span><span class="token punctuation">[</span>root@datateam-k8s-control-plane-01 manifest<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># kubectl get pod -o wide | sed -rn '1p;/10.200.1.193/Ip'</span>NAME                     READY   STATUS    RESTARTS   AGE   IP             NODE                     NOMINATED NODE   READINESS GATESnginx-764ddbb9cf-d2q56   1/1     Running   0          25m   10.200.1.193   ip-10-20-1-85   <span class="token operator">&lt;</span>none<span class="token operator">></span>           <span class="token operator">&lt;</span>none<span class="token operator">></span><span class="token punctuation">[</span>root@datateam-k8s-control-plane-01 manifest<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Extended-practice-add-the-route-rules-to-switches-to-make-it-available-for-all-servers-and-workstations-in-LAN"><a href="#Extended-practice-add-the-route-rules-to-switches-to-make-it-available-for-all-servers-and-workstations-in-LAN" class="headerlink" title="Extended practice - add the route rules to switches to make it available for all servers and workstations in LAN"></a>Extended practice - add the route rules to switches to make it available for all servers and workstations in LAN</h2><p>So, the theory is as same as the scenario above, I just asked a colleague who is in charge of network devices to add the route rules above.</p><p>Then it works, as below.</p><p>After adding route rules, I configured DNS server to forward searching related my Kubernetes cluster’s service to CoreDNS’s service IP.</p><p><strong><em>datateam.local</em></strong> is our cluster’s domain name.</p><p><img src="dns_forfwarder_on_condition.png" alt="Configure LAN DNS server to forward Kubernetes cluster&#39;s DNS resolving" title="Forwarding DNS resolving"></p><p>And then I can access any service directly, without <code>ingress</code>, <code>NodePort</code>, <code>LoadBalancer</code> or <code>ExternalName</code>.<sup id="a2"><a href="#f2">Footnote-2</a></sup></p><p><img src="accessing_service_from_outside_cluster-01.jpg" alt="Access a service" title="Accessing service - nginx in namespace - default"></p><p><img src="accessing_service_from_outside_cluster-02.jpg" alt="Access a service" title="Accessing service - nginx in namespace - kube-public"></p><p>Now, CoreDNS is available on my whole LAN!!</p><h1 id="Footnote"><a href="#Footnote" class="headerlink" title="Footnote"></a>Footnote</h1><p><b id="f1">footnote-1</b><br><a href="https://docs.projectcalico.org/v3.7/networking/bgp" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.7/networking/bgp</a><br><a href="#a1">↩</a></p><p><b id="f2">footnote-2</b><br><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#publishing-services-service-types" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/concepts/services-networking/service/#publishing-services-service-types</a><br><a href="#a2">↩</a></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calico </tag>
            
            <tag> k8s </tag>
            
            <tag> SDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Calico Practicing-2 -- About Calico Networking Option</title>
      <link href="2020/01/19/calico-practicing-2-about-calico-networking-option/"/>
      <url>2020/01/19/calico-practicing-2-about-calico-networking-option/</url>
      
        <content type="html"><![CDATA[<h1 id="Calico-Networking-Option"><a href="#Calico-Networking-Option" class="headerlink" title="Calico Networking Option"></a>Calico Networking Option</h1><h1 id="Circumstance"><a href="#Circumstance" class="headerlink" title="Circumstance"></a>Circumstance</h1><p>Our on-premise environment consists of several VLAN(layer 2 network), each VLAN is created by VCenter. Physically, every VLAN is based on several baremetal machines on the same floor, the baremetal machines are connected by ethernet switches. And we have several floors of baremetal machines, they’re connected by ToR(Top of Rack) routers(layer 3 network).</p><h1 id="Standardized-Glossary"><a href="#Standardized-Glossary" class="headerlink" title="Standardized Glossary"></a>Standardized Glossary</h1><ul><li><p><a href="https://docs.projectcalico.org/v3.11/reference/architecture/design/l3-interconnect-fabric" target="_blank" rel="noopener">layer 3</a><br>In many on-premise data centers, each server connects to a top-of-rack (ToR) router operating at the IP layer (layer 3). In that situation, we would need to peer each node with its corresponding ToR router, so that the ToR learns routes to the containers.<br>In this circumstance, you need to <a href="https://docs.projectcalico.org/v3.11/networking/bgp" target="_blank" rel="noopener">Configure BGP peering</a>.<br>On top of the advantages of non-peered unencapsulated traffic:</p><ul><li>Your cluster can span multiple L2 subnets without needing encapsulation</li><li>Resources outside your cluster can talk directly to your pods without NAT</li><li>You can even expose pods directly to the internet if you want!<br><img src="https://docs.projectcalico.org/images/l3-fabric-diagrams-as-rack-l2-spine.png" alt="The AS Per Rack model"></li></ul></li><li><p><a href="https://docs.projectcalico.org/v3.11/reference/architecture/design/l2-interconnect-fabric" target="_blank" rel="noopener">layer 2</a><br>Since we are running in an VLAN within a single subnet(on the same floor), the hosts have ethernet (layer 2) connectivity with one another, meaning there are no routers between them. Thus, they can peer directly with each other.<br>If we spread k8s nodes(calico workloads) into several VLAN(subnet), we need either to configure <a href="https://docs.projectcalico.org/v3.11/networking/bgp" target="_blank" rel="noopener">peer over BGP with your routers</a> or <a href="https://docs.projectcalico.org/v3.11/networking/determine-best-networking#ip-in-ip-or-vxlan-encapsulation" target="_blank" rel="noopener">cross-subnet encapsulation</a>.</p><p><img src="https://docs.projectcalico.org/images/l2-spine-planes.png" alt="Ethernet topology"></p></li><li><p><a href="https://docs.projectcalico.org/v3.11/networking/vxlan-ipip" target="_blank" rel="noopener">overlay/encapsulation</a><br>Although the official project recommends running Calico without network overlay/encapsulation, becase that provides the highest performance adn simplest network.<br>However, if you are not familiar with physical network devices and infrustructure or you are going to use Calico in a public cloud like AWS, you can also use the overlay networking.<br>Calico can perform encapsulation on: all traffic, no traffic, or only on traffic that crosses a subnet boundary.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Autonomous_system_(Internet)" target="_blank" rel="noopener">AS(Autonomous system)</a></p><p>An autonomous system (AS) is a collection of connected <a href="https://en.wikipedia.org/wiki/Internet_Protocol" target="_blank" rel="noopener">Internet Protocol (IP)</a> routing prefixes under the control of one or more network operators on behalf of a single administrative entity or domain that presents a common, clearly defined routing policy to the internet.[^1]</p></li></ul><h1 id="Footnote"><a href="#Footnote" class="headerlink" title="Footnote"></a>Footnote</h1><p>[^1]: <a href="https://en.wikipedia.org/wiki/Multihoming" target="_blank" rel="noopener">multihomed</a>: An AS that maintains connections to more than one other AS. This allows the AS to remain connected to the internet in the event of a complete failure of one of their connections. However, unlike a transit AS, this type of AS would not allow traffic from one AS to pass through on its way to another AS.</p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calico </tag>
            
            <tag> k8s </tag>
            
            <tag> SDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Calico Practicing-1 -- About Calico</title>
      <link href="2020/01/19/calico-practicing-1-about-calico/"/>
      <url>2020/01/19/calico-practicing-1-about-calico/</url>
      
        <content type="html"><![CDATA[<h1 id="About-Calico"><a href="#About-Calico" class="headerlink" title="About Calico"></a>About Calico</h1><p>I’m going to talk about my comprehension about Calico.<br>In addition, I’m only talking about cases upon <code>Kubernetes</code>.</p><ul><li><p>First of all, <code>Clico</code> is a project, not a single software.</p></li><li><p>Calico has variety of options for networking implementing. Whether in public cloud or on-prem. Whether L2 network or L3 network or even your environment doesn’t allow either L3 peering or L2 connectivity.</p><p>For detailed list of Calico networking options, please refer to this <a href="https://docs.projectcalico.org/v3.11/networking/determine-best-networking#unencapsulated-peered-with-physical-infrastructure" target="_blank" rel="noopener">page</a>.</p></li></ul><h1 id="Standardized-Glossary"><a href="#Standardized-Glossary" class="headerlink" title="Standardized Glossary"></a>Standardized Glossary</h1><ul><li><strong><em>Networking Option</em></strong>: An option is a combination of <code>networking backend</code>, <code>CNI</code>, <code>network policy</code>.</li><li><strong><em>network backend</em></strong>: The bottom implementation of network. Which are: IPIP, VXLAN, BGP. IPIP and VXLAN are overlay networking, which is under encapsulation. BGP is based on IP routing.</li><li><strong><em>network policy</em></strong>: Rules that enforce which network traffic that is allowed or denied in the network.</li></ul><p>The three things above are all logical concept, not specific software.<br>So, if you see something described as “using Flannel as networking backend, Calico as network policy”, don’t get confused. Every option is listed on Calico’s official site.</p><ul><li><strong><em>workload</em></strong>: A workload is a container or VM that Calico handles the virtual networking for. In Kubernetes, workloads are pods.</li><li><strong><em>workload endpoint</em></strong>: A workload endpoint is the virtual network interface a workload uses to connect to the Calico network.</li></ul><h1 id="Quick-Glance-of-Architecture"><a href="#Quick-Glance-of-Architecture" class="headerlink" title="Quick Glance of Architecture"></a>Quick Glance of Architecture</h1><p>Calico is a <code>modularized</code> system which is made up of the following independent components:</p><ul><li>Felix</li><li>The Orchestrator plugin</li><li>etcd</li><li>BIRD</li><li>BGP Router Reflector</li></ul><p>Let’s take a look at official architecture diagram.:arrow_down:</p><p><img src="calico-arch-gen-v3.2.svg" alt="You can see this is as a typical service mesh microcosmic architecture as well."><br>To take a look of detail, please refer to this <a href="https://docs.projectcalico.org/v3.11/reference/architecture/" target="_blank" rel="noopener">official document – Calico architecture</a></p><h2 id="Core-Concept"><a href="#Core-Concept" class="headerlink" title="Core Concept"></a>Core Concept</h2><ul><li>Assigns IP addresses to pods using Calico’s IP address management (IPAM)</li><li>Programs the local node’s routing table</li><li>Distributes routes to other nodes and network devices</li></ul><p>As we can see at the <a href="https://docs.projectcalico.org/v3.11/introduction/" target="_blank" rel="noopener">About Calico</a> page, Calico is a SDN, which allows system and network administrators to use their familiar tools for troubleshooting.</p><p>So, I’m going to give it a try to my first goal – to let microservice endpoint communicating directly with other endpoint which is in a Kubernetes cluster.</p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calico </tag>
            
            <tag> k8s </tag>
            
            <tag> SDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在Kubernetes中如何配置Traefik作为Ingress Controller暴露服务以及如何排错</title>
      <link href="2019/12/31/how-to-setup-traefik-as-ingress-controller-to-expose-service-from-kubernetes/"/>
      <url>2019/12/31/how-to-setup-traefik-as-ingress-controller-to-expose-service-from-kubernetes/</url>
      
        <content type="html"><![CDATA[<h1 id="使用-traefik-ingress方案。ingress配置了自定义域名，无法访问。"><a href="#使用-traefik-ingress方案。ingress配置了自定义域名，无法访问。" class="headerlink" title="使用 traefik-ingress方案。ingress配置了自定义域名，无法访问。"></a>使用 traefik-ingress方案。ingress配置了自定义域名，无法访问。</h1><p><strong>背景</strong><br>客户的服务申请了<strong>pingan.com</strong>集团二级域名,并配置到应用的 ingress 中，但是访问不了</p><p><strong>排错步骤</strong></p><ol><li>二级域名 <strong>pingan.com（本例）</strong> 属于外网 DNS 记录，公司办公网络解析不到地址，因此需要先判断 DNS 寻址是否正常。</li><li>如果 DNS 解析不到 IP 地址，则需要客户向网络组确认是否成功添加记录。</li><li>确认从域名到服务的网络路径。</li><li>依次判断网络是否连通，后端服务是否正常。</li></ol><p>下面是客户在servicebot上申请的域名解析: </p><table><thead><tr><th>url</th><th>vip</th></tr></thead><tbody><tr><td>xxx.pingan.com</td><td>xxx.xxx.xxx.xxx</td></tr></tbody></table><p>可以看到解析出了正确地址。</p><p>域名到服务的网络路径:</p><p><img src="kubernetes_ingress_web-service_flow.png" alt="Kubernetes_web-service_flow" title="Kubernetes_web-service_flow"></p><p>过程解释:<br>通常客户的应用需要提供对互联网的服务，都需要部署在 DMZ 网络区域中。</p><ol><li>客户通过网络组申请了公网域名，公网域名对应一个公网 IP(e.g. xxx.28.212.56)，公网 IP 将流量转发到 VIP。</li><li>ELB（负载均衡）在云平台上购置，会产生一个VIP，这个VIP将前端过来的流量转发到客户的 node 节点上。</li><li>在 node 上运行 traefik，监听 80 端口，接收来自 VIP 的请求。<br>traefik 将接收到的请求丢给apiserver，实际上 traefik 担任 ingress controller。</li><li>traefik 和 Kubernetes API 实时地打交道，获取 ingress 的信息，从而得知具体的 URL 与 service 对应关系。</li><li>traefik 将请求转发到后端 service。</li></ol><p>通过命令验证：</p><pre class="line-numbers language-shell"><code class="language-shell"># 首先要看 VIP 是否可访问# 这里例子是可访问的，当然就没问题了[root@SHB-L0117161 ~]# telnet xxx.128.118.241 80Trying xxx.128.118.241...Connected to xxx.128.118.241.Escape character is '^]'.exit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>1.验证VIP是否可访问，注意要在VIP所在网段，否则可能没有开通防火墙。</p><pre class="line-numbers language-shell"><code class="language-shell"># traefik label# 215.128.118.201 是 VIP 转发过来的其中一个 node[root@SHB-L0075967 ~]# kubectl get nodes -o=wide --show-labels | sed -n '1p;/215\.128\.118\.201/p'NAME              STATUS     ROLES     AGE       VERSION   EXTERNAL-IP   OS-IMAGE                  KERNEL-VERSION               CONTAINER-RUNTIME   LABELS215.128.118.201   Ready      <none>    42d       v1.9.1    <none>        CentOS Linux 7 (Core)     3.10.0-693.21.1.el7.x86_64   docker://17.5.0     CAAS_LOG_CLOUD=INSTALLING,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,caas_cluster=shb-dmz-fp-core-stg-106593af,host_name=SHB-L0117161,kubernetes.io/hostname=215.128.118.201,lb=traefik[root@SHB-L0075967 ~]# kubectl get pods -o=wide --namespace=kube-system | sed -n '1p;/215\.128\.118\.201/p'NAME                                      READY     STATUS             RESTARTS   AGE       IP                NODEcaas-log-helper-2hvqt                     1/1       Running            0          42d       172.1.115.2       215.128.118.201traefik-ingress-lb-4z76b                  1/1       Running            0          5d        215.128.118.201   215.128.118.201[root@SHB-L0075967 ~]#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.查看 VIP 的后端 node IP是否有运行 traefik，这里只查看了其中一个节点，如上图所示的三个节点都必须要有运行才正常。</p><pre class="line-numbers language-shell"><code class="language-shell">[root@SHB-L0117161 ~]# ifconfig eth0eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500        inet 215.128.118.201  netmask 255.255.255.192  broadcast 215.128.118.255        inet6 fe80::453:7aff:fe01:d3d3  prefixlen 64  scopeid 0x20<link>        ether 06:53:7a:01:d3:d3  txqueuelen 1000  (Ethernet)        RX packets 18242994  bytes 11646816745 (10.8 GiB)        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 17043276  bytes 25852317409 (24.0 GiB)        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0[root@SHB-L0117161 ~]# lsof -i :80COMMAND   PID USER   FD   TYPE   DEVICE SIZE/OFF NODE NAMEtraefik 23819 root    3u  IPv6 64172224      0t0  TCP *:http (LISTEN)[root@SHB-L0117161 ~]#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3.查看node上 traefik 是否正常监听，到这一步都正常，且确保防火墙开通的话，那浏览器过来的请求就已经到达 k8s 集群中了，剩下的就是 k8s 集群通过 ingress 组件找到具体服务。</p><pre class="line-numbers language-shell"><code class="language-shell"># ingress[root@SHB-L0075967 ~]# kubectl get ingresses -o=wide --namespace=shb-dmz-fp-core-stg-106593afNAME                    HOSTS                      ADDRESS   PORTS     AGEfp-core-outer-gateway   ifin-emp-stg1.pingan.com             80        17dises-ds-service         *                                    80        12d[root@SHB-L0075967 ~]# kubectl -o=yaml get ingresses fp-core-outer-gateway --namespace=shb-dmz-fp-core-stg-106593afapiVersion: extensions/v1beta1kind: Ingressmetadata:  creationTimestamp: 2018-10-12T03:58:19Z  generation: 1  name: fp-core-outer-gateway  namespace: shb-dmz-fp-core-stg-106593af  resourceVersion: "51549923"  selfLink: /apis/extensions/v1beta1/namespaces/shb-dmz-fp-core-stg-106593af/ingresses/fp-core-outer-gateway  uid: 0e12b199-cdd3-11e8-9a84-06f8b40001a8spec:  rules:  - host: xxxx-emp-stg1.pingan.com    http:      paths:      - backend:          serviceName: fp-core-outer-gateway          servicePort: 8088        path: /status:  loadBalancer: {}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4.验证一下 ingress 中所配置的 URL 与 后端 service 的对应关系是否正确。</p><p>*<em>PS. 本次举例的问题最后排查到是三台负责负载的客户 node 的其中一台上，flanneld 组件挂了，导致 ELB 判断服务不可用。所以必须保证负载节点所有节点服务一直正常。换句话说，ELB 只管负载均衡，不管高可用。<br>*</em></p><blockquote><p>Ingress简介: </p><ul><li>理解 Ingress<br>简单的说，Ingress 就是从 Kubernetes <strong>集群外</strong>访问集群的入口，将用户的 URL 请求转发到不同的 service 上。Ingress 相当于 nginx、apache 等负载均衡反向代理服务器，其中还包括规则定义，即 URL 的路由信息，路由信息的刷新由 Ingress controller 来提供。</li><li>理解 Ingress Controller<br>Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 Kubernetes API 打交道，实时地感知后端 service、pod 等变化，比如新增和减少 pod，service 增加和减少等；当得到这些变化信息后， Ingress Controller 再结合 Ingress 生产配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。</li></ul></blockquote><blockquote><p>介绍 Traefik:<br>Traefik 是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Story about microservice migrating from cloud host to Kubernetes - 01</title>
      <link href="2019/12/11/wei-fu-wu-cong-xu-ni-ji-zhuan-k8s-de-ji-lu-01/"/>
      <url>2019/12/11/wei-fu-wu-cong-xu-ni-ji-zhuan-k8s-de-ji-lu-01/</url>
      
        <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul><li><strong>IoT品牌网站</strong>基础设施列表<ul><li><a href="brand-website/A">A（旧架构）</a></li><li><a href="brand-website/B">B（新架构）</a></li><li><a href="brand-website/C">C（新架构）</a></li></ul></li></ul><h1 id="架构、备忘和注意事项"><a href="#架构、备忘和注意事项" class="headerlink" title="架构、备忘和注意事项"></a>架构、备忘和注意事项</h1><h2 id="理解IoT事业部网站产品的技术栈"><a href="#理解IoT事业部网站产品的技术栈" class="headerlink" title="理解IoT事业部网站产品的技术栈"></a>理解IoT事业部网站产品的技术栈</h2><ul><li><p>前端 – Node.js</p><p>在Node.js诞生之前，Javascript主要用来做客户端的开发。<br>在Node.js诞生之后，Javascript可以用来做完整的后台开发。<br>这其中的原理很简单，Node.js的解析器(runtime)就相当于一个Chrome浏览器，并且对于OS拥有全部访问权限。<br>运行在浏览器上的Javascript受限于浏览器的权限，而运行在Node.js中的Javascript可以完全操作OS。</p><p>所以，简单来说，就是现在你可以用Javascript来写跟Python、Java一样功能的APP啦。</p><p>IoT网站项目前端使用的框架是风头正热的<a href="https://vuejs.org" target="_blank" rel="noopener"><strong>Vue.js</strong></a>。</p></li><li><p>后端 – Spring Boot</p><p>Spring Boot是基于Java编程语言的微服务框架。<br>我们来看下它的组件逻辑图。</p><p><img src="spring_boot_microservice.png" alt="Spring Boot Microservice" title="MSA with Spring Cloud (by A. Lukyanchikov)"></p><p>可以看到关键的组件有（括号内是实际使用的软件名）：</p><ul><li><strong>Gateway – zuul</strong></li><li><strong>Service Discovery – eureka</strong></li><li><strong>Config Service – config server</strong></li></ul></li></ul><hr><h2 id="拓扑"><a href="#拓扑" class="headerlink" title="拓扑"></a>拓扑</h2><h3 id="新架构（逻辑拓扑）"><a href="#新架构（逻辑拓扑）" class="headerlink" title="新架构（逻辑拓扑）"></a>新架构（逻辑拓扑）</h3><p>我不了解Spring框架，但是作为运维方，我们比较关心的有这些问题：</p><ul><li><p>在Spring Boot自己的API Gateway之前，还有我们的Nginx，如果做HA架构，是否只需单纯地增加反向代理的后端节点？</p><p>答案是肯定的。注册到后端微服务框架自身的gateway服务中的业务服务有自己的名字，只要前端对应地访问相应的名字就行。</p></li><li><p>HA架构下，是否可以有多个Service Discovery、Config Service组件同时工作？</p><p>答案是肯定的。现在作为registry组件的方案一般有Eureka, Zookeeper, Consul。<br>他们都有各自的集群实现方式。</p></li></ul><h3 id="单节点情况下部署的情况"><a href="#单节点情况下部署的情况" class="headerlink" title="单节点情况下部署的情况"></a>单节点情况下部署的情况</h3><p><img src="IoT_website_one_server.png" alt="IoT website on one server" title="IoT website on one server"></p><h3 id="多节点情况下HA部署的情况"><a href="#多节点情况下HA部署的情况" class="headerlink" title="多节点情况下HA部署的情况"></a>多节点情况下HA部署的情况</h3><p>可以把上图的Nginx抽出来<strong>单独放（在生产环境下应该放在一个public子网）</strong>，作为一个整体的网关入口，<strong>前端服务和后端服务（这些都放在private子网）</strong>都用这个网关做负载均衡。</p><p>e.g.</p><p>前端:</p><ul><li>server_name: registration.product-1.com ==&gt; 192.168.100.1:3001, 192.168.100.2:3001</li><li>server_name: registration.product-2.com ==&gt; 192.168.100.1:3002, 192.168.100.2:3002</li><li>…</li></ul><p>后端gateway:</p><ul><li>server_name: aggregated-api-gateway.group.com ==&gt; 192.168.100.1:3001, 192.168.100.2:3001</li><li>…</li></ul><p>前端访问后端:</p><ul><li>访问product-1后端服务 ==&gt; aggregated-api-gateway.group.com/gateway/product-1-user/</li><li>访问product-2后端服务 ==&gt; aggregated-api-gateway.group.com/gateway/product-2-user/</li><li>…</li></ul><h3 id="旧架构（物理拓扑）（待补充）"><a href="#旧架构（物理拓扑）（待补充）" class="headerlink" title="旧架构（物理拓扑）（待补充）"></a>旧架构（物理拓扑）（待补充）</h3><p>不补充了，乱七八糟……<br>参见部署product-3德国站时总结的笔记。</p><hr><h2 id="备忘和注意事项"><a href="#备忘和注意事项" class="headerlink" title="备忘和注意事项"></a>备忘和注意事项</h2><ul><li><p>IoT事业部的软件架构中，所谓新旧架构，指的是<strong>物理拓扑</strong>上的概念，不是<strong>逻辑拓扑</strong>上的概念。</p><p>i.e.</p><p><strong>新架构</strong>：每个产品使用自己独立的基础设施。</p><p><strong>旧架构</strong>：有些组件会部署在共用的基础设施上。</p></li><li><p>旧架构下的网站列表（待补充）</p><ul><li>Ahome.com</li></ul></li><li><p>新架构下的网站列表（待补充）</p><ul><li>registration.B.com</li><li>registration.C.com</li></ul></li><li><p>旧的网站项目使用的各个组件的配置文件的仓库</p><ul><li><p>内网环境共用的Service Discovery服务 – 10.20.0.152:8761</p></li><li><p>其Config Service服务 – 10.20.0.152:8763</p><p><a href="http://gitlab.company.com.cn/system_operation/erp-config/tree/master" target="_blank" rel="noopener">http://gitlab.company.com.cn/system_operation/erp-config/tree/master</a></p><p>在这里看到注册进来的具体服务所使用的配置文件。我猜Spring Boot的业务逻辑是：</p><ol><li>Service Discovery启动。</li><li>Config Service和Service Discovery建立会话。</li><li>其他服务和Service Discovery连接，“注册”到Service Discovery中。</li><li>可能是由Service Discovery告诉Config Service哪个服务注册进来了，你去拉取（git pull）这个服务的配置文件然后丢给他吧。</li></ol></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 微服务 </tag>
            
            <tag> Spring Boot </tag>
            
            <tag> Spring Cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Docker文件系统</title>
      <link href="2019/10/21/guan-yu-docker-wen-jian-xi-tong/"/>
      <url>2019/10/21/guan-yu-docker-wen-jian-xi-tong/</url>
      
        <content type="html"><![CDATA[<h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>调查 Docker 的文件系统</p><ul><li><p>有哪些文件系统。</p><ul><li>关于文件系统。</li><li>有什么特征。</li></ul></li><li><p>介绍 Docker 分层文件系统概念。</p><ul><li>通过实例验证文件系统分层，以及复用。<h1 id="Docker-的文件系统"><a href="#Docker-的文件系统" class="headerlink" title="Docker 的文件系统"></a>Docker 的文件系统</h1></li></ul></li><li><p>首先应该想到的是 aufs。</p><p>aufs 已经是比较古老的了。</p><p>实际上有以下集中类型可供使用。</p></li></ul><table><thead><tr><th>存储驱动</th><th>支持的文件系统</th></tr></thead><tbody><tr><td>overlay, overlay2</td><td>xfs, ext4</td></tr><tr><td>devicemapper</td><td>direct-lvm, loopback-lvm</td></tr><tr><td>aufs</td><td>xfs, ext4</td></tr><tr><td>btrfs</td><td>btrfs</td></tr><tr><td>zfs</td><td>zfs</td></tr><tr><td>vfs</td><td>其他文件系统</td></tr></tbody></table><ul><li>关于各种存储驱动的应用场景，可以参考官方文档<a href="https://docs.docker.com/storage/storagedriver/select-storage-driver/#suitability-for-your-workload" target="_blank" rel="noopener">Suitability for your workload</a>。</li></ul><p>注: aufs 在 Fedora、CentOS 上没有被支持。</p><h2 id="默认的文件系统是什么？"><a href="#默认的文件系统是什么？" class="headerlink" title="默认的文件系统是什么？"></a>默认的文件系统是什么？</h2><p>目前官方推荐 <strong>overlay2</strong> ，如果 OS 支持，在首次安装 Docker 便会采用此文件系统。以前有过默认采用 aufs 的年代，如果已经安装了 Docker 并使用 aufs，更新版本时仍会使用。</p><ul><li><p>如何查看当前使用的storage driver？</p><p>  <code>docker info</code></p><pre class="line-numbers language-bash"><code class="language-bash">  $ docker info  Containers: 34   Running: 16   Paused: 0   Stopped: 18  Images: 13  Server Version: 18.09.4  Storage Driver: overlay2   Backing Filesystem: xfs   Supports d_type: <span class="token boolean">true</span>   Native Overlay Diff: <span class="token boolean">true</span>  Logging Driver: json-file  Cgroup Driver: systemd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="DeviceMapper"><a href="#DeviceMapper" class="headerlink" title="DeviceMapper"></a>DeviceMapper</h2><ul><li><p>DeviceMapper 由2个磁盘构成，分别是 metadata 和 data。</p></li><li><p>拥有 Thin-Provisioning 功能。</p><p><img src="doc/DeviceMapper.png" alt="DeviceMapper"></p></li></ul><hr><h3 id="DeviceMapper-与-LVM-的关系"><a href="#DeviceMapper-与-LVM-的关系" class="headerlink" title="DeviceMapper 与 LVM 的关系"></a>DeviceMapper 与 LVM 的关系</h3><ul><li><p>DeviceMapper 使用 LVM。</p><ul><li>Direct LVM</li><li>Loopback LVM</li></ul><p>有以上2种方式。</p></li><li><p>mount 方式不一样。</p><p><img src="doc/DeviceMapper_and_LVM.png" alt="DeviceMapper_and_LVM"></p></li></ul><hr><h2 id="OverlayFS"><a href="#OverlayFS" class="headerlink" title="OverlayFS"></a>OverlayFS</h2><ul><li>OverlayFS是什么?<ul><li>lowerdir: 作为基层的目录</li><li>upperdir: 覆盖在基层状态目录上的目录</li></ul></li></ul><p>由以上部分组成，创建成如下的镜像:</p><p>  <img src="doc/OverlayFS.png" alt="OverlayFS"></p><h2 id="为什么要分层"><a href="#为什么要分层" class="headerlink" title="为什么要分层?"></a>为什么要分层?</h2><ul><li><p>Docker 在设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。镜像实际是由多层文件系统联合组成。</p></li><li><p>镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。</p><p>比如，删除前一层文件的操作，实际不是真的删除前一层的的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要的添加的东西，任何额外的东西应该在该层构建结束前清理掉。</p></li><li><p>分层存储的特征还使得镜像的复用、定制变得更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。</p></li></ul><h2 id="如何看分层-如何看分层复用"><a href="#如何看分层-如何看分层复用" class="headerlink" title="如何看分层?如何看分层复用?"></a>如何看分层?如何看分层复用?</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># pull a new image</span>$ docker pull busyboxUsing default tag: latestlatest: Pulling from library/busyboxfc1a6b909f82: Pull completeDigest: sha256:954e1f01e80ce09d0887ff6ea10b13a812cb01932a0781d6b0cc23f743a874fdStatus: Downloaded newer image <span class="token keyword">for</span> busybox:latest$ <span class="token function">clear</span>$ <span class="token function">ls</span> -lht总用量 12Kdrwx------. 3 root root     30 4月  13 16:05 289e5e2a228b1ff909050212126a2fa99dd6f2665134a19770e5b7eea50b2e86drwx------. 2 root root   8.0K 4月  13 16:05 l<span class="token comment" spellcheck="true"># be aware of 'docker/overlay2/l' is recording image layers' connection</span>$ <span class="token function">pwd</span>/var/lib/docker/overlay2/l$ <span class="token function">ls</span> -lht <span class="token operator">|</span> <span class="token function">head</span> -n5总用量 0lrwxrwxrwx. 1 root root 72 4月  13 16:38 HN6VUXSR7NBMXZW37AIXRTEUTM -<span class="token operator">></span> <span class="token punctuation">..</span>/7b36dab9d59a60efcd4a3995971ad062a25e6a390bf3f3e897292d53503a466d/difflrwxrwxrwx. 1 root root 77 4月  13 16:38 Y4QZHPBVBNBITRHGYYVM5TDHDY -<span class="token operator">></span> <span class="token punctuation">..</span>/7b36dab9d59a60efcd4a3995971ad062a25e6a390bf3f3e897292d53503a466d-init/difflrwxrwxrwx. 1 root root 72 4月  13 16:38 V6CDQKWZHYWB5JVR5DVERURLCD -<span class="token operator">></span> <span class="token punctuation">..</span>/202659682f2b0823d72cd8316717bbfd494dee22e85efd092d782d3927b04fed/difflrwxrwxrwx. 1 root root 77 4月  13 16:38 LCXZSOCIBLIKGLXHDKQV52ZKXG -<span class="token operator">></span> <span class="token punctuation">..</span>/202659682f2b0823d72cd8316717bbfd494dee22e85efd092d782d3927b04fed-init/diff$<span class="token comment" spellcheck="true"># let's check out what is in busybox image's filesystem</span>$ <span class="token function">pwd</span>/var/lib/docker/overlay2/289e5e2a228b1ff909050212126a2fa99dd6f2665134a19770e5b7eea50b2e86$ <span class="token function">ls</span> -lh总用量 4.0Kdrwxr-xr-x. 10 root root 96 4月  13 16:05 <span class="token function">diff</span>-rw-r--r--.  1 root root 26 4月  13 16:05 <span class="token function">link</span>$ <span class="token function">cd</span> <span class="token function">diff</span>$ <span class="token function">ls</span> -lh总用量 16Kdrwxr-xr-x. 2 root  root  12K 4月   2 12:32 bindrwxr-xr-x. 2 root  root    6 4月   2 12:32 devdrwxr-xr-x. 3 root  root   79 4月   2 12:33 etcdrwxr-xr-x. 2 65534 65534   6 4月   2 12:32 homedrwx------. 2 root  root    6 4月   2 12:32 rootdrwxrwxrwt. 2 root  root    6 4月   2 12:32 tmpdrwxr-xr-x. 3 root  root   18 4月   2 12:32 usrdrwxr-xr-x. 4 root  root   30 4月   2 12:32 var$ <span class="token function">cd</span> usr$ <span class="token function">ls</span> -lh总用量 0drwxr-xr-x. 2 bin bin 6 4月   2 12:32 sbin$<span class="token comment" spellcheck="true"># build my image</span>$ docker build -t go-server-9860 <span class="token keyword">.</span>Sending build context to Docker daemon  7.311MBStep 1/5 <span class="token keyword">:</span> FROM busybox ---<span class="token operator">></span> af2f74c517aaStep 2/5 <span class="token keyword">:</span> MAINTAINER https://github.com/aruruka ---<span class="token operator">></span> Running <span class="token keyword">in</span> f50d07d89177Removing intermediate container f50d07d89177 ---<span class="token operator">></span> d02fd95709deStep 3/5 <span class="token keyword">:</span> COPY server_9860 /usr/local/bin/ ---<span class="token operator">></span> 94f0965b4ce9Step 4/5 <span class="token keyword">:</span> WORKDIR /usr/local/bin/ ---<span class="token operator">></span> Running <span class="token keyword">in</span> f0afc045a941Removing intermediate container f0afc045a941 ---<span class="token operator">></span> cc90efd3c240Step 5/5 <span class="token keyword">:</span> ENTRYPOINT <span class="token punctuation">[</span><span class="token string">"server_9860"</span><span class="token punctuation">]</span> ---<span class="token operator">></span> Running <span class="token keyword">in</span> 51ca3dd962ecRemoving intermediate container 51ca3dd962ec ---<span class="token operator">></span> 2f31d1b7c458Successfully built 2f31d1b7c458Successfully tagged go-server-9860:latest<span class="token comment" spellcheck="true"># check out upper layer's contents</span><span class="token comment" spellcheck="true"># layer link info is in 'lower' file</span>$ <span class="token function">pwd</span> <span class="token punctuation">;</span> <span class="token function">ls</span>/var/lib/docker/overlay2/1fbb0c7d6b69db147dea4de13b70dad9cc2c98536869c3a53082ad961836403d<span class="token function">diff</span>  <span class="token function">link</span>  lower  work$ <span class="token function">cat</span> lowerl/UFX72NO2WA4Y7EELEHYCA6YXNZ$<span class="token comment" spellcheck="true"># newer file will be in 'diff' dir</span>$ <span class="token function">pwd</span>/var/lib/docker/overlay2/1fbb0c7d6b69db147dea4de13b70dad9cc2c98536869c3a53082ad961836403d/diff/usr/local/bin$ <span class="token function">ls</span> -lh总用量 7.0M-rwxrwxr-x. 1 root root 7.0M 4月  13 16:23 server_9860$<span class="token comment" spellcheck="true"># use docker history command to verify image content</span>$ docker images <span class="token operator">|</span> <span class="token function">sed</span> -n <span class="token string">'1p;/go-server/Ip'</span>REPOSITORY                                                                       TAG                 IMAGE ID            CREATED             SIZEgo-server-9860                                                                   latest              2f31d1b7c458        24 hours ago        8.51MB$ docker <span class="token function">history</span> go-server-9860IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT2f31d1b7c458        24 hours ago        /bin/sh -c <span class="token comment" spellcheck="true">#(nop)  ENTRYPOINT ["server_9860"]   0B</span>cc90efd3c240        24 hours ago        /bin/sh -c <span class="token comment" spellcheck="true">#(nop) WORKDIR /usr/local/bin/       0B</span>94f0965b4ce9        24 hours ago        /bin/sh -c <span class="token comment" spellcheck="true">#(nop) COPY file:f7a8106426c88c00…   7.31MB</span>d02fd95709de        24 hours ago        /bin/sh -c <span class="token comment" spellcheck="true">#(nop)  MAINTAINER https://github…   0B</span>af2f74c517aa        11 days ago         /bin/sh -c <span class="token comment" spellcheck="true">#(nop)  CMD ["sh"]                   0B</span><span class="token operator">&lt;</span>missing<span class="token operator">></span>           11 days ago         /bin/sh -c <span class="token comment" spellcheck="true">#(nop) ADD file:6051b0ebe4098ccbb…   1.2MB</span>$<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="容器层"><a href="#容器层" class="headerlink" title="容器层"></a>容器层</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># run a container by the image I just build and observe filesystem's change</span>$ docker run -d --name go-server-9860 -p 127.0.0.1:9860:9860/tcp --rm go-server-9860<span class="token punctuation">[</span>root@lab-01 1fbb0c7d6b69db147dea4de13b70dad9cc2c98536869c3a53082ad961836403d<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># mount | grep overlay</span>overlay on /var/lib/docker/overlay2/ee0374838eb0133ee43da57551c49ccd7b63eb04b9221d02e50c4966cdbae440/merged <span class="token function">type</span> overlay <span class="token punctuation">(</span>rw,relatime,seclabel,lowerdir<span class="token operator">=</span>/var/lib/docker/overlay2/l/ZEHVHEBBCDTTPJNQSCPZEU5MTC:/var/lib/docker/overlay2/l/7XHO5S33QZEIRZFKS4N4GZCPCO:/var/lib/docker/overlay2/l/UFX72NO2WA4Y7EELEHYCA6YXNZ,upperdir<span class="token operator">=</span>/var/lib/docker/overlay2/ee0374838eb0133ee43da57551c49ccd7b63eb04b9221d02e50c4966cdbae440/diff,workdir<span class="token operator">=</span>/var/lib/docker/overlay2/ee0374838eb0133ee43da57551c49ccd7b63eb04b9221d02e50c4966cdbae440/work<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="确认容器层下面的层级以及文件是否被复用"><a href="#确认容器层下面的层级以及文件是否被复用" class="headerlink" title="确认容器层下面的层级以及文件是否被复用"></a>确认容器层下面的层级以及文件是否被复用</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># check go-server container's id</span>$ docker <span class="token function">ps</span>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                      NAMES65a25b310253        go-server-9860      <span class="token string">"server_9860"</span>       5 hours ago         Up 5 hours          127.0.0.1:9860-<span class="token operator">></span>9860/tcp   go-server-9860<span class="token comment" spellcheck="true"># determine container's filesystem directory</span>$ docker inspect --format<span class="token operator">=</span><span class="token string">'{{json .GraphDriver.Data.MergedDir}}'</span> 65a25b310253<span class="token string">"/var/lib/docker/overlay2/ee0374838eb0133ee43da57551c49ccd7b63eb04b9221d02e50c4966cdbae440/merged"</span>$ <span class="token function">cd</span> /var/lib/docker/overlay2/ee0374838eb0133ee43da57551c49ccd7b63eb04b9221d02e50c4966cdbae440/merged$ <span class="token function">cd</span> usr/local/bin$ <span class="token function">ls</span> -lh总用量 7.0M<span class="token comment" spellcheck="true"># be careful of the inode of the program</span>-rwxrwxr-x. 1 root root 7.0M 4月  13 16:23 server_9860$ <span class="token function">ls</span> -i server_986033638603 server_9860<span class="token comment" spellcheck="true"># now let's find lower layers of the container layer</span>$ <span class="token function">cd</span> /var/lib/docker/overlay2/ee0374838eb0133ee43da57551c49ccd7b63eb04b9221d02e50c4966cdbae440/merged$ <span class="token function">cd</span> <span class="token punctuation">..</span>$ <span class="token function">ls</span> -lh总用量 8.0Kdrwxr-xr-x. 2 root root  6 4月  14 17:52 <span class="token function">diff</span>-rw-r--r--. 1 root root 26 4月  14 17:52 <span class="token function">link</span>-rw-r--r--. 1 root root 86 4月  14 17:52 lowerdrwxr-xr-x. 1 root root  6 4月  14 17:52 mergeddrwx------. 3 root root 18 4月  14 17:52 work$ <span class="token function">cat</span> lowerl/ZEHVHEBBCDTTPJNQSCPZEU5MTC:l/7XHO5S33QZEIRZFKS4N4GZCPCO:l/UFX72NO2WA4Y7EELEHYCA6YXNZ$<span class="token comment" spellcheck="true"># the second lower layer would be the server_9860 file layer which was build by myself</span>$ <span class="token function">ls</span> -AFlh /var/lib/docker/overlay2/l/7XHO5S33QZEIRZFKS4N4GZCPCOlrwxrwxrwx. 1 root root 72 4月  13 16:27 /var/lib/docker/overlay2/l/7XHO5S33QZEIRZFKS4N4GZCPCO -<span class="token operator">></span> <span class="token punctuation">..</span>/1fbb0c7d6b69db147dea4de13b70dad9cc2c98536869c3a53082ad961836403d/diff/$ <span class="token function">cd</span> /var/lib/docker/overlay2/l/<span class="token punctuation">..</span>/1fbb0c7d6b69db147dea4de13b70dad9cc2c98536869c3a53082ad961836403d/diff/$ <span class="token function">ls</span> -lh总用量 0drwxr-xr-x. 3 root root 19 4月  13 16:27 usr$ <span class="token function">cd</span> usr/local/bin$ <span class="token function">ls</span> -lh总用量 7.0M-rwxrwxr-x. 1 root root 7.0M 4月  13 16:23 server_9860<span class="token comment" spellcheck="true"># be aware of the inode is as same as the file in container's layer</span>$ <span class="token function">ls</span> -i server_986033638603 server_9860$<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对于kubernetes(k8s)入门需要了解的docker基础</title>
      <link href="2019/10/21/dui-yu-kubernetes-k8s-ru-men-xu-yao-liao-jie-de-docker-ji-chu/"/>
      <url>2019/10/21/dui-yu-kubernetes-k8s-ru-men-xu-yao-liao-jie-de-docker-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文假设读者都是开发者，并熟悉 Linux 环境，所以就不介绍基础的技术概念了。命令行环境以 Linux 示例。</p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><p>本文包括以下内容:</p><ol><li>在 CentOS7 下安装 Docker</li><li>制作 Docker 镜像</li><li>通过 Docker 部署服务<h1 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h1>对于已经熟悉的 Docker 的朋友，推荐阅读<a href="https://medium.com/@clous/a-tutorial-about-continuous-integration-and-continuous-delivery-by-dockerize-jenkins-pipeline-ca377b02889b" target="_blank" rel="noopener">使用 Docker + Jenkins 构建 CI/CD 环境</a>。<br>阅读完这篇文章，你将可以达到以下的效果：</li></ol><p><strong>| <em>只通过一个 commit(GitHub)，将应用通过 Docker 容器部署出来。</em></strong></p><p>这其中包括以下操作:</p><ul><li>检出代码</li><li>运行测试</li><li>编译代码</li><li>对代码进行 SonarQube (代码质量管理工具)分析</li><li>创建 Docker 镜像</li><li>推送镜像到Docker Hub</li><li>拉取并运行镜像</li></ul><p>简单地说，就是大概是神兵结合平安CaaS后台所做的事情。</p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="通过-Docker-启动一个简单的-web-应用"><a href="#通过-Docker-启动一个简单的-web-应用" class="headerlink" title="通过 Docker 启动一个简单的 web 应用"></a>通过 Docker 启动一个简单的 web 应用</h2><h3 id="在-CentOS7-下安装-Docker"><a href="#在-CentOS7-下安装-Docker" class="headerlink" title="在 CentOS7 下安装 Docker"></a>在 CentOS7 下安装 Docker</h3><p>Docker 现在有2个版本，分别是 Docker CE 和 Docker EE。这2个版本都是由官方维护的，EE 是商业版,由官方提供认证了的OS、云平台层的技术支持，并提供 LDAP、Active Directory之类的认证（鉴权）系统技术支持以及镜像认证等安全方面的支持。<br>本文以 Docker CE（社区版）为例。</p><ol><li><p>OS 前提条件</p><p> 要安装 Docker CE，你需要一个正在维护版本的 CentOS 7。归档版本没有得到测试以及支持。</p><p> <code>centos-extras</code> 仓库需要被启用。默认情况下是启用的。</p><p> <code>overlay2</code> 存储驱动是推荐使用的。</p></li><li><p>删除旧版本</p><p> 旧版本的 Docker 被称为 <code>docker</code> 或 <code>docker-engine</code>。如果它们有被安装，先删除它们，包括相关联的依赖组建。</p><pre class="line-numbers language-bash"><code class="language-bash"> <span class="token punctuation">(</span>sudo<span class="token punctuation">)</span> yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> 如果 <code>yum</code>报告说没有安装这些，也是OK的。</p><p> <code>/var/lib/docker</code>当中的内容包括镜像、容器、卷、网络等默认内容。Docker CE 安装包现在被称为 <code>docker-ce</code>。</p></li><li><p>安装 Docker CE</p><p> 安装 Docker CE 通常有3中方法：</p><ul><li>大多数人使用通过<a href="https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository" target="_blank" rel="noopener">包管理工具仓库</a>。由于步骤简单并易于管理版本，官方推荐使用这种方法。</li><li>有些人下载 RPM 包并<a href="https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository" target="_blank" rel="noopener">手动安装</a>，并且完全手动管理版本。在与互联网隔绝的环境，这种方法比较有用。</li><li>在测试以及开发环境，有些人通过<a href="https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository" target="_blank" rel="noopener">自动化脚本</a>来安装Docker。</li></ul></li><li><p>通过包管理工具仓库安装</p><p> 4.1 配置软件源仓库</p><ul><li>需要预先安装依赖包。<code>yum-utils</code>提供<code>yum-config-manager</code>工具，然后<code>device-mapper-persistent-data</code>和<code>lvm2</code>被<code>devicemapper</code>存储驱动依赖。<pre class="line-numbers language-bash"><code class="language-bash">  <span class="token punctuation">(</span>sudo<span class="token punctuation">)</span> yum <span class="token function">install</span> -y yum-utils \  device-mapper-persistent-data \  lvm2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li>使用下列命令配置稳定仓库源。<pre class="line-numbers language-bash"><code class="language-bash">  <span class="token punctuation">(</span>sudo<span class="token punctuation">)</span> yum-config-manager \  --add-repo \  https://download.docker.com/linux/centos/docker-ce.repo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><p> 4.2 安装 Docker CE</p><p> <code>(sudo) yum install docker-ce docker-ce-cli containerd.io</code></p><p> 如果命令行提示说需要接受GPG key，确认fingerprint与下列相同。<br> <code>060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35</code>，如果相同，接受它。</p><p> 4.3 (可选项)安装指定版本的Docker CE。</p><ul><li><p>排序列出仓库源中的可用版本<br>例如:</p><pre class="line-numbers language-bash"><code class="language-bash">  $ yum list docker-ce --showduplicates <span class="token operator">|</span> <span class="token function">sort</span> -r  docker-ce.x86_64  3:18.09.1-3.el7                     docker-ce-stable  docker-ce.x86_64  3:18.09.0-3.el7                     docker-ce-stable  docker-ce.x86_64  18.06.1.ce-3.el7                    docker-ce-stable  docker-ce.x86_64  18.06.0.ce-3.el7                    docker-ce-stable<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>通过有效的软件包全名来安装制定版本</p><p>包的全名由2部分组成。分别是 <strong>(1)包名</strong> (<code>docker-ce</code>)加上从第1个冒号(<code>:</code>)后开头，直到第1个连字符(<code>-</code>)结束的 <strong>(2)版本字符串</strong> (第2列)，中间由连字符(<code>-</code>)相连。</p><p>例如: <code>docker-ce-18.09.1</code>。</p><p><code>(sudo) yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io</code></p><p>完成以上步骤之后，Docker已经安装，系统中创建了一个<code>docker</code>组，但没有用户被添加到这个组。</p></li></ul><p> 4.4 启动 Docker</p><p> <code>(sudo) systemctl start docker</code></p><p> 4.5 验证 Docker CE 安装正常，通过运行<code>hello-world</code>镜像。</p><p> <code>(sudo) docker run hello-world</code></p></li></ol><h3 id="制作-Docker-镜像"><a href="#制作-Docker-镜像" class="headerlink" title="制作 Docker 镜像"></a>制作 Docker 镜像</h3><p>关于<code>docker</code>命令可以参考网友分享的<a href="https://github.com/wsargent/docker-cheat-sheet" target="_blank" rel="noopener">docker cheat sheet</a>(有中文版)。</p><ol><li><p>搜索可用的docker镜像</p><p> 使用docker最简单的方式莫过于从现有的容器镜像开始。Docker官方网站专门有一个页面来存储所有可用的镜像，网址是：<br> <a href="https://index.docker.io" target="_blank" rel="noopener">https://index.docker.io</a>。你可以通过浏览这个网页来查找你想要使用的镜像，或者使用命令行的工具来检索。</p><p> <img src="doc/searching_image_on_docker-hub.png" alt="搜索镜像"></p></li><li><p>拉取镜像<br> <code>(sudo) docker pull &lt;IMAGE_NAME/IMAGE_ID&gt;</code></p><pre class="line-numbers language-bash"><code class="language-bash"> $ docker images REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE hello-world                                latest              fce289e99eb9        3 months ago        1.84kB kindest/node                               v1.12.2             58eadc0ca522        5 months ago        1.5GB k8s.gcr.io/kube-proxy-amd64                v1.11.3             be5a6e1ecfa6        7 months ago        97.8MB k8s.gcr.io/kube-controller-manager-amd64   v1.11.3             a710d6a92519        7 months ago        155MB k8s.gcr.io/kube-apiserver-amd64            v1.11.3             3de571b6587b        7 months ago        187MB k8s.gcr.io/kube-scheduler-amd64            v1.11.3             ca1f38854f74        7 months ago        56.8MB k8s.gcr.io/coredns                         1.1.3               b3b94275d97c        10 months ago       45.6MB k8s.gcr.io/etcd-amd64                      3.2.18              b8df3b177be2        12 months ago       219MB k8s.gcr.io/pause                           3.1                 da86e6ba6ca1        15 months ago       742kB $ docker pull busybox Using default tag: latest latest: Pulling from library/busybox fc1a6b909f82: Pull complete Digest: sha256:954e1f01e80ce09d0887ff6ea10b13a812cb01932a0781d6b0cc23f743a874fd Status: Downloaded newer image <span class="token keyword">for</span> busybox:latest $ docker images REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE busybox                                    latest              af2f74c517aa        5 days ago          1.2MB hello-world                                latest              fce289e99eb9        3 months ago        1.84kB kindest/node                               v1.12.2             58eadc0ca522        5 months ago        1.5GB k8s.gcr.io/kube-proxy-amd64                v1.11.3             be5a6e1ecfa6        7 months ago        97.8MB k8s.gcr.io/kube-scheduler-amd64            v1.11.3             ca1f38854f74        7 months ago        56.8MB k8s.gcr.io/kube-apiserver-amd64            v1.11.3             3de571b6587b        7 months ago        187MB k8s.gcr.io/kube-controller-manager-amd64   v1.11.3             a710d6a92519        7 months ago        155MB k8s.gcr.io/coredns                         1.1.3               b3b94275d97c        10 months ago       45.6MB k8s.gcr.io/etcd-amd64                      3.2.18              b8df3b177be2        12 months ago       219MB k8s.gcr.io/pause                           3.1                 da86e6ba6ca1        15 months ago       742kB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>制作 Docker 镜像</p><p> 语法: <code>docker build [OPTIONS] PATH | URL | -</code></p><p> <code>docker build</code>命令从<code>Dockerfile</code>和一个”上下文”制作镜像。一个<code>build</code>的上下文是一个特定位置下的文件的集合，这个位置可以通过<code>PATH</code>或<code>URL</code>来指定。<br> 换句话说，你可以通过git仓库指定”上下文”。</p><p> 3.1 编写 Dockerfile搜索可用的docker镜像</p><p> 我用go语言写了一个简单的<a href="src/server_9860.go">web应用</a>。<br> 现在我基于基础镜像<a href="https://hub.docker.com/_/busybox" target="_blank" rel="noopener">busybox</a>来制作我自己的web应用镜像，取名为”go-server-9860”。</p><ul><li>编写 Dockerfile<br>Dockerfile可以参照<a href="dockerfiles/Dockerfile">此处</a>。</li></ul><p> 3.2 创建镜像</p><p> 使用<code>docker build</code>命令通过指定URL来创建镜像。</p><p> 例子:<br> <code>docker build -t go-server-9860 https://github.com/aruruka/hello-world.git#master:docker_basis</code></p><p> 3.3 运行镜像</p><p> 以后台方式启动一个容器:</p><p> <code>docker run -d --name go-server-9860 -p 127.0.0.1:9860:9860/tcp --rm go-server</code></p><p> 进入一个容器并分配一个终端:</p><p> <code>docker exec -it go-server-9860 sh</code></p><p> 给一个容器发送<code>SIGKILL</code>信号:</p><p> <code>docker kill go-server-9860</code></p></li></ol><h3 id="访问刚刚部署的web服务"><a href="#访问刚刚部署的web服务" class="headerlink" title="访问刚刚部署的web服务"></a>访问刚刚部署的web服务</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 查看正在云从的容器:</span>docker <span class="token function">ps</span> <span class="token operator">|</span> <span class="token function">sed</span> -n '1p<span class="token punctuation">;</span>/go-server/Ip<span class="token comment" spellcheck="true"># 测试端口是否在宿主机上监听</span>telnet 127.0.0.1 9860<span class="token comment" spellcheck="true"># 访问服务</span>curl http://127.0.0.1:9860/world<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s版本规划的意义</title>
      <link href="2019/10/21/k8s-ban-ben-gui-hua-de-yi-yi/"/>
      <url>2019/10/21/k8s-ban-ben-gui-hua-de-yi-yi/</url>
      
        <content type="html"><![CDATA[<h1 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h1><p>从以下几方面分析k8s版本的意义，作为维护生产环境k8s版本的参考标准。</p><ul><li>k8s版本号的含义。</li><li>每个版本的支持时期。</li><li>新版本和旧版本有什么区别？</li><li>新版本对我们开发产品调用k8s API接口有什么影响？</li><li>在生产环境中是否应该尽量使用新版本？</li></ul><hr><h1 id="k8s版本号的含义"><a href="#k8s版本号的含义" class="headerlink" title="k8s版本号的含义"></a>k8s版本号的含义</h1><p>Kubernetes版本表示为xyz，其中x是主要版本，y是次要版本，z是补丁版本，遵循<a href="http://semver.org/" target="_blank" rel="noopener">语义版本控制术语</a>。有关更多信息，请参阅<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#kubernetes-release-versioning" target="_blank" rel="noopener">Kubernetes发布版本</a>。</p><ul><li><p>k8s 发行版与 github 分支的关系</p><p>简单来讲，kubernetes项目存在3类分支(branch)，分别为<code>master</code>，<code>release-X.Y</code>,<code>release-X.Y.Z</code>。<br>master分支上的代码是最新的，每隔2周会生成一个发布版本(release)，由新到旧以此为<code>master</code>–&gt;<code>alpha</code>–&gt;<code>beta</code>–&gt;<code>Final release</code>，这当中存在一些cherry picking的规则，也就是说从一个分支上挑选一些必要pull request应用到另一个分支上。<br>我们可以认为<code>X.Y.0</code>为稳定的版本，这个版本号意味着一个<code>Final release</code>。一个<code>X.Y.0</code>版本会在<code>X.(Y-1).0</code>版本的3到4个月后出现。<br><code>X.Y.Z</code>为经过cherrypick后解决了必须的安全性漏洞、以及影响大量用户的无法解决的问题的补丁版本。<br>总体而言，我们一般关心<code>X.Y.0</code>(稳定版本)，和<code>X.Y.Z</code>(补丁版本)的特性。</p></li><li><p>例子</p><dl><dt><code>v1.14.0</code><br>: <code>1</code>为主要版本<br>: <code>14</code>为次要版本</dt><dd><code>0</code>为补丁版本</dd></dl></li></ul><hr><h1 id="每个版本的支持时期"><a href="#每个版本的支持时期" class="headerlink" title="每个版本的支持时期"></a>每个版本的支持时期</h1><p>Kubernetes项目维护最新三个次要版本的发布分支。结合上述<strong>一个<code>X.Y.0</code>版本会在<code>X.(Y-1).0</code>版本的3到4个月后出现</strong>的描述，也就是说1年前的版本就不再维护，每个次要版本的维护周期为9~12个月，就算有安全漏洞也不会有补丁版本。</p><hr><h1 id="新版本和旧版本有什么区别？"><a href="#新版本和旧版本有什么区别？" class="headerlink" title="新版本和旧版本有什么区别？"></a>新版本和旧版本有什么区别？</h1><p>综上所述，新版本与旧版本区别主要在于<strong>应用了社区中经过cherrypick挑选出来的PR以及修复了安全性漏洞、没有workaround(临时解决办法)的bug。</strong><br>以下链接中维护了所有当前的发行版的链接，可在此链接中查询相应版本与之前版本的区别:<br><a href="https://github.com/kubernetes/kubernetes/releases" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/releases</a></p><p>每个稳定版本之间的release note也可以在kubernetes官网上查阅到:<br><a href="https://kubernetes.io/docs/setup/release/notes/" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/release/notes/</a><br>这其中包括了一些版本升级前必须要确认的事宜，以<code>v1.14</code>为例:<br><a href="https://kubernetes.io/docs/setup/release/notes/#no-really-you-must-read-this-before-you-upgrade" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/release/notes/#no-really-you-must-read-this-before-you-upgrade</a></p><blockquote><p>KUBE-API服务器： 默认RBAC策略不再授予对未经身份验证的用户的发现和权限检查API（由kubectl auth can -i使用）的访问权限。升级的群集保留了先前的行为，但希望在新群集中授予未经身份验证的用户访问权限的群集管理员需要明确选择公开发现和/或权限检查API：<br>kubectl create clusterrolebinding anonymous-discovery –clusterrole=system:discovery –group=system:unauthenticated<br>kubectl create clusterrolebinding anonymous-access-review –clusterrole=system:basic-user –group=system:unauthenticated<br>…</p></blockquote><hr><h1 id="新版本对我们开发app调用k8s-API接口有什么影响？"><a href="#新版本对我们开发app调用k8s-API接口有什么影响？" class="headerlink" title="新版本对我们开发app调用k8s API接口有什么影响？"></a>新版本对我们开发app调用k8s API接口有什么影响？</h1><p>由于k8s本身是基于api的为服务架构，k8s系统内部也是通过互相调用api来运作的，总体而言kubernetes api在设计时遵循向上和/或向下兼容的原则。k8s的api是一个api的集合，称之为”API groups”。每一个API group维护着3个主要版本，分别是<code>GA</code>，<code>Beta</code>，<code>Alpha</code>。<br>API的弃用只会通过在新的<code>API group</code>启用的同时宣告旧<code>API group</code>将会弃用的方式来推进。GA版本在宣告启用后将会向下兼容12个月或3个发行版。Beta版本则为9个月或3个发行版。而Alpha则会立刻启用。<br>这个遵循kubernetes版本的升级规则，也就是整体而言集群升级不支持跨度在2个Final release发行版之上的操作。<br>每个发行版的release note中也有对API重大改动的描述。开发者们可以参阅其修改API。</p><hr><h1 id="在生产环境中是否应该尽量使用新版本？"><a href="#在生产环境中是否应该尽量使用新版本？" class="headerlink" title="在生产环境中是否应该尽量使用新版本？"></a>在生产环境中是否应该尽量使用新版本？</h1><p>从结论上来说，是的。原因也是由于上述的发行版都存在这对应的生命周期。<br>但值得注意的时，升级集群理论上只支持跨度为2个次要版本的操作。</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="https://kubernetes.io/docs/setup/version-skew-policy/" target="_blank" rel="noopener">Kubernetes Version and Version Skew Support Policy</a><br><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md" target="_blank" rel="noopener">Kubernetes Release Versioning</a><br><a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/" target="_blank" rel="noopener">Kubernetes Deprecation Policy</a><br><a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning" target="_blank" rel="noopener">The Kubernetes API</a></p>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS Support FAQ</title>
      <link href="2019/10/16/aws-support-faq/"/>
      <url>2019/10/16/aws-support-faq/</url>
      
        <content type="html"><![CDATA[<h2 id="内存相关"><a href="#内存相关" class="headerlink" title="内存相关"></a>内存相关</h2><ul><li><p><strong>Q: What’s the meaning of metric “FreeableMemory” on ElastiCache?</strong></p><p><code># Problem Description</code></p><p>======================</p><p>I understand that you are confused about these two values, “the memory of Node Type” and “the memory of freeable.” And you want to know which one or other metrics you should concern. If I misunderstood, please feel free to correct me.</p><p><code># Explanation</code></p><p>=================</p><p>As you know, when Redis is used as a cache, often, it is handy to let it automatically evict old data as you add a new one. There is a maxmemory setting directive used to configure Redis to use a specified amount of memory of the data set. When the specified amount of memory is reached, it is possible to select among different behaviors, called policies. Redis can return errors for commands that could result in more memory being used, or it can evict some old data in order to return back to the specified limit every time new data is added. To see more information, you can refer to [1].</p><p>When you launched a Redis and chose t2.micro in the Node type, the default maxmemory is 5819680 bytes, equivalent to 555 MB. Here is a document that shows the default values for the maxmemory for each node type. The value of maxmemory is the maximum number of bytes available to you for use, data, and other uses, on the node [2].</p><p>As for FreeableMemory, it represents the amount of free memory available on the host. This is derived from the RAM, buffers, and cache that the OS reports as freeable[3].</p><p>For Redis, there is a metric named “BytesUsedForCache.” This metric gives the exact size of the data stored by Redis or Memcache. This metric is obtained by running info or stats command in the engines[4].</p><p>FreeableMemory = RAM +SWAP - ( BytesUsedForCache + Host OS memory usage).</p><p>RAM+SWAP is constant. Therefore, if free memory is decreasing and BytesUsedForCache is constant, it means it is getting utilized by OS cache. The thing about OS cache is if free memory is there, OS likes to keep it utilized, but if there is memory pressure in the machine, it can also be freed by the OS to give it to other processes like Redis or Memcache.</p><p>To answer your question, “what kind of metrics should I focus on?” My answer is that you should monitor the BytesUsedForCache. For example, you can put a cloudwatch alarm on BytesUsedForCache if it goes above 75% of the maxmemory in the node.</p><p>I hope the information above answered your questions.</p><p>Should you have any further questions, please feel free to reach back and I will be glad to assist you further.</p><p><code># References</code></p><p>==============================</p><p>[1] <a href="https://redis.io/topics/lru-cache" target="_blank" rel="noopener">https://redis.io/topics/lru-cache</a></p><p>[2] Redis Specific Parameters - Redis Node-Type Specific Parameters, <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/ParameterGroups.Redis.html#ParameterGroups.Redis.NodeSpecific" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/ParameterGroups.Redis.html#ParameterGroups.Redis.NodeSpecific</a></p><p>[3] Host-Level Metrics - <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.HostLevel.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.HostLevel.html</a></p><p>[4] Metrics for Redis - <a href="https://docs.aws.amazon.com/en_us/AmazonElastiCache/latest/red-ug/CacheMetrics.Redis.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/en_us/AmazonElastiCache/latest/red-ug/CacheMetrics.Redis.html</a></p><p>这次FAQ的概括：</p><ol><li><strong>FreeableMemory</strong>这个metric的意思是ElastiCache实例的宿主机的可用内存+可用swap。</li><li>如果<strong>FreeableMemory</strong>持续下降，而<strong>BytesUsedForCache</strong>稳定，那么说明宿主机的OS占用的内存变多了。</li><li>OS有占用资源的倾向，OS是这么设计的。OS会为应用程序释放自己占用的资源，我们不太需要关心OS占用了多少资源，只要确保OS有可用(free)资源就行了。</li><li>ElastiCache实际使用了多少内存是看<strong>BytesUsedForCache</strong>。</li><li>ElastiCache实例的最大内存大小是和实例的Node Type相关的，参考<strong>[2]</strong>。</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes应用管理工具插件helm的安装与使用</title>
      <link href="2019/10/09/kubernetes-ying-yong-guan-li-gong-ju-cha-jian-helm-de-an-zhuang-yu-shi-yong/"/>
      <url>2019/10/09/kubernetes-ying-yong-guan-li-gong-ju-cha-jian-helm-de-an-zhuang-yu-shi-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="helm组件结构"><a href="#helm组件结构" class="headerlink" title="helm组件结构"></a>helm组件结构</h1><hr><p>Helm 是 C/S 架构，主要分为客户端 <code>helm</code> 和服务端 <code>Tiller</code>。安装时可直接在 <a href="https://github.com/helm/helm/releases/tag/v2.14.0" target="_blank" rel="noopener">Helm 仓库的 Release 页面</a> 下载所需二进制文件或者源码包。<br>由于存储在GCS中，注意需要科学上网。<br>下载二进制可执行程序的文件包后解压文件，这里以 Linux amd64 为例。</p><pre><code>[root@master helm]# ls -lhtotal 26M-rw-r--r--. 1 root root 26M May 29 11:41 helm-v2.14.0-linux-amd64.tar.gzdrwxr-xr-x. 2 root root  64 May 15 11:44 linux-amd64[root@master helm]# tree linux-amd64/linux-amd64/├── helm├── LICENSE├── README.md└── tiller0 directories, 4 files[root@master helm]#</code></pre><p>解压完成后，可看到其中包含 <code>helm</code> 和 <code>tiller</code> 二进制文件。</p><h2 id="客户端-helm"><a href="#客户端-helm" class="headerlink" title="客户端 helm"></a>客户端 helm</h2><hr><p><code>helm</code> 是个二进制文件，直接将它移动至 <code>/usr/bin</code> 目录下即可。</p><pre><code>[root@master helm]# cp linux-amd64/helm /usr/bin/[root@master helm]# which helm/usr/bin/helm</code></pre><p>这时候便可直接通过 helm 命令使用了。比如，我们验证当前使用的版本。</p><pre><code>[root@master helm]# helm versionClient: &amp;version.Version{SemVer:&quot;v2.14.0&quot;, GitCommit:&quot;05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0&quot;, GitTreeState:&quot;clean&quot;}Error: could not find tiller[root@master helm]#</code></pre><p>这里解释一下，为什么helm工具直接就可以访问k8s的apiserver了呢？<br>其实helm是先找当前主机上的kubectl的配置文件(环境变量 <code>$KUBECONFIG</code> 或者 <code>~/.kube/config</code> 中的内容)，通过解析这个来获得apiserver的地址、以及用来访问的证书，以此来访问apiserver。</p><h2 id="服务端-Tiller"><a href="#服务端-Tiller" class="headerlink" title="服务端 Tiller"></a>服务端 Tiller</h2><hr><p>以下讨论中，前提都是 环境变量<code>$KUBECONFIG</code>或文件<code>$HOME/.kube/config</code> 已正确配置，并且 <code>kebectl</code> 有集群管理员级别操作集群的权限。</p><h3 id="本地安装"><a href="#本地安装" class="headerlink" title="本地安装"></a>本地安装</h3><p>刚才我们解压的文件中，还有一个二进制文件 <code>tiller</code> 。我们可以直接执行它，用于在本地启动服务。</p><pre><code>[root@master linux-amd64]# ./tiller[main] 2019/05/29 13:27:15 Starting Tiller v2.14.0 (tls=false)[main] 2019/05/29 13:27:15 GRPC listening on :44134[main] 2019/05/29 13:27:15 Probes listening on :44135[main] 2019/05/29 13:27:15 Storage driver is ConfigMap[main] 2019/05/29 13:27:15 Max history per release is 0</code></pre><p>直接执行时，默认会监听 <code>44134</code> 和 <code>44135</code> 端口，<code>44134</code> 端口用于和 <code>helm</code> 进行通信，而 <code>44135</code> 主要是用于做探活的，在部署至 <code>K8S</code> 时使用。<br>当我们使用客户端连接时，只需设置 <code>HELM_HOST</code> 环境变量即可。</p><pre><code>[root@master ~]# export HELM_HOST=localhost:44134You have new mail in /var/spool/mail/root[root@master ~]# helm versionClient: &amp;version.Version{SemVer:&quot;v2.14.0&quot;, GitCommit:&quot;05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.14.0&quot;, GitCommit:&quot;05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0&quot;, GitTreeState:&quot;clean&quot;}[root@master ~]#</code></pre><h3 id="默认安装"><a href="#默认安装" class="headerlink" title="默认安装"></a>默认安装</h3><p>官方提供了一种一键式安装的方式。那便是 <code>helm init</code> 执行这条命令后，会同时在 K8S 中部署服务端 <code>Tiller</code> 和初始化 <code>helm</code> 的默认目录 <code>$HELM_HOME</code> 默认值为 <code>$HOME/.helm</code>。</p><p>这种方式下会默认使用官方镜像 <code>k8s.gcr.io/kubernetes-helm/tiller</code> 网络原因可能会导致安装失败。所以我从阿里云的docker镜像仓库上pull下来后重新打了tag。</p><p>如果不用这种默认镜像名，也可以在helm init时指定镜像名。<br><code>helm init --tiller-image IMAGE_REPOSITORY/tiller:v2.14.0</code></p><pre><code>#!/usr/bin/env bashimages=(    tiller:v2.14.0)for imageName in ${images[@]} ; do    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageNamedone</code></pre><p>然后执行helm自动安装命令。</p><pre><code>[root@master linux-amd64]# helm initCreating /root/.helmCreating /root/.helm/repositoryCreating /root/.helm/repository/cacheCreating /root/.helm/repository/localCreating /root/.helm/pluginsCreating /root/.helm/startersCreating /root/.helm/cache/archiveCreating /root/.helm/repository/repositories.yamlAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.comAdding local repo with URL: http://127.0.0.1:8879/charts$HELM_HOME has been configured at /root/.helm.Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy.To prevent this, run `helm init` with the --tiller-tls-verify flag.For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installationYou have new mail in /var/spool/mail/root[root@master linux-amd64]#</code></pre><p>再次执行 helm version 测试服务端 tiller 是否成功安装。</p><pre><code>[root@master linux-amd64]# helm versionClient: &amp;version.Version{SemVer:&quot;v2.14.0&quot;, GitCommit:&quot;05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0&quot;, GitTreeState:&quot;clean&quot;}Error: could not find a ready tiller podYou have new mail in /var/spool/mail/root[root@master linux-amd64]#</code></pre><p>发现 tiller 并没有成功部署。<br>于是调查了一下 deployment 的状态，发现 image 的名称和我刚才打的tag不一致……</p><pre><code>[root@master linux-amd64]# kubectl get deploy -o wide --all-namespaces | sed -n &#39;1p;/tiller/Ip&#39;NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS                IMAGES                                  SELECTORkube-system   tiller-deploy             0/1     1            0           32m     tiller                    gcr.io/kubernetes-helm/tiller:v2.14.0   app=helm,name=tiller[root@master linux-amd64]#</code></pre><p>重新打一下tag。</p><pre><code>[root@master linux-amd64]# docker images | sed -n &#39;1p;/tiller/Ip&#39;REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZEk8s.gcr.io/tiller                    v2.14.0             a982228e2db1        2 weeks ago         94.2MB[root@master linux-amd64]# docker tag k8s.gcr.io/tiller:v2.14.0 gcr.io/kubernetes-helm/tiller:v2.14.0 &amp;&amp; \&gt; docker rmi k8s.gcr.io/tiller:v2.14.0Untagged: k8s.gcr.io/tiller:v2.14.0[root@master linux-amd64]# docker images | sed -n &#39;1p;/tiller/Ip&#39;REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZEgcr.io/kubernetes-helm/tiller        v2.14.0             a982228e2db1        2 weeks ago         94.2MB[root@master linux-amd64]#</code></pre><p>由于我的环境是一个master加一个sysnode，我想在sysnode上运行集群相关的addons，所以将 <code>tiller</code> 的镜像导出出来，传到sysnode上导入。</p><dl><dt>sysnode</dt><dd>sysnode这个role的名字是通过给node打一个<code>topology key</code>，<code>node-role.kubernetes.io/sysnode=sysnode</code>，这样实现的。</dd></dl><pre><code>[root@master linux-amd64]# kubectl get noNAME      STATUS   ROLES     AGE     VERSIONmaster    Ready    master    6d10h   v1.14.0sysnode   Ready    sysnode   6d9h    v1.14.0[root@master linux-amd64]# kubectl get no --show-labels | sed -n &#39;1p;/sysnode/Ip&#39;NAME      STATUS   ROLES     AGE     VERSION   LABELSsysnode   Ready    sysnode   6d9h    v1.14.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=sysnode,kubernetes.io/os=linux,node-role.kubernetes.io/sysnode=sysnode[root@master linux-amd64]#</code></pre><p>在sysnode上导入镜像后tiller成功部署了。<br>helm 客户端也成功连接上 tiller了。</p><pre><code>[root@master linux-amd64]# kubectl get deploy -o wide -n kube-system | sed -n &#39;1p;/tiller/Ip&#39;NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS                IMAGES                                  SELECTORtiller-deploy             1/1     1            1           61m     tiller                    gcr.io/kubernetes-helm/tiller:v2.14.0   app=helm,name=tiller[root@master linux-amd64]# helm versionClient: &amp;version.Version{SemVer:&quot;v2.14.0&quot;, GitCommit:&quot;05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.14.0&quot;, GitCommit:&quot;05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0&quot;, GitTreeState:&quot;clean&quot;}You have new mail in /var/spool/mail/root[root@master linux-amd64]#</code></pre><h3 id="手动安装"><a href="#手动安装" class="headerlink" title="手动安装"></a>手动安装</h3><p>通过上面的描述，可能你已经发现，安装服务端，其实也就是一次普通的部署，我们可以通过以下方式来自行通过 <code>kubectl</code> 完成部署。</p><pre><code>[root@master linux-amd64]# helm init --dry-run --debug # 篇幅原因，以下内容进行了省略---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  creationTimestamp: null  labels:    app: helm    name: tiller  name: tiller-deploy  namespace: kube-systemspec:  replicas: 1  strategy: {}  ...status: {}---apiVersion: v1kind: Servicemetadata:  creationTimestamp: null  labels:    app: helm    name: tiller  name: tiller-deploy  namespace: kube-systemspec:  ports:  - name: tiller    port: 44134    targetPort: tiller  selector:    app: helm    name: tiller  type: ClusterIPstatus:  loadBalancer: {}...[root@master linux-amd64]#</code></pre><p>将输出内容保存至文件中，自行修改后，通过 <code>kubectl</code> 进行部署即可。建议在修改过程中，尽量不要去更改标签及选择器。</p><h3 id="修改默认的stable-repository"><a href="#修改默认的stable-repository" class="headerlink" title="修改默认的stable repository"></a>修改默认的stable repository</h3><p>由于国内网络环境，我们可以在helm init的时候指定一个stable repo url，而不用默认的Google的repo。<br><code>helm init --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts ... # 这里是使用的阿里的repo</code><br>如果已经配置了默认的stable repo,想要修改，可以参照<a href="https://helm.sh/docs/helm/#helm-repo" target="_blank" rel="noopener">官方文档</a>的helm repo命令。</p><h3 id="关于helm的安全性问题"><a href="#关于helm的安全性问题" class="headerlink" title="关于helm的安全性问题"></a>关于helm的安全性问题</h3><h4 id="如何安全地安装helm"><a href="#如何安全地安装helm" class="headerlink" title="如何安全地安装helm"></a>如何安全地安装helm</h4><p>我们应该注意到，现在helm客户端和tiller服务端都已经安装好，并且helm客户端是通过当前节点上kubectl的配置文件中设置的权限来执行对apiserver的操作的，这在专用的集群下，没有问题，如果你和其他用户共享这个集群，并且不论是不是专用的集群，如果是生产环境下使用tiller，强烈建议遵照 <a href="https://helm.sh/docs/using_helm/#securing-your-helm-installation" target="_blank" rel="noopener">helm 官方secure指引</a>，配置必须的安全选项。<br>大致上来说，和tiller默认的安装(部署)方式有以下区别:</p><ul><li>release的数据并不会以configmap的形式存放于集群中，而是以secret的形式存放。</li><li>tiller的api服务启用tls认证。</li><li>给tiller服务分配一个有合适权限的serviceAccount。</li></ul><h4 id="关于为什么要这么做，以及由此可以拓展出在集群内部署拥有操作apiserver的应用时的安全性考虑"><a href="#关于为什么要这么做，以及由此可以拓展出在集群内部署拥有操作apiserver的应用时的安全性考虑" class="headerlink" title="关于为什么要这么做，以及由此可以拓展出在集群内部署拥有操作apiserver的应用时的安全性考虑"></a>关于为什么要这么做，以及由此可以拓展出在集群内部署拥有操作apiserver的应用时的安全性考虑</h4><ul><li>在实际生产环境应用中，往往需要配置一个客户端(kubectl或某种编程语言的k8s client)控制多个k8s集群的情况，如果有高手通过某些途径获得了其中一个k8s集群的地址，就能访问到没有做安全限制的tiller的api，从而达到绕过k8s本身的rbac权限控制的情况。</li><li>假如业务应用运行在k8s上，此应用又被黑客控制了，就可以通过这个应用获得访问apiserver的权限，进而操纵没有认证控制的tiller。</li></ul><h2 id="RBAC-使用"><a href="#RBAC-使用" class="headerlink" title="RBAC 使用"></a>RBAC 使用</h2><hr><p>上面的内容中，均未提及到权限控制相关的内容，但是在生产环境中使用，我们一般都是会进行权限控制的。<br>这里我们创建一个 <code>ServiceAccount</code> 命名为 <code>tiller</code>，为了简单，我们直接将它与 <code>cluster-admin</code> 进行绑定。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> tiller  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> tiller<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> cluster<span class="token punctuation">-</span>admin<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> tiller    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将此内容保存为 <code>tiller-rbac.yaml</code>，开始进行部署操作。</p><pre><code>[root@master helm]# kubectl apply -f tiller-rbac.yamlserviceaccount/tiller createdclusterrolebinding.rbac.authorization.k8s.io/tiller created</code></pre><p>更新一下tiller的设置(此处为在deployment中设置serviceaccount)。</p><pre><code>[root@master helm]# helm init --service-account tiller --history-max 200 --upgrade$HELM_HOME has been configured at /root/.helm.Tiller (the Helm server-side component) has been upgraded to the current version.You have new mail in /var/spool/mail/root[root@master helm]# kubectl get deploy tiller-deploy -o yaml -n kube-system | grep -i serviceaccount      automountServiceAccountToken: true      serviceAccount: tiller      serviceAccountName: tillerYou have new mail in /var/spool/mail/root[root@master helm]#</code></pre><p>以此方式完成部署。</p><h1 id="用helm部署应用"><a href="#用helm部署应用" class="headerlink" title="用helm部署应用"></a>用helm部署应用</h1><hr><h2 id="helm-概念"><a href="#helm-概念" class="headerlink" title="helm 概念"></a>helm 概念</h2><hr><h3 id="Chart"><a href="#Chart" class="headerlink" title="Chart"></a>Chart</h3><p><code>chart</code> 就是 Helm 所管理的包，类似于 <code>Yum</code> 所管理的 <code>rpm</code> 包或者是 <code>Homebrew</code> 管理的 <code>Formulae</code>。它包含着一个应用要部署至 K8S 上所必须的所有资源。</p><h3 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h3><p><code>release</code> 就是 <code>chart</code> 在 K8S 上部署后的实例。<code>chart</code> 的每次部署将产生一次 <code>Release</code>。这和上面类比的包管理器就有所不同了，多数的系统级包管理器所安装的包只会在系统中存在一份。我们可以以 <code>Pip</code> 在虚拟环境下单得包安装，或者 <code>Npm</code> 的local install来进行类比。</p><h3 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h3><p><code>repository</code> 就是字面意思，存储 <code>chart</code> 的仓库。还记得我们上面执行 <code>helm init</code> 时的输出吗？默认情况下，初始化 Helm 的时候，会添加两个仓库，一个是 stable 仓库kubernetes-charts.storage.googleapis.com，另一个则是 <code>local</code> 仓库，地址是<a href="http://127.0.0.1:8879/charts。" target="_blank" rel="noopener">http://127.0.0.1:8879/charts。</a></p><h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><p>前面提到了 <code>chart</code> 是应用程序所必须的资源，当然我们实际部署的时候，可能就需要有些自定义的配置了。<code>Config</code> 便是用于完成此项功能的，在部署的时候，会将 <code>config</code> 与 <code>chart</code> 进行合并，共同构成我们将部署的应用。</p><h2 id="chart-结构"><a href="#chart-结构" class="headerlink" title="chart 结构"></a>chart 结构</h2><hr><p>在项目目录下，通过以下命令创建一个 <code>chart</code>。</p><pre><code>[root@master helm]# pwd/root/helm[root@master helm]# helm create saythxCreating saythx[root@master helm]# tree -a saythxsaythx├── charts├── Chart.yaml├── .helmignore├── templates│   ├── deployment.yaml│   ├── _helpers.tpl│   ├── ingress.yaml│   ├── NOTES.txt│   ├── service.yaml│   └── tests│       └── test-connection.yaml└── values.yaml3 directories, 9 files[root@master helm]#</code></pre><p>先来看一下默认创建的 <code>chart</code> 中包含了什么文件和目录，对其进行解释。</p><h3 id="Chart-yaml"><a href="#Chart-yaml" class="headerlink" title="Chart.yaml"></a>Chart.yaml</h3><pre><code>[root@master helm]# cat saythx/Chart.yamlapiVersion: v1appVersion: &quot;1.0&quot;description: A Helm chart for Kubernetesname: saythxversion: 0.1.0You have new mail in /var/spool/mail/root</code></pre><p>这个文件是每个 <code>chart</code> 必不可少的一个文件，其中包含着几个重要的属性，如:</p><ul><li><code>apiVersion</code>: 目前版本都为<code>v1</code>。</li><li><code>appVersion</code>: 这是应用的版本号，注意需要与 <code>apiVersion</code>，<code>version</code>等字段区分。</li><li><code>name</code>: 通常要求 <code>chart</code> 的名字必须和它所在的目录保持一致，且此字段必须。</li><li><code>version</code>: 表明当前 <code>chart</code> 的版本号，会直接影响 <code>Release</code> 的记录，且此字段必须。</li><li><code>description</code>: 描述。</li></ul><h3 id="charts"><a href="#charts" class="headerlink" title="charts"></a>charts</h3><p><code>charts</code> 文件夹是用于存放依赖的 <code>chart</code> 的。当有依赖需要管理时，可添加 <code>requirements.yaml</code> 文件，可用于管理项目内或外部的依赖。</p><h3 id="helmignore"><a href="#helmignore" class="headerlink" title=".helmignore"></a>.helmignore</h3><p><code>.helmignore</code> 类似于 <code>.gitignore</code> 和 <code>.dockerignore</code> 之类的，用于忽略一些不想包含在 <code>chart</code> 内的文件。</p><h3 id="templates"><a href="#templates" class="headerlink" title="templates"></a>templates</h3><p><code>templates</code> 文件夹内存放着 <code>chart</code> 所使用的模板文件，也是 <code>chart</code> 的实际执行内容。在使用 <code>chart</code> 进行安装的时候，会将下面介绍的 <code>values.yaml</code> 中的配置项与 <code>templates</code> 中的模板进行组装，生产最终要执行的配置文件。</p><p><code>templates</code> 中，推荐命名应该清晰，如 <code>xx-deployment.yaml</code>，中间使用 <code>-</code> 进行分割，避免使用驼峰式命名。</p><p><code>NOTES.txt</code> 文件在 <code>helm install</code> 完成后，会进行回显，可用于解释说明如何访问服务等。</p><h3 id="values-yaml"><a href="#values-yaml" class="headerlink" title="values.yaml"></a>values.yaml</h3><p><code>values.yaml</code> 存放着项目的一些可配置项，如镜像的名称或者 tag 之类的。作用就是用于和模板进行组装。</p><h2 id="编写-chart"><a href="#编写-chart" class="headerlink" title="编写 chart"></a>编写 chart</h2><hr><p>了解完结构之后，我们来实际编写chart。所有完整代码可在<a href="https://github.com/aruruka/saythx" target="_blank" rel="noopener">SayThx项目</a>获取。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># Chart.yaml</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">appVersion</span><span class="token punctuation">:</span> <span class="token string">"1.0"</span><span class="token key atrule">description</span><span class="token punctuation">:</span> A Helm chart for SayThx.<span class="token key atrule">name</span><span class="token punctuation">:</span> saythx<span class="token key atrule">version</span><span class="token punctuation">:</span> 0.1.0<span class="token key atrule">maintainers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> Jintao Zhang<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可添加 <code>maintainers</code> 字段，表示维护者。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># values.yaml</span><span class="token comment" spellcheck="true"># backend is the values for backend</span><span class="token key atrule">backend</span><span class="token punctuation">:</span>  <span class="token key atrule">image</span><span class="token punctuation">:</span> taobeier/saythx<span class="token punctuation">-</span>be  <span class="token key atrule">tag</span><span class="token punctuation">:</span> <span class="token string">"1.0"</span>  <span class="token key atrule">pullPolicy</span><span class="token punctuation">:</span> IfNotPresent  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token comment" spellcheck="true"># namespace is the values for deploy namespace</span><span class="token key atrule">namespace</span><span class="token punctuation">:</span> work<span class="token comment" spellcheck="true"># service.type is the values for service type</span><span class="token key atrule">service</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>values.yaml</code> 文件中定义了我们预期哪些东西是可配置的，比如 <code>namespace</code> 以及镜像名称 tag 等。这里只是贴出了部分内容，仅做说明使用，完整内容可查看我们的<a href="https://github.com/aruruka/saythx/blob/master/saythx/values.yaml" target="_blank" rel="noopener">示例项目</a> 。</p><p>写 <code>values.yaml</code> 文件的时候，由于是使用 <code>YAML</code> 格式的配置，所以它非常的灵活，即可以使用如上面例子中的 <code>backend</code> 那种字典类型的， 也可以写成简单的 k-v 形式。但通常来讲，应该尽可能的将它写的清晰明确。并且容易被替换。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># templates/backend-service.yaml </span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> backend  <span class="token key atrule">name</span><span class="token punctuation">:</span> saythx<span class="token punctuation">-</span>backend  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">{</span> .Values.namespace <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> backend  <span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">{</span> .Values.service.type <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将部署文件模板化，与配置项进行组装。</p><pre class="line-numbers language-go"><code class="language-go"><span class="token number">1</span><span class="token punctuation">.</span> Get the application URL by running these commands<span class="token punctuation">:</span><span class="token punctuation">{</span><span class="token punctuation">{</span><span class="token operator">-</span> <span class="token keyword">if</span> contains <span class="token string">"NodePort"</span> <span class="token punctuation">.</span>Values<span class="token punctuation">.</span>service<span class="token punctuation">.</span><span class="token keyword">type</span> <span class="token punctuation">}</span><span class="token punctuation">}</span>  export NODE_PORT<span class="token operator">=</span>$<span class="token punctuation">(</span>kubectl get <span class="token operator">--</span>namespace <span class="token punctuation">{</span><span class="token punctuation">{</span> <span class="token punctuation">.</span>Values<span class="token punctuation">.</span>namespace <span class="token punctuation">}</span><span class="token punctuation">}</span> <span class="token operator">-</span>o jsonpath<span class="token operator">=</span><span class="token string">"{.spec.ports[0].nodePort}"</span> services saythx<span class="token operator">-</span>frontend<span class="token punctuation">)</span>  export NODE_IP<span class="token operator">=</span>$<span class="token punctuation">(</span>kubectl get nodes <span class="token operator">--</span>namespace <span class="token punctuation">{</span><span class="token punctuation">{</span> <span class="token punctuation">.</span>Values<span class="token punctuation">.</span>namespace <span class="token punctuation">}</span><span class="token punctuation">}</span> <span class="token operator">-</span>o jsonpath<span class="token operator">=</span><span class="token string">"{.items[0].status.addresses[0].address}"</span><span class="token punctuation">)</span>  echo http<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>$NODE_IP<span class="token punctuation">:</span>$NODE_PORT<span class="token punctuation">{</span><span class="token punctuation">{</span><span class="token operator">-</span> <span class="token keyword">else</span> <span class="token keyword">if</span> contains <span class="token string">"ClusterIP"</span> <span class="token punctuation">.</span>Values<span class="token punctuation">.</span>service<span class="token punctuation">.</span><span class="token keyword">type</span> <span class="token punctuation">}</span><span class="token punctuation">}</span>  export POD_NAME<span class="token operator">=</span>$<span class="token punctuation">(</span>kubectl get pods <span class="token operator">--</span>namespace <span class="token punctuation">{</span><span class="token punctuation">{</span> <span class="token punctuation">.</span>Values<span class="token punctuation">.</span>namespace <span class="token punctuation">}</span><span class="token punctuation">}</span> <span class="token operator">-</span>l <span class="token string">"app=frontend"</span> <span class="token operator">-</span>o jsonpath<span class="token operator">=</span><span class="token string">"{.items[0].metadata.name}"</span><span class="token punctuation">)</span>  echo <span class="token string">"Visit http://127.0.0.1:8080 to use your application"</span>  kubectl <span class="token operator">--</span>namespace <span class="token punctuation">{</span><span class="token punctuation">{</span> <span class="token punctuation">.</span>Values<span class="token punctuation">.</span>namespace <span class="token punctuation">}</span><span class="token punctuation">}</span> port<span class="token operator">-</span>forward $POD_NAME <span class="token number">8080</span><span class="token punctuation">:</span><span class="token number">80</span><span class="token punctuation">{</span><span class="token punctuation">{</span><span class="token operator">-</span> end <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面这是 <code>NOTES.txt</code> 文件内的内容。 这些内容会在 <code>helm install</code> 执行成功后显示在终端，用于说明服务如何访问或者其他注意事项等。<br>当然，这里的内容主要是为了说明如何编写 <code>chart</code> ，在实践中，尽量避免硬编码配置在里面。</p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><hr><h3 id="直接部署"><a href="#直接部署" class="headerlink" title="直接部署"></a>直接部署</h3><p>Helm 的 chart 可以直接在源码目录下通过 helm install 完成部署。例如:</p><pre class="line-numbers language-bash"><code class="language-bash">helm <span class="token function">install</span> saythx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><pre class="line-numbers language-bash"><code class="language-bash">helm package saythx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> helm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>事到如今已经不敢问的网络基础-互联网-基础-网络-TCP</title>
      <link href="2019/09/25/shi-dao-ru-jin-yi-jing-bu-gan-wen-de-wang-luo-ji-chu-hu-lian-wang-ji-chu-wang-luo-tcp/"/>
      <url>2019/09/25/shi-dao-ru-jin-yi-jing-bu-gan-wen-de-wang-luo-ji-chu-hu-lian-wang-ji-chu-wang-luo-tcp/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>计算机的东西总是不停地学不停的忘，于是把网络相关的基础·基本知识整理了以下。</p><p>如果有什么错误的话能留言指正以下的话我会很高兴的。</p><h2 id="什么是网络？"><a href="#什么是网络？" class="headerlink" title="什么是网络？"></a>什么是网络？</h2><blockquote><p>将复数的计算机连接起来，使它们可以相互通信的状态。<br><a href="https://kotobank.jp/word/%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF-7058" target="_blank" rel="noopener">https://kotobank.jp/word/%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF-7058</a></p></blockquote><p>把计算机与计算机连接，使它们处于可以交流信息的状态，就叫作网络或者计算机网络。</p><p>家庭内或者公司内之类的场景，把附近的计算机连接起来的网络叫做LAN(Local Area Network)。把远方的计算机连接起来的网络叫做WAN(Wide Area Network)。</p><p>另外连接手机的网络叫做mobile network。</p><h2 id="什么是互联网？"><a href="#什么是互联网？" class="headerlink" title="什么是互联网？"></a>什么是互联网？</h2><p>网络之中规模最大的就是互联网。<br>它可以把全世界的企业的网络，大学的网络，家庭的网络之类的都连接起来随意地交流信息。</p><h2 id="网络通信的构造"><a href="#网络通信的构造" class="headerlink" title="网络通信的构造"></a>网络通信的构造</h2><p>在计算机网络中将信息分割为叫做”包”的细小的单位来通信。<br>一个”包”包含了发信者和收信者的地址信息，基于这个信息来交换”包”。</p><p>之所以要分割成”包”来交换信息的理由是，通过将数据一点一点地传送这个方法，可以避免占用电路，这样可以与多个对象同时灵活地进行通信。</p><p>模拟电话以及手机的3G电路之类的并不是”包交换方式”，而是一种叫做”电路交换方式”的占用通信通道的方式。</p><h2 id="计算机网络的层级模型"><a href="#计算机网络的层级模型" class="headerlink" title="计算机网络的层级模型"></a>计算机网络的层级模型</h2><p>为了实现网络通信，有许多的程序和机器在共同运作。<br>为了使得它们之间的分工与角色明确，定义了”层级模型”。</p><p>层级模型中有”OSI引用模型”和”TCP/IP 4层级模型”。这里我们主要讲”TCP/IP 4层级模型”。</p><h2 id="什么是-TCP-IP-4层级模型？"><a href="#什么是-TCP-IP-4层级模型？" class="headerlink" title="什么是 TCP/IP 4层级模型？"></a>什么是 TCP/IP 4层级模型？</h2><table><thead><tr><th align="left">层级名</th><th align="left">功能</th><th align="left">关联key-word</th></tr></thead><tbody><tr><td align="left">app层</td><td align="left">提供Web服务，电子邮件之类的服务给用户</td><td align="left">HTTP, SMTP, POP3, FTP, SSH之类</td></tr><tr><td align="left">传输层</td><td align="left">融通app层和互联网层，使得数据能正确地送达</td><td align="left">TCP, UDP</td></tr><tr><td align="left">互联网层</td><td align="left">基于对象的IP address使得给计算机发送的数据能送达</td><td align="left">IP address, IPv4, IPv6, ICMP, routing</td></tr><tr><td align="left">Network Interface层</td><td align="left">控制network adaptor之类的通信用硬件，实际上传送数据。</td><td align="left">Ethernet, 无线LAN, MAC Address, PPP 之类</td></tr></tbody></table><p>以上的层级中，app层是提供服务的，其他3个都是担当通信职能。</p><h2 id="什么是protocol"><a href="#什么是protocol" class="headerlink" title="什么是protocol?"></a>什么是protocol?</h2><p>在搞清楚各个层级的作用之前，需要指导一个关键字，那就是”protocol”。</p><blockquote><p>计算机·网络中，计算机之间为了交换信息定下的痛惜规矩。<br><a href="https://kotobank.jp/word/%E3%83%97%E3%83%AD%E3%83%88%E3%82%B3%E3%83%AB-8535#E6.97.A5.E6.9C.AC.E5.A4.A7.E7.99.BE.E7.A7.91.E5.85.A8.E6.9B.B8.28.E3.83.8B.E3.83.83.E3.83.9D.E3.83.8B.E3.82.AB.29" target="_blank" rel="noopener">https://kotobank.jp/word/%E3%83%97%E3%83%AD%E3%83%88%E3%82%B3%E3%83%AB-8535#E6.97.A5.E6.9C.AC.E5.A4.A7.E7.99.BE.E7.A7.91.E5.85.A8.E6.9B.B8.28.E3.83.8B.E3.83.83.E3.83.9D.E3.83.8B.E3.82.AB.29</a></p></blockquote><p>人与人之间的交流根据状况也有大概的规则。</p><p>比如，商业场合，”交换名片” -&gt; “稍微闲聊” -&gt; “主题”之类的；<br>如果是和朋友在居酒屋的话，”点餐” -&gt; “干杯” -&gt; “聊聊近况”等等。</p><p>人类活动中有这些不成文的规矩，那计算机之间则需要严密地规矩。</p><p>而那些用来”交流”的规矩便是protocol(协议)。</p><p>网络之间的通信便有许多场景使用到各种protocol。</p><h2 id="代表性protocol"><a href="#代表性protocol" class="headerlink" title="代表性protocol"></a>代表性protocol</h2><table><thead><tr><th align="left">层级</th><th align="left">protocol名</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left">app层</td><td align="left">HTTP</td><td align="left">用来交流网页的数据</td></tr><tr><td align="left"></td><td align="left">HTTPS</td><td align="left">通过加密手段来交流网页的数据</td></tr><tr><td align="left"></td><td align="left">POP3</td><td align="left">把保管在服务器的邮件获取出来</td></tr><tr><td align="left"></td><td align="left">SMTP</td><td align="left">发送邮件</td></tr><tr><td align="left"></td><td align="left">FTP</td><td align="left">传送文件</td></tr><tr><td align="left"></td><td align="left">Telnet</td><td align="left">远程操作计算机</td></tr><tr><td align="left"></td><td align="left">SSH</td><td align="left">通过加密手段来远程操作计算机</td></tr><tr><td align="left"></td><td align="left">DHCP</td><td align="left">给计算机分配私有IP address</td></tr><tr><td align="left"></td><td align="left">DNS</td><td align="left">将URL与IP address相互转换</td></tr><tr><td align="left"></td><td align="left">SSL</td><td align="left">通过加密手段来通信</td></tr><tr><td align="left">传输层</td><td align="left">TCP</td><td align="left">优先正确性地收发app数据</td></tr><tr><td align="left"></td><td align="left">UDP</td><td align="left">优先速度地收发app数据</td></tr><tr><td align="left">互联网层</td><td align="left">IP</td><td align="left">将包传送到目的地</td></tr><tr><td align="left"></td><td align="left">ICMP</td><td align="left">通知基于IP的通信错误</td></tr><tr><td align="left"></td><td align="left">IPsec</td><td align="left">加密包的传输</td></tr><tr><td align="left"></td><td align="left">ARP</td><td align="left">查询网络机器的MAC address</td></tr><tr><td align="left">Network Interface层</td><td align="left">Ethernet</td><td align="left">通过金属电缆或光纤来传送数据</td></tr><tr><td align="left"></td><td align="left">PPP</td><td align="left">认证用户并允许与远端机器通信</td></tr></tbody></table><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>详细介绍各层级重要的协议，例如TCP、HTTP、SSL。</p>]]></content>
      
      
      <categories>
          
          <category> 网络基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TCP/IP </tag>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人用-常用命令-笔记</title>
      <link href="2019/09/20/ge-ren-yong-chang-yong-ming-ling-bi-ji/"/>
      <url>2019/09/20/ge-ren-yong-chang-yong-ming-ling-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>个人用 常用命令 笔记（重新整理）</p><p><a href="https://gist.github.com/aruruka/27a446db7a8ce0c2738f73c912418e42" target="_blank" rel="noopener">https://gist.github.com/aruruka/27a446db7a8ce0c2738f73c912418e42</a></p><script src="https://gist.github.com/aruruka/27a446db7a8ce0c2738f73c912418e42.js"></script>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> Docker </tag>
            
            <tag> Python </tag>
            
            <tag> Linux </tag>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7 Install Docker and Docker Compose</title>
      <link href="2019/09/17/centos7-install-docker-and-docker-compose/"/>
      <url>2019/09/17/centos7-install-docker-and-docker-compose/</url>
      
        <content type="html"><![CDATA[<h1 id="CentOS7-Install-Docker-and-docker-compose"><a href="#CentOS7-Install-Docker-and-docker-compose" class="headerlink" title="CentOS7_Install_Docker_and_docker-compose"></a>CentOS7_Install_Docker_and_docker-compose</h1><h1 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h1><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Install Docker CE</span><span class="token comment" spellcheck="true">## Set up the repository</span><span class="token comment" spellcheck="true">### Install required packages.</span>yum <span class="token function">install</span> yum-utils device-mapper-persistent-data lvm2<span class="token comment" spellcheck="true">### Add Docker repository.</span>yum-config-manager \  --add-repo \  https://download.docker.com/linux/centos/docker-ce.repoyum update <span class="token operator">&amp;&amp;</span> yum <span class="token function">install</span> docker-ce-18.06.2.ce<span class="token comment" spellcheck="true"># If you are going to use local rpm to install Docker, follow steps below:</span><span class="token comment" spellcheck="true"># First copy docker rpms from 10.20.0.198:/root/rpms/docker</span><span class="token comment" spellcheck="true"># mkdir -pv /root/rpms/docker &amp;&amp; \</span><span class="token comment" spellcheck="true"># scp -r root@10.20.0.198:/root/rpms/docker /root/rpms/docker &amp;&amp; \</span><span class="token comment" spellcheck="true"># cd /root/rpms/docker &amp;&amp; \</span><span class="token comment" spellcheck="true"># yum localinstall ./*.rpm</span><span class="token comment" spellcheck="true"># Add your user to the docker group with the following command.</span><span class="token function">usermod</span> -aG docker <span class="token variable"><span class="token variable">$(</span><span class="token function">whoami</span><span class="token variable">)</span></span><span class="token comment" spellcheck="true">## Create /etc/docker directory.</span><span class="token function">mkdir</span> -pv /etc/docker<span class="token comment" spellcheck="true"># Setup daemon.</span><span class="token function">cat</span> <span class="token operator">></span> /etc/docker/daemon.json <span class="token operator">&lt;&lt;</span><span class="token string">EOF{  "exec-opts": ["native.cgroupdriver=systemd"],  "log-driver": "json-file",  "log-opts": {    "max-size": "20m"  },  "storage-driver": "overlay2",  "storage-opts": [    "overlay2.override_kernel_check=true"  ]}EOF</span><span class="token function">mkdir</span> -p /etc/systemd/system/docker.service.d<span class="token comment" spellcheck="true"># Don't forget to set SELinux to permissive:</span><span class="token comment" spellcheck="true"># Set SELinux in permissive mode (effectively disabling it)</span>setenforce 0<span class="token function">sed</span> -i <span class="token string">'s/^SELINUX=enforcing$/SELINUX=permissive/'</span> /etc/selinux/config<span class="token comment" spellcheck="true"># Set Docker to start automatically at boot time:</span>systemctl <span class="token function">enable</span> docker.service<span class="token comment" spellcheck="true"># Restart Docker</span>systemctl daemon-reloadsystemctl restart docker<span class="token comment" spellcheck="true"># Check docker daemon info</span>docker info<span class="token comment" spellcheck="true"># Check if docker runs normally</span>docker run --rm --name<span class="token operator">=</span>hello-world hello-world<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Install-docker-compose"><a href="#Install-docker-compose" class="headerlink" title="Install docker-compose"></a>Install docker-compose</h1><ol><li><p>Run this command to download the current stable release of Docker Compose:</p><p><code>sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</code></p></li><li><p>Apply executable permissions to the binary:</p><p><code>sudo chmod +x /usr/local/bin/docker-compose</code></p><p><strong>Note:</strong> If the command docker-compose fails after installation, check your path.<br>You can also create a symbolic link to /usr/bin or any other directory in your path.</p><p>For example:</p><p><code>sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</code></p></li><li><p>Test the installation.</p><pre><code>$ docker-compose --versiondocker-compose version 1.24.1, build 1110ad01</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分析Dockbix的Dockerfile--通用的多程序镜像的设计思路</title>
      <link href="2019/09/16/fen-xi-dockbix-de-dockerfile-tong-yong-de-duo-cheng-xu-jing-xiang-de-she-ji-si-lu/"/>
      <url>2019/09/16/fen-xi-dockbix-de-dockerfile-tong-yong-de-duo-cheng-xu-jing-xiang-de-she-ji-si-lu/</url>
      
        <content type="html"><![CDATA[<p>一般来说一个软件的镜像的最后由<code>ENTRYPOINT</code>和<code>CMD</code>来构成，有的镜像可能没有<code>ENTRYPOINT</code>,<br><code>ENTRYPOINT</code>和<code>CMD</code>的主要区别在于即使你在运行镜像时指定了“命令”<br>(e.g. <code>docker run nginx echo &quot;test&quot;</code>的<code>echo &quot;test&quot;</code>部分就是“命令”)，<code>ENTRYPOINT</code>中<br>指定的命令(e.g. <code>ENTRYPOINT entrypoint.sh</code>)也不会被忽略。<br>总的来说，可以把以下几条规则当做创作Dockerfile的thumb rules：</p><ul><li><p><code>RUN</code>: 用来安装软件或添加依赖，因为<code>RUN</code>会创建一层<code>layer</code>。</p></li><li><p><code>ENTRYPOINT</code>: 用来指定该镜像为可执行程序，并且指定一个一个脚本用来初始化基础环境（<br>e.g. 创建镜像运行时所使用的用户、加载程序的依赖、初始化环境变量等。）。</p></li><li><p><code>CMD</code>: 用来运行软件。</p></li><li><p><code>ENTRYPOINT</code>与<code>CMD</code>结合使用：</p><p>在<code>ENTRYPOINT entrypoint.sh</code>中初始化环境，最后执行<code>exec &quot;$@&quot;</code>，以此来继续执行<br><code>CMD</code>中的内容。</p><p><strong>参考：</strong><a href="https://github.com/bitnami/bitnami-docker-nginx/blob/master/1.16/centos-7/Dockerfile" target="_blank" rel="noopener">bitnami/nginx’s Dockerfile</a></p></li></ul><h1 id="Dockbix的Dockerfile的结构"><a href="#Dockbix的Dockerfile的结构" class="headerlink" title="Dockbix的Dockerfile的结构"></a>Dockbix的Dockerfile的结构</h1><p>大致上来说，和我们构建一个物理Zabbix服务器的结构是一样的。<br>都是先装OS，再安装Zabbix软件，以及准备一系列我们需要用来达到“自省”的脚本和工具软件。<br>以下是dockbix的Dockerfile，这个镜像构建出来的可以算是一个“一体化”软件。包括了Zabbix Server、<br>Zabbix Web Interface、Nginx作为web服务器。</p><p><a href="https://github.com/monitoringartist/dockbix-xxl/blob/master/Dockerfile/dockbix-xxl-4.0/Dockerfile" target="_blank" rel="noopener">dockbix’s Dockerfile</a></p><p>用图片来表示这个Dockerfile做了什么：</p><p><img src="dockbix_dockerfile_structure.png" alt="Dockerfile"></p><h1 id="从Dockbix镜像学习到实用的工具"><a href="#从Dockbix镜像学习到实用的工具" class="headerlink" title="从Dockbix镜像学习到实用的工具"></a>从Dockbix镜像学习到实用的工具</h1><p>Dockbix镜像运行起来之后，其程序树是这样的：</p><pre><code>[root@8a11e51b8ddb /]# ps axjf  PPID    PID   PGID    SID TTY       TPGID STAT   UID   TIME COMMAND     0    615    615    615 pts/0      2343 Ss       0   0:00 /bin/bash   615   2343   2343    615 pts/0      2343 R+       0   0:00  \_ ps axjf     0      1      1      1 ?            -1 Ss       0   0:01 /usr/bin/python /usr/bin/supervisord -n -c /etc/supervisord.conf     1    379    379      1 ?            -1 S        0   0:00 nginx: master process /usr/sbin/nginx   379    389    379      1 ?            -1 S       80   0:00  \_ nginx: worker process   379    390    379      1 ?            -1 S       80   0:00  \_ nginx: worker process     1    380    380      1 ?            -1 S        0   0:01 php-fpm: master process (/etc/php-fpm.conf)     1    381    381      1 ?            -1 S        0   0:00 /bin/sh /config/ds.sh /tmp/zabbix_server.pid sudo -u zabbix /usr/local/sbin/zabbix_server --foreground -c /usr/local/etc/zab   381    383    381      1 ?            -1 S        0   0:00  \_ sudo -u zabbix /usr/local/sbin/zabbix_server --foreground -c /usr/local/etc/zabbix_server.conf   383    392    381      1 ?            -1 S     1000   0:00      \_ /usr/local/sbin/zabbix_server --foreground -c /usr/local/etc/zabbix_server.conf   392    399    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: configuration syncer [synced configuration in 0.016130 sec, idle 60 sec]   392    400    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: alerter #1 started   392    401    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: alerter #2 started   392    402    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: alerter #3 started   392    403    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: housekeeper [deleted 50373 hist/trends, 0 items/triggers, 85 events, 0 sessions,   392    404    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: timer #1 [updated 0 hosts, suppressed 0 events in 0.000859 sec, idle 59 sec]   392    405    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: http poller #1 [got 0 values in 0.000503 sec, idle 5 sec]   392    406    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: discoverer #1 [processed 0 rules in 0.000670 sec, idle 60 sec]   392    407    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: history syncer #1 [processed 0 values, 1 triggers in 0.000493 sec, idle 1 sec]   392    408    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: history syncer #2 [processed 0 values, 0 triggers in 0.000010 sec, idle 1 sec]   392    409    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: history syncer #3 [processed 1 values, 0 triggers in 0.001232 sec, idle 1 sec]   392    410    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: history syncer #4 [processed 0 values, 0 triggers in 0.000015 sec, idle 1 sec]   392    411    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: escalator #1 [processed 0 escalations in 0.000964 sec, idle 3 sec]   392    412    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: proxy poller #1 [exchanged data with 0 proxies in 0.000045 sec, idle 5 sec]   392    413    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: self-monitoring [processed data in 0.000026 sec, idle 1 sec]   392    414    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: task manager [processed 0 task(s) in 0.000324 sec, idle 5 sec]   392    415    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: poller #1 [got 0 values in 0.000013 sec, idle 1 sec]   392    416    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: poller #2 [got 0 values in 0.000007 sec, idle 1 sec]   392    417    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: poller #3 [got 0 values in 0.000007 sec, idle 1 sec]   392    418    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: poller #4 [got 0 values in 0.000009 sec, idle 1 sec]   392    419    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: poller #5 [got 1 values in 0.000538 sec, idle 1 sec]   392    420    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: unreachable poller #1 [got 0 values in 0.000095 sec, idle 1 sec]   392    421    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: trapper #1 [processed data in 0.000000 sec, waiting for connection]   392    422    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: trapper #2 [processed data in 0.000000 sec, waiting for connection]   392    423    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: trapper #3 [processed data in 0.000000 sec, waiting for connection]   392    424    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: trapper #4 [processed data in 0.001913 sec, waiting for connection]   392    425    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: trapper #5 [processed data in 0.011211 sec, waiting for connection]   392    426    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: icmp pinger #1 [got 0 values in 0.000022 sec, idle 5 sec]   392    427    381      1 ?            -1 S     1000   0:01          \_ /usr/local/sbin/zabbix_server: alert manager #1 [sent 0, failed 0 alerts, idle 5.010558 sec during 5.010662 sec]   392    428    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: preprocessing manager #1 [queued 0, processed 4 values, idle 5.002023 sec during   392    429    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: preprocessing worker #1 started   392    430    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: preprocessing worker #2 started   392    431    381      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_server: preprocessing worker #3 started     1    382    382      1 ?            -1 S        0   0:00 /usr/bin/bash -c while true; do sleep 3600; /usr/bin/bash /config/init/12-xxl-ping.sh; done   382   1761    382      1 ?            -1 S        0   0:00  \_ sleep 3600     1    384    384      1 ?            -1 S        0   0:00 bash -c while inotifywait -q -r -e create,delete,modify,move,attrib --exclude &quot;/\.&quot; /etc/nginx/ /data/conf/nginx/; do nginx   384    388    384      1 ?            -1 S        0   0:00  \_ inotifywait -q -r -e create,delete,modify,move,attrib --exclude /\. /etc/nginx/ /data/conf/nginx/     1    385    385      1 ?            -1 S        0   0:00 /bin/sh /config/ds.sh /tmp/zabbix_agentd.pid sudo -u zabbix /usr/local/sbin/zabbix_agentd --foreground -c /usr/local/etc/zab   385    387    385      1 ?            -1 S        0   0:00  \_ sudo -u zabbix /usr/local/sbin/zabbix_agentd --foreground -c /usr/local/etc/zabbix_agentd.conf   387    391    385      1 ?            -1 S     1000   0:00      \_ /usr/local/sbin/zabbix_agentd --foreground -c /usr/local/etc/zabbix_agentd.conf   391    393    385      1 ?            -1 S     1000   0:02          \_ /usr/local/sbin/zabbix_agentd: collector [idle 1 sec]   391    394    385      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_agentd: listener #1 [waiting for connection]   391    396    385      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_agentd: listener #2 [waiting for connection]   391    397    385      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_agentd: listener #3 [waiting for connection]   391    398    385      1 ?            -1 S     1000   0:00          \_ /usr/local/sbin/zabbix_agentd: active checks #1 [idle 1 sec]</code></pre><p></p><h2 id="利用supervisor和inotify-util来做程序的自省和动态重载配置文件"><a href="#利用supervisor和inotify-util来做程序的自省和动态重载配置文件" class="headerlink" title="利用supervisor和inotify-util来做程序的自省和动态重载配置文件"></a>利用supervisor和inotify-util来做程序的自省和动态重载配置文件</h2><p><a href="http://supervisord.org/" target="_blank" rel="noopener">Supervisor</a>是用Python开发的一个client/server服务，是Linux/Unix系统下的一个进程管理工具，不支持Windows系统。它可以很方便的监听、启动、停止、重启一个或多个进程。用Supervisor管理的进程，当一个进程意外被杀死，supervisor监听到进程死后，会自动将它重新拉起，很方便的做到进程自动恢复的功能，不再需要自己写shell脚本来控制。</p><p>Dockbix中也实用supervisor来做Nginx和php-fpm的自动重启以及配置文件动态更新。</p><p>Dockbix镜像最终启动过程是大概如下：</p><ol><li><p>Dockbix的入口为<code>config/bootstrap.sh</code>，由<code>Dockerfile</code>最后的<code>CMD</code>定义。</p></li><li><p>执行<code>config/bootstrap.sh</code>过程中调用<code>/config/init/bootstrap.sh</code>，由传入的环境变量（环境变量在<code>docker run --env=...</code>来指定），来决定过滤掉哪些配置文件，也就是是否启动某些软件（功能）。</p><p>比如如果不启动Nginx(i.e. Zabbix Web Interface)，就传入<code>ZW_enabled=false</code>。</p><p><a href="https://github.com/monitoringartist/dockbix-xxl/blob/e7ddb4d72ce94d7db742704637df2f18fe237119/Dockerfile/dockbix-xxl-4.0/container-files-zabbix/config/init/bootstrap.sh#L191" target="_blank" rel="noopener">Dockbix Web Interface’s enabling logic</a></p></li><li><p>过滤不需要要启动的软件后启动，也就是删除掉不需要启动的软件的supervisor配置文件。</p><p>supervisor的配置文件内容：</p><pre><code>[include]files = /etc/supervisor.d/*.conf</code></pre><p>supervisor.d/nginx.conf的内容：</p><pre><code>[program:nginx]command = /usr/sbin/nginxautorestart = truestderr_logfile = NONEstdout_logfile = NONE# Watch for changes in Nginx conf directories and restart Nginx when a config change occured[program:nginx-reload]command=bash -c &#39;while inotifywait -q -r -e create,delete,modify,move,attrib --exclude &quot;/\.&quot; /etc/nginx/ /data/conf/nginx/; do nginx -t &amp;&amp; nginx -s reload; done&#39;</code></pre><p>这里可以看到，<code>[program:nginx-reload]</code>中定义了检查nginx的配置文件状态是否发生改变，如果发生改变，就调用<code>nginx -s reload</code>。</p></li><li><p>最后继续执行<code>config/bootstrap.sh</code>，调用supervisor，来启动这个镜像需要执行的程序。</p></li></ol><h1 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h1><p>虽然使用supervisor可以解决Docker容器同时运行多个业务程序，就算业务程序挂了也无感知的问题，<br>能不能完美地做到单个容器中监控多个程序是否运行，应该也要看supervisor的配置文件中命令是否合理。</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Supervisor </tag>
            
            <tag> Inotify </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Docker部署Zabbix监控系统来监控主机状态</title>
      <link href="2019/09/11/shi-yong-docker-bu-shu-zabbix-jian-kong-xi-tong-lai-jian-kong-zhu-ji-zhuang-tai/"/>
      <url>2019/09/11/shi-yong-docker-bu-shu-zabbix-jian-kong-xi-tong-lai-jian-kong-zhu-ji-zhuang-tai/</url>
      
        <content type="html"><![CDATA[<h1 id="使用-Docker-部署-Zabbix-监控系统来监控主机状态"><a href="#使用-Docker-部署-Zabbix-监控系统来监控主机状态" class="headerlink" title="使用 Docker 部署 Zabbix 监控系统来监控主机状态"></a>使用 Docker 部署 Zabbix 监控系统来监控主机状态</h1><p>内容摘要：</p><ul><li><p>使用 <a href="https://github.com/monitoringartist/zabbix-docker-monitoring" target="_blank" rel="noopener">monitoring/zabbix-docker-monitoring</a> 的镜像来部署 Zabbix，Zabbix 的 <code>server</code>、<code>proxy</code>、<code>agent</code> 包括 <code>server</code> 的 <code>Mariadb</code>，全部都是容器化部署。感兴趣的可以阅读 GitHub 原文以了解更多。</p><p>使用这套工具主要目的是使用其提供的模块来监控宿主机的 Docker 的状态。</p><p>使用 Zabbix 监控 Docker 容器。 可用的CPU，内存，blkio，净容器指标和一些容器配置详细信息，例如 IP，名称，… Zabbix Docker模块本身支持 Docker 容器（包含Systemd），并且还应该支持一些其他容器类型（例如LXC）。 请随时测试并提供反馈/未解决的问题。 该模块侧重于性能，请参阅<a href="https://github.com/monitoringartist/zabbix-docker-monitoring#module-vs-userparameter-script" target="_blank" rel="noopener">模块与用户参数脚本</a>一节。</p></li><li><p>Dockbix（Docker化Zabbix-server）</p></li><li><p>Zabbix agent（使用Docker部署Zabbix-agent）</p></li></ul><h1 id="部署-Dockbix"><a href="#部署-Dockbix" class="headerlink" title="部署 Dockbix"></a>部署 Dockbix</h1><p>Dockbix XXL是一款预配置的容器化 Zabbix，可轻松实现Docker监控。 此Docker镜像包含标准Zabbix +其他XXL社区插件。 包括常规任务：自动导入Zabbix DB，自动导入Docker监控模板，Dockbix agent XXL的自动注册规则，…</p><h2 id="下载-Dockbix-镜像"><a href="#下载-Dockbix-镜像" class="headerlink" title="下载 Dockbix 镜像"></a>下载 Dockbix 镜像</h2><p>Dockbix的镜像包括：</p><ul><li><p>monitoringartist/zabbix-db-mariadb</p><p>给 <code>Zabbix</code> 用的 <code>Mariadb</code>。</p></li><li><p>monitoringartist/dockbix-xxl</p><p>Dockbix 本体。</p></li></ul><p>下载这2个镜像：</p><pre><code>docker pull monitoringartist/zabbix-db-mariadbdocker pull monitoringartist/dockbix-xxl</code></pre><p>另外，还需要下载 <code>busybox</code> 这个<strong>工具箱镜像</strong>创作一个容器：<br><code>docker pull busybox</code></p><p>我们需要使用这个工具箱镜像单独创建一个容器命名为 <strong>“dockbix-db-storage”</strong>，作为 Dockbix 的数据库的 <code>data-container</code>。</p><p>使用卷(volume)的好处有很多，包括：</p><ul><li>易于备份或迁移</li><li>可以使用 <code>Docker CLI</code> 或 <code>DockerAPI</code> 管理卷</li><li>可以在多个容器之间更安全的共享卷</li><li>可以使用远程主机上的或云供应商的存储卷，可以加密卷内容或添加其他功能</li><li>可以通过容器预先添加内容到卷里</li></ul><h2 id="部署-Dockbix-XXL"><a href="#部署-Dockbix-XXL" class="headerlink" title="部署 Dockbix XXL"></a>部署 Dockbix XXL</h2><p>运行以下命令或者使用docker-compose命令：<code>docker-compose up -d</code>，<strong><em>配置文件</em></strong>如下：</p><script src="https://gist.github.com/aruruka/66a59267afb738964d8d324726dcaef0.js"></script><p>以创建Dockbix所需的容器。</p><p><strong>注意：</strong></p><p>请根据自身情况修改具体内容。<br>直接使用 <code>docker</code> 命令则如下：</p><pre><code># Create data container with persistent storage in the /var/lib/mysql folderdocker run -d -v /var/lib/mysql --name dockbix-db-storage busybox:latest# Start DB for Dockbix - default 1GB innodb_buffer_pool_size is used# You might define your own password by changing &#39;my_password&#39; to anything you want in &quot;MARIADB_PASS=my_password&quot; below.docker run \    -d \    --name dockbix-db \    -v /backups:/backups \    -v /etc/localtime:/etc/localtime:ro \    --volumes-from dockbix-db-storage \    --env=&quot;MARIADB_USER=zabbix&quot; \    --env=&quot;MARIADB_PASS=my_password&quot; \    monitoringartist/zabbix-db-mariadb# Start Dockbix linked to the started DBdocker run \    -d \    --name dockbix \    -p 80:80 \    -p 10051:10051 \    -v /etc/localtime:/etc/localtime:ro \    --link dockbix-db:dockbix.db \    --env=&quot;ZS_DBHost=dockbix.db&quot; \    --env=&quot;ZS_DBUser=zabbix&quot; \    --env=&quot;ZS_DBPassword=my_password&quot; \    --env=&quot;XXL_zapix=true&quot; \    --env=&quot;XXL_grapher=true&quot; \    gself/customized-dockbix:v1.0# Wait ~30 seconds for Zabbix initialization# Zabbix web will be available on the port 80, Zabbix server on the port 10051# Default credentials: Admin/zabbix</code></pre><p>输入<a href="http://YOUR_IP/" target="_blank" rel="noopener">http://YOUR_IP/</a>，就可以看到Zabbix的登陆界面，初始密码如上述的 <code>Admin/zabbix</code>。<br>如下图：</p><p><img src="Dockbix_first_login.png" alt="First time login to Dockbix"></p><h2 id="Dockbix-日常管理工作"><a href="#Dockbix-日常管理工作" class="headerlink" title="Dockbix 日常管理工作"></a>Dockbix 日常管理工作</h2><p>作为运维人员，免不了要做一些日常备份等管理工作，以下提供一些命令以供参考。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">## Backup of DB Zabbix - configuration data only, no item history/trends</span>docker <span class="token function">exec</span> \    -ti dockbix-db \    /zabbix-backup/zabbix-mariadb-dump -u zabbix -p my_password -o /backups<span class="token comment" spellcheck="true">## Full compressed backup of Zabbix DB</span>docker <span class="token function">exec</span> \    -ti dockbix-db \    <span class="token function">bash</span> -c <span class="token string">"\    mysqldump -u zabbix -pmy_password zabbix | \    bzip2 -cq9 > /backups/zabbix_db_dump_<span class="token variable"><span class="token variable">$(</span><span class="token function">date</span> +%Y-%m-%d-%H.%M.%S<span class="token variable">)</span></span>.sql.bz2"</span><span class="token comment" spellcheck="true">## DB data restore</span><span class="token comment" spellcheck="true"># Remove Dockbix container</span>docker <span class="token function">rm</span> -f dockbix<span class="token comment" spellcheck="true"># Restore DB data from the dump (all your current data will be dropped!!!)</span>docker <span class="token function">exec</span> -i dockbix-db sh -c <span class="token string">'bunzip2 -dc /backups/zabbix_db_dump_2017-28-09-02.57.46.sql.bz2 | mysql -uzabbix -p --password=my_password zabbix'</span><span class="token comment" spellcheck="true"># Run Dockbix container again</span>docker run <span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Dockbix-一些额外启动项"><a href="#Dockbix-一些额外启动项" class="headerlink" title="Dockbix 一些额外启动项"></a>Dockbix 一些额外启动项</h2><p>Dockbix 已经预设了 <code>Java gateway</code>、<code>SNMP</code> 支持，如果想开启，只需再启动时添加参数，以及如果想启用 <code>SSL</code> 也可以覆盖原本的启动配置来设置 <code>Nginx</code> 证书等。</p><h1 id="Dockbix-额外功能"><a href="#Dockbix-额外功能" class="headerlink" title="Dockbix 额外功能"></a>Dockbix 额外功能</h1><h2 id="Dockbix-可选功能以及自定义配置的支持"><a href="#Dockbix-可选功能以及自定义配置的支持" class="headerlink" title="Dockbix 可选功能以及自定义配置的支持"></a>Dockbix 可选功能以及自定义配置的支持</h2><p>另外还有一些可选功能可以通过启动时传递的环境变量来控制。</p><p>可选的和特定功能有关的环境变量：</p><table><thead><tr><th>Variable</th><th>Default value</th><th>Description</th></tr></thead><tbody><tr><td>XXL_searcher</td><td>true</td><td>enable/disable integrated <a href="https://github.com/monitoringartist/zabbix-searcher" target="_blank" rel="noopener">Zabbix searcher project</a></td></tr><tr><td>XXL_zapix</td><td>false</td><td>enable/disable integrated <a href="https://github.com/monitoringartist/zapix" target="_blank" rel="noopener">Zapix project</a></td></tr><tr><td>XXL_grapher</td><td>false</td><td>enable/disable integrated <a href="https://github.com/sepich/zabbixGrapher" target="_blank" rel="noopener">Grapher project</a></td></tr><tr><td>XXL_api</td><td>true</td><td>enable/disable auto import of templates (<code>.xml</code>), API curl commands (<code>.curl</code>) or API scripts (<code>.sh</code>) located in path <code>/etc/zabbix/api/&lt;custom_folder&gt;</code></td></tr><tr><td>XXL_apiuser</td><td>Admin</td><td>username used for API commands</td></tr><tr><td>XXL_apipass</td><td>zabbix</td><td>password used for API commands</td></tr><tr><td>XXL_analytics</td><td>true</td><td>enable/disable collecting of statistics via Google Analytics</td></tr><tr><td>XXL_updatechecker</td><td>true</td><td>enable/disable check of the latest Docker image - checks are executed in the user browser once per day</td></tr></tbody></table><p>还有更多可配置的选项可以通过环境变量来设置，<strong>详情可以参考作者的<a href="https://github.com/monitoringartist/dockbix-xxl" target="_blank" rel="noopener">GitHub页面</a></strong>。<br>包括 <code>Zabbix Web UI</code>、<code>Zabbix proxy</code> 等的配置项，这些配置项也可以通过环境变量来配置到 Dockbix 中。<br>另外还可以使用卷来挂载配置文件、Zabbix的template、自定义脚本和SQL文件等。</p><h1 id="部署-Zabbix-agent"><a href="#部署-Zabbix-agent" class="headerlink" title="部署 Zabbix-agent"></a>部署 Zabbix-agent</h1><p>Dockbix的作者<a href="https://github.com/monitoringartist" target="_blank" rel="noopener">monitorartist</a>当然也发布了定制化过的Zabbix-agent镜像 – <a href="https://github.com/monitoringartist/dockbix-agent-xxl" target="_blank" rel="noopener">Dockbix Agent XXL</a>，然而免费版不支持添加自定义的<code>Userparameter脚本</code><sup id="a1"><a href="#f1">1</a></sup>。<br>以及不支持使用 <code>docker exec</code> 进入 <code>Dockbix-agent</code> 的容器实例等。<br>可以通过作者提供的<a href="https://github.com/monitoringartist/dockbix-agent-xxl#public-test-instance-of-dockbix-agent-xxl" target="_blank" rel="noopener">Public test instance of Dockbix Agent XXL</a>来体验Dockbix-agent的功能。</p><p>这些现实都是通过 <code>Zabbix</code> 原理或者 <code>Docker</code> 镜像的一些原理来实现的，比如阉割掉<code>Userparameter脚本</code><sup id="a1"><a href="#f1">1</a></sup>这个功能，就是在镜像中去掉Zabbix-agnet的配置文件的相关配置项；限制 <code>docker exec</code> 进入 <code>Dockbix-agent</code> 的容器则是通过在容器中删除掉所有基础OS命令(sh, ls, pwd, cd …)来实现的。<br>当然，了解 Docker 的文件系统的人，还是可以找到 <code>Dockbix-agent</code> 镜像的内容，我使用 Zabbix 来监控的目的主要是做<strong>主机层面</strong>的监控，也就是包括：</p><ol><li>OS基础服务的状态</li><li>OS性能（CPU、内存、磁盘等使用率）</li><li>关键用户服务（比如<code>Docker</code>、<code>kubelet</code>）的状态</li><li>关键用户服务的性能</li><li>使用 Zabbix 来自动发现局域网主机、云服务商商部署的服务（AWS的EC2、S3、RDS、ElasticCache；阿里云的ECS等）</li></ol><p>而 <code>Dockbix</code> 和一个“干净”的 <code>Zabbix</code> 相比，主要的区别在于作者加入了<strong>可以发现容器化应用的<code>模块</code><sup id="a2"><a href="#f2">2</a></sup>和<code>模板</code><sup id="a3"><a href="#f3">3</a></sup></strong>。<br>这些自定义的功能使得我们可以在Zabbix上看到主机的 Docker 实例的状态（各种资源占用率等）。<br>我找了一台机器部署了 <code>Dockbix-agent</code>，并且找到了其内部使用的自定义模块，如下图：<br><img src="Dockbix-agent_module.png" alt="Dockbix-agent module"></p><p>由于模块属于原作者著作权内容，此处就补贴出共享连接了，大家要在自己的环境下用于商业目的，就各自承担风险了。<br>另外，对于 <a href="../../docker_basis/docker_filesystem.md">Docker 文件系统</a>，我以前也总结过一点，有兴趣的可以看看，都是初级的知识……</p><p>我试着用官方的 <code>Zabbix-agent</code> 镜像导入 <code>Dockbix-agent</code> 的<code>模块</code><sup id="a2"><a href="#f2">2</a></sup>，也成功了，只是在 <code>Zabbix-web</code> 上还看不到那些自定义模块自动发现出来的 <code>item</code>，估计是还需要修改其他的配置。<br>由于我并不是很在乎是否能在 Zabbix 中监控到 Docker 实例，所以这块先搁置不继续深究了。</p><hr><h2 id="下载-Zabbix-agnet-镜像"><a href="#下载-Zabbix-agnet-镜像" class="headerlink" title="下载 Zabbix-agnet 镜像"></a>下载 Zabbix-agnet 镜像</h2><p>参考<a href="https://hub.docker.com/r/zabbix/zabbix-agent" target="_blank" rel="noopener">官方链接</a></p><h2 id="部署-Zabbix-agent-容器"><a href="#部署-Zabbix-agent-容器" class="headerlink" title="部署 Zabbix-agent 容器"></a>部署 Zabbix-agent 容器</h2><p><strong>注意：</strong><br>我这里使用的环境变量、挂载卷都是参考了 <code>Zabbix-agent</code> <a href="https://hub.docker.com/r/zabbix/zabbix-agent" target="_blank" rel="noopener">官方链接</a> 和 <a href="https://github.com/monitoringartist/dockbix-agent-xxl" target="_blank" rel="noopener">Dockbix Agent XXL</a> 的文档来配置的，若要了解用途，最好就是实际部署一下 <code>Dockbix-agent</code>，再对比干净的 <code>Zabbix-agent</code> 的目录结构和内容。</p><ul><li><p>在需要监控的主机上安装 Zabbix-agent：</p><pre><code>docker run \  --name zabbix-agent-3.4 \  --net host \  --privileged \  -v /:/rootfs \  -v /var/run:/var/run \  -v /etc/zabbix/zabbix_agentd.d:/etc/zabbix/zabbix_agentd.d:ro \  -v /var/lib/zabbix/modules:/var/lib/zabbix/modules:ro \  -v /var/lib/zabbix/enc:/var/lib/zabbix/enc:ro \  --restart unless-stopped \  -e ZBX_HOSTNAME=&quot;`uname -n`&quot; \  -e ZBX_SERVER_HOST=&quot;192.168.79.254&quot; \  -e ZBX_LOADMODULE=&quot;zabbix_module_docker.so,zabbix_module_sockets.so,zabbix_module_stress.so,zabbix_module_systemd.so&quot; \  -d zabbix/zabbix-agent:centos-3.4-latest</code></pre></li></ul><p>这里贴一下使用 <code>Zabbix-agent</code> 成功导入 <code>Dockbix-agnent</code> 的模板的确认信息。<br><img src="zabbix-agent_load_dockbix-module.png" alt="Zabbix-agent loaded docker module"></p><p>以下2个主机，master运行的是Dockbix-agent，sysnode运行的是Zabbix-agent（原生），Zabbix-agent 导入了 Dockbix 中的模块，但是没有自动发现出其主机上所运行的 Docker 容器，可能是还有哪里需要配置吧……<br><img src="Dockbix-agent_and_Zabbix-agent.png" alt="Dockbix-agent and Zabbix-agent"></p><hr><h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><p><b id="f1">See how to use “UserParameter Script” within Zabbix.</b><br><a href="https://techexpert.tips/zabbix/zabbix-userparameter-script-on-linux/" target="_blank" rel="noopener">https://techexpert.tips/zabbix/zabbix-userparameter-script-on-linux/</a><br><a href="#a1">↩</a></p><p><b id="f2">See how to Use “Custom Module” within Zabbix.</b><br><a href="https://github.com/monitoringartist/zabbix-docker-monitoring#installation" target="_blank" rel="noopener">https://github.com/monitoringartist/zabbix-docker-monitoring#installation</a><br><a href="#a2">↩</a></p><p><b id="f3">See how to think of “Template” in Zabbix.</b><br><a href="https://hackernoon.com/understanding-zabbix-f2a83eeb1221" target="_blank" rel="noopener">https://hackernoon.com/understanding-zabbix-f2a83eeb1221</a><br>模板：可以视为可应用于各种主机组的监控模板。 根据经验，我总是将模板应用于主机组。 如果想要删除主机或仅将模板应用于一个主机，这是我会提醒自己，我可以修改主机组或添加符合当前条件的新主机组。 这为我节省了很多时间。 我在其他标准之前提到模板，因为我认为从模板和组的角度思考一切是非常重要的。<br><a href="#a3">↩</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开发Zabbix的自定义监控项和自动发现AWS服务实例</title>
      <link href="2019/09/11/zabbix-zi-dong-fa-xian-ji-yu-aws-python-api-boto-3-de-kai-fa-zhi-nan/"/>
      <url>2019/09/11/zabbix-zi-dong-fa-xian-ji-yu-aws-python-api-boto-3-de-kai-fa-zhi-nan/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前介绍过，在<strong>Zabbix</strong>中添加监控的一些<a href="https://shouneng.website/2019/09/09/li-jie-zabbix/#toc-heading-6">经验法则</a>。也就是总是围绕模板(Templates)、主机组(Host groups)来思考，接下来的内容记录了我如何添加一个实际的自定义监控项。<br>这篇开发指南项使用开源项目的<a href="https://github.com/wawastein/zabbix-cloudwatch" target="_blank" rel="noopener">Python脚本</a>，目的是为了在Zabbix中自动发现AWS中的服务实例。</p><h1 id="逻辑关系图"><a href="#逻辑关系图" class="headerlink" title="逻辑关系图"></a>逻辑关系图</h1><p>先上个逻辑图：<br><img src="./doc/zabbix-cloudwatch_logical-diagram.png" alt="Zabbix discovery of AWS cloudwatch logical diagram."></p><p>这个逻辑图并不是完整的逻辑关系，比如Zabbix Frontend不是直接去调用Zabbix Agent，而是Zabbix Server去调用。<br>而Zabbix Server获取Agent的数据又分为主动和被动2种类型，对应着Server方不同的子进程。<br>这个逻辑图展示的是在用户操作需要设计的组件之间大致的一个关系。</p><h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><ul><li><p>自动发现(LLD)：<br>LLD – Low Level Discovery<br>Zabbix中的“发现”(Discovery)主要为了实现2大类目的：</p><ol><li>发现被监控的对象以及自动注册。这种功能可以让我们免去每次新增监控对象就要手动配置的烦恼。</li><li>发现不同实体并自动创建监控项、触发器和图形等。以上图为例，自动发现的实体就是通过Python脚本调用AWS API获取到的服务(RDS, ElastiCache等)的实例,并根据规则创建出来的这些实例的监控项、触发器等。</li></ol></li><li><p>userparameter:<br>Zabbix中实现自定义监控程序的常用方法有2种：</p><ol><li><p>开发自定义的可加载模块（.so文件），这个跟Linux中的模块文件是一个道理，通常用C语言来开发，当然，用这种方式来扩展Zabbix的功能可以获得最佳性能，然而我不会C语言……</p></li><li><p>调用OS的fork()创建子进程，这里面常用的方式就是userparameter。其他的external check、调用Agent的system.run[]监控项不实用<sup id="a1"><a href="#f1">脚注1</a></sup>。不予考虑。</p></li><li><p>目前我还没有验证过，但我猜测可以通过以下方式实现LLD的负载均衡：</p><ul><li>创建一个load balance，比如在2个Linux主机上通过Nginx+keepalived实现。LB的ip为192.168.1.100。</li><li>在Zabbix上配置Host，ip为LB的ip – 192.168.1.100，取名为LLD LB host。</li><li>将这个LLD LB Host与LLD的template相关联。已达到负载均衡的目的。</li></ul><p>显然，如果要这么做的话，只有采用userparameter来配置自定义监控脚本时最方便开发，维护的。</p></li></ol></li><li><p>LLD宏：<br>Zabbix的宏功能对于开发自定义监控项来说必不可少，非常方便。比如，用自定义脚本，按照Zabbix规定的json格式返回回来，就可以在别监控项或者触发器中使用！LLD宏就是自动发现后注册的变量，和其他“Agent内置宏”、“用户自定义宏”等区分。详情<a href="https://www.zabbix.com/documentation/3.4/zh/manual/config/macros/lld_macros" target="_blank" rel="noopener">官方文档</a>。</p></li></ul><h1 id="userparameter自定义脚本的说明"><a href="#userparameter自定义脚本的说明" class="headerlink" title="userparameter自定义脚本的说明"></a>userparameter自定义脚本的说明</h1><p>使用userparameter，只需要在Zabbix Agent的配置文件中定义userparameter指令即可。<br>一般在使用上，会在主配置文件中使用include指令来指定额外加载的配置文件。</p><h2 id="配置文件命名风格"><a href="#配置文件命名风格" class="headerlink" title="配置文件命名风格"></a>配置文件命名风格</h2><p>以下是我的配置文件命名风格：</p><ol><li>住配置文件中只包含基本配置，额外的参数都放在文件中，命名为：<code>extra_zabbix-agent_parameters.conf</code>。</li><li>userparameter以项目为单位，例如这次开发的是整合AWS相关的功能，命名为：<code>userparameter_aws-discovery-cloudwatch.conf</code>。</li></ol><h2 id="userparameter相关配置"><a href="#userparameter相关配置" class="headerlink" title="userparameter相关配置"></a>userparameter相关配置</h2><ol><li><p>extra_zabbix-agent_parameters.conf<br>由于Zabbix Agent调用Python脚本访问AWS API，返回数据有可能会超过默认的timeout配置的时常（5秒），所以我修改了Timeout属性：<br><code>Timeout=30</code>。</p></li><li><p>userparameter_aws-discovery-cloudwatch.conf</p><pre><code>UserParameter=aws.discovery[*],bash /etc/zabbix/zabbix_agentd.d/aws_discovery/aws_discovery.sh $1 $2 $3 $4 $5 $6 $7 $8 $9UserParameter=cloudwatch.metric[*],bash /etc/zabbix/zabbix_agentd.d/aws_discovery/cloudwatch_metric.sh  $1 $2 $3 $4 $5 $6 $7 $8 $9</code></pre><p>说明：<br>UserParameter=<item_key_name>[<parameter>],<command>[parameter]<br>具体定义语法细节请参考<a href="https://www.zabbix.com/documentation/3.4/zh/manual/config/items/userparameters" target="_blank" rel="noopener">官方文档</a>。</parameter></item_key_name></p><p>脚本简单逻辑说明：</p><ol><li>userparameter.conf文件中定义了自定义监控项的程序入口，这里使用的时shell脚本作为入口，只要时OS能执行的都行。shell脚本的参数在Zabbix Frontend（web控制台）上定义。</li><li>shell脚本调用Python脚本，在Python脚本中组织参数，调用相应的服务相关的模块，再获取AWS服务实例的数据，组织成Zabbix规定的json格式，并打印。</li><li>Zabbix Agent接受到打印的json格式字符串，作为这个userparameter监控项的返回值。</li></ol></li></ol><h2 id="实际操作截图"><a href="#实际操作截图" class="headerlink" title="实际操作截图"></a>实际操作截图</h2><ol><li>创建监控模板<br><img src="./doc/create_template-01.png" alt="create template"><br>在开发一个新监控功能时，我们都要先考虑将它放在什么模板中。<br>这里，我创建了一个名为”Template AWS-CloudWatch”的模板。<br><img src="./doc/create_template-02.png" alt="template should belongs to a group and follow naming standard"></li><li>创建自动发现规则<br><img src="./doc/create_template-03.png" alt="create LLD rule"><br>自动发现规则中指定的监控项的key，就是刚才通过userparameter定义的key。<br><img src="./doc/create_template-04.png" alt="LLD rule"><br>这里使用到了用户自定义宏：<code>{$REGION}</code>, <code>{$ACCOUNT}</code>。用户自定义宏一般可以在模板中定义，或是主机中定义。</li><li>通过zabbix_get命令行工具测试userparameter是否正常工作<br>在正式配置下一步自动发现的“实际”项目之前，我们需要先确认刚才配置的userparameter是否能正常工作。<br>这是可以使用zabbix_get命令行工具。<br><img src="./doc/create_template-05.png" alt="use zabbix_get to debug"></li><li>依次创建所需的监控项原型、触发器原型等<br><img src="./doc/create_template-06.png" alt="create item prototype etc."></li></ol><h1 id="使用AWS-API-boto-3时的一些心得"><a href="#使用AWS-API-boto-3时的一些心得" class="headerlink" title="使用AWS API boto 3时的一些心得"></a>使用AWS API boto 3时的一些心得</h1><ul><li>直接查看boto 3的<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" target="_blank" rel="noopener">参考手册</a>，看看对于某个服务（例如EC2、CloudWatch）有哪些增删改查的操作。</li><li>在AWS web控制台上查看某个服务有哪些指标：<br><img src="./doc/cloudwatch_metric.png" alt="check what kind of metrics are available"></li><li>如果不知道该针对什么指标做监控，可以直接搜索”what metrics should I monitor”。AWS的文档真的很全！<br><img src="./doc/which-metrics-should-i-monitor.png" alt="what metrics should I monitor"></li></ul><h1 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h1><ul><li>调用AWS API使用的IAM用户的权限需要控制适当。</li><li>userparameter指向的脚本文件等的用户权限是否与Zabbix Agent进程的用户权限相符。</li><li>待补充……</li></ul><p>项目代码和Zabbix Template内容，已经放在代码库中。</p><h1 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h1><p><b id="f1">脚注1</b></p><ol><li>external check 是在Zabbix Server上执行的，虽然不需要Agent,但是我们不希望额外消耗Servere的性能。另外，这不利于实现“负载均衡”和“高可用”。</li><li>Agent的监控项system.run[]就更加不用说了，谁会把复杂的逻辑写在这里面呢？如果是调用一个脚本，直接用userparameter更好。<br><a href="#a1">↩</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> Zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解 Zabbix</title>
      <link href="2019/09/09/li-jie-zabbix/"/>
      <url>2019/09/09/li-jie-zabbix/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://hackernoon.com/understanding-zabbix-f2a83eeb1221" target="_blank" rel="noopener">https://hackernoon.com/understanding-zabbix-f2a83eeb1221</a></p><p>监控所有服务器基础设施。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在为我们的产品设置基础设施时<a href="http://bit.ly/use-highlights" target="_blank" rel="noopener">http://bit.ly/use-highlights</a>我正在寻找用于服务器基础设施监控的开源工具，而不是藏着一颗炸弹（译者按：不监控大概就是藏着一颗炸弹吧），并且可以随着我们改进（服务架构）帮助我们自定义化。 两个主要的竞争者是Nagios和Zabbix。 我读了几篇比较两者的帖子，但是<a href="https://www.comparitech.com/net-admin/nagios-vs-zabbix/?lphiltid=5bfd07b68e39bb080950ca0b" target="_blank" rel="noopener">https://www.comparitech.com/net-admin/nagios-vs-zabbix/?lphiltid=5bfd07b68e39bb080950ca0b</a>是我最喜欢的文章，因为我觉得它非常客观和详细。 我对Nagios的一个主要优势是你可以使用现有的设置升级到Nagios ** Nagios XI **。 但是对于我们的场景，我们觉得Zabbix绰绰有余。 所以就使用Zabbix吧。</p><p>这不是详细的”HowTo”博客。 但这将是我的学习总结，链接到适用于我的相关安装指南，链接到详细的”TODO”文章和我的经验法则。</p><h1 id="安装前的一些基础知识"><a href="#安装前的一些基础知识" class="headerlink" title="安装前的一些基础知识"></a>安装前的一些基础知识</h1><p>Zabbix有三个主要组件。 Zabbix Server，保存…（译者按：意思为保存数据） Zabbix Agent和Zabbix Web Interface。 Zabbix Server是从您要监视的服务器收集所有相关数据的服务器。 要监视的服务器称为“Agent”。 您可以在没有Zabbix Web部件的情况下监控服务器。 但我建议使用它，因为它可以让体验变得更好。</p><p>如果您只想在安装任何内容之前了解用户界面，请访问<a href="https://zabbix.org/zabbix/index.php" target="_blank" rel="noopener">https://zabbix.org/zabbix/index.php</a>并“以访客身份登录”。 您将没有“配置”和“管理”选项卡，但可以查看“监视”，“清单”和“报告”部分。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>我将在此处跳过详细信息，因为您可以从互联网上找到“HOWTO”文章。 我们使用的是Ubuntu服务器，<a href="https://twitter.com/tecadmin" target="_blank" rel="noopener">https://twitter.com/tecadmin</a>的以下文章非常方便。</p><ol><li>安装Zabbix Server – <a href="https://tecadmin.net/install-zabbix-on-ubuntu/" target="_blank" rel="noopener">https://tecadmin.net/install-zabbix-on-ubuntu/</a></li><li>安装Zabbix Agent – <a href="https://tecadmin.net/install-zabbix-agent-on-ubuntu-and-debian" target="_blank" rel="noopener">https://tecadmin.net/install-zabbix-agent-on-ubuntu-and-debian</a></li><li>将主机(host)添加到 Zabbix Server中以便监控 – <a href="https://tecadmin.net/add-host-zabbix-server-monitor" target="_blank" rel="noopener">https://tecadmin.net/add-host-zabbix-server-monitor</a></li></ol><p>默认情况下，Zabbix Web Interface使用apache。 如果您不想在其位置使用Nginx，请在您的nginx中添加以下配置（译者按：配置内容为译者补充）。</p><pre><code>server {  listen      80 default;  root        /data/www/default;  index       index.php index.html;  include     /etc/nginx/conf.d/default-*.conf;  include     /data/conf/nginx/conf.d/default-*.conf;  # PHP backend is not in the default-*.conf file set,  # as some vhost might not want to include it.  include     /etc/nginx/conf.d/php-location.conf;  # Import configuration files for status pages for Nginx and PHP-FPM  include /etc/nginx/conf.d/stub-status.conf;  include /etc/nginx/conf.d/fpm-status.conf;}</code></pre><p>可参考<a href="https://github.com/monitoringartist/dockbix-xxl/blob/e7ddb4d72ce94d7db742704637df2f18fe237119/Dockerfile/dockbix-xxl-4.0/container-files-base/etc/nginx/hosts.d/default.conf#L2" target="_blank" rel="noopener">Dockbix的Nginx配置</a>译者按：译者补充，<a href="[https://github.com/monitoringartist/dockbix-xxl](https://github.com/monitoringartist/dockbix-xxl)">monitoringartist/dockbix-xxl</a>包含了具有启迪意义的代码，代码涉及到Dockerfile的分层思路、supervisor、inotify-util和脚本配合，从容器环境变量可选择性地启动容器的功能等，在代码结构上也层次分明，具有通用性。）</p><h1 id="经验法则"><a href="#经验法则" class="headerlink" title="经验法则"></a>经验法则</h1><p>从主机组，用户组和模板这些方面，考虑您要进行监控的所有操作。</p><h1 id="快速词汇表"><a href="#快速词汇表" class="headerlink" title="快速词汇表"></a>快速词汇表</h1><ul><li><p>Zabbix Server</p><p>这是监控所有内容的主服务器。 如需安装，请点击<a href="https://tecadmin.net/install-zabbix-on-ubuntu/" target="_blank" rel="noopener">此链接</a>。<br>通常，Web Interface也仅安装在此服务器上。 我个人觉得（不太确定）最好将它与需要监控的其他服务器分开。<br>这样，即使实际的服务器群体停机，我们也能够至少监控停机时间以及它们可能已经停机的原因。</p></li><li><p>Zabbix Agent</p><p>这是一个软件，可以帮助将数据从被监控的服务器发送到Zabbix Server。 如果您还要监视Zabbix服务器，也可以在该服务器中安装Zabbix Agent。</p></li><li><p>Zabbix Host</p><p>Host是您需要监控的服务器。 因此，如果您需要监视三个不同的服务器，则在所有这些服务器上安装Zabbix Agent。</p></li><li><p>在Zabbix Server中添加Host</p><p>在各自的服务器中安装Zabbix Server和Agent后，还需要在Zabbix Server中添加主机。</p></li><li><p>Host Groups</p><p>主机组可以作为分类或标记。 我觉得在主机组的使用上想得自由一点比较有益。<br>以下是我创建的一些主机组#LearningPaths，＃staging，＃live，＃database，＃mongodb，＃appserver，＃search。<br>使用模板或“创建”操作时，这些主机组将非常方便。</p></li><li><p>Templates</p><p>将模板视为可应用于各种主机组的监控模板。 根据经验，我总是将模板应用于主机组。<br>如果想要删除主机或仅将模板应用于一个主机，这是我的指导标准，我可以重新设置主机组或添加符合当前条件的新主机组。<br>这为我节省了很多时间。 我在其他标准之前提到模板，因为我认为从模板和组的角度思考一切是非常重要的。</p></li><li><p>Template OS Linux</p><p>默认情况下安装Zabbix完就存在。 这提供了其他监视服务中通常可用的大多数参数，如CPU使用率，CPU负载，内存使用量等。<br>这已经配置了数据指标，对于服务器级别监控来说已经足够了。 我们更改了平均CPU负载的触发器，因为它引发了太多错误警报。<br>我们目前正在配置以下两个（模板）。 如果你有幸安装这两个，请发表评论。 我能够获得所有相关数据，但我无法将其推送到Zabbix服务器（指Zabbix官网·社区）。<br>我认为它与trap设置有关。 我还在搞清楚。</p><ul><li>Zabbix MongoDB Template — <a href="https://github.com/omni-lchen/zabbix-mongodb" target="_blank" rel="noopener">https://github.com/omni-lchen/zabbix-mongodb</a></li><li>Zabbix Elastic Search Templates — <a href="https://github.com/zarplata/zabbix-agent-extension-elasticsearch" target="_blank" rel="noopener">https://github.com/zarplata/zabbix-agent-extension-elasticsearch</a></li></ul></li></ul><h1 id="定制化"><a href="#定制化" class="headerlink" title="定制化"></a>定制化</h1><p>正如我之前提到的，我发现总是在Templates和Host Groups方面进行思考是有用的。<br>因此，如果您计划创建任何新项目/触发器，请确保将它们添加到相关模板中，然后关联到主机组。</p><p>首先为要跟踪的每个数据点创建一个项目。 您可以按照这些步骤<br><a href="https://www.zabbix.com/documentation/3.4/manual/config/items/item" target="_blank" rel="noopener">https://www.zabbix.com/documentation/3.4/manual/config/items/item</a><br>中的步骤操作。</p><p>然后，您可以根据这些项目值创建触发器。 您可以按照<br><a href="https://www.zabbix.com/documentation/3.4/manual/config/triggers/trigger" target="_blank" rel="noopener">https://www.zabbix.com/documentation/3.4/manual/config/triggers/trigger</a><br>中的步骤创建触发器。</p><h1 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h1><p>虽然Web Interface挺好的，但是我觉得对于调试来说，用控制台会更好。</p><h2 id="在监控者服务器上"><a href="#在监控者服务器上" class="headerlink" title="在监控者服务器上"></a>在监控者服务器上</h2><ul><li><p>检查您可以通过10050端口连接上Agent。</p><p><code>telnet &lt;ip-of-your-agent&gt; 10050</code></p></li><li><p>使用zabbix-get</p><pre><code>[root@8a11e51b8ddb nginx]# zabbix_getusage:  zabbix_get -s host-name-or-IP [-p port-number] [-I IP-address] -k item-key  zabbix_get -s host-name-or-IP [-p port-number] [-I IP-address]                --tls-connect cert --tls-ca-file CA-file                [--tls-crl-file CRL-file] [--tls-agent-cert-issuer cert-issuer]                [--tls-agent-cert-subject cert-subject]                --tls-cert-file cert-file --tls-key-file key-file -k item-key  zabbix_get -s host-name-or-IP [-p port-number] [-I IP-address]                --tls-connect psk --tls-psk-identity PSK-identity                --tls-psk-file PSK-file -k item-key  zabbix_get -h  zabbix_get -V[root@8a11e51b8ddb nginx]# zabbix_get -s 192.168.79.128 -k agent.ping1[root@8a11e51b8ddb nginx]# zabbix_get -s 192.168.79.128 -k agent.hostnamek8s-master-01.local[root@8a11e51b8ddb nginx]#</code></pre></li></ul><h2 id="在需要被监控的服务器-客户端上"><a href="#在需要被监控的服务器-客户端上" class="headerlink" title="在需要被监控的服务器/客户端上"></a>在需要被监控的服务器/客户端上</h2><p>在基本配置（Passive Agent）中，监控者服务器将询问数据。 因此，如果您的服务器可以与代理进行通信，那就足够了。<br>但如果使用Active Agent模式，那么您需要保证您的Agent可以连接您的监控者服务器并且可以推送数据。（编者按：监控者服务器指Zabbix Server所在的服务器）</p><ul><li><p>检查您可以连接到服务器端的10050端口。</p><p><code>telnet &lt;ip-of-your-server&gt; 10050</code></p></li><li><p>检查是否有安装zabbix-sender，没有的话安装它。</p><p><code>sudo apt-get install zabbix-sender</code></p></li><li><p>当Zabbix sender安装好后您可以运行以下命令</p><p><code>zabbix_sender -vv -z [serverIp] -p 10051 -s [clientName] -k traptest -o &quot;Test value&quot;</code></p></li></ul><p>在Active Agent配置中请注意，推送到服务器的所有数据都应该是Item类型trapper。<br>因此，在上面的示例中，您应该在服务器上使用密钥traptest创建了一个trapper类型的项目。<br>还要确保在服务器上创建项目时，您发送的数据是指定的类型。</p><h1 id="报告"><a href="#报告" class="headerlink" title="报告"></a>报告</h1><p>仪表板是可自定义的。 所以你可以改变它以列出的所有相关问题。 我个人喜欢的另一个功能是屏幕。 我们添加了服务器的所有心跳图，我们可以像这样跟踪它们的数据。</p><p><img src="Zabbix_screens.png" alt="Zabbix screens"></p><h1 id="通知"><a href="#通知" class="headerlink" title="通知"></a>通知</h1><p>我认为Zabbix中的通知很棒，因为它是高度可配置的。<br>我们使用结合sendgrid使用Zabbix。Zabbix UI中的默认通知对我们来说效果不佳。<br>所以我们使用了脚本路由。我们使用了库<a href="https://github.com/mkgin/sendgrid_zabbix_alert" target="_blank" rel="noopener">https://github.com/mkgin/sendgrid_zabbix_alert</a>。<br>Zabbix通知的调试可能有点刺激。 我刚刚创建了<a href="https://medium.com/@gokulnk/draft-zabbix-notifications-set-up-and-debugging-1d53b6d68371" target="_blank" rel="noopener">一个草稿</a>，当我有时间的时候会更新。</p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[译]Kubernetes集群灾备(disaster recovery)的终极指南</title>
      <link href="2019/08/31/yi-kubernetes-ji-qun-zai-bei-disaster-recovery-de-zhong-ji-zhi-nan/"/>
      <url>2019/08/31/yi-kubernetes-ji-qun-zai-bei-disaster-recovery-de-zhong-ji-zhi-nan/</url>
      
        <content type="html"><![CDATA[<p><a href="https://medium.com/velotio-perspectives/the-ultimate-guide-to-disaster-recovery-for-your-kubernetes-clusters-94143fcc8c1e" target="_blank" rel="noopener">原文 - The Ultimate Guide to Disaster Recovery for Your Kubernetes Clusters</a></p><p><img src="backup.jpg" alt="backup"></p><h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>Kubernetes允许我们运行大规模容器化app而不需要过多关注app的负载均衡细节。你可以通过在Kubernetes上运行多个app副本(replicas)(pods)来保证你的app的高可用性。所有容器编排的复杂细节安全地隐藏着，所以你可以专注于开发app而不是专注在如何部署它。你可以在这里了解更多关于Kubernetes集群高可用以及如何<a href="https://velotio.com/blog/2018/6/15/kubernetes-high-availability-kubeadm" target="_blank" rel="noopener">通过Kubeadm实现Kubernetes高可用(use Kubedm for high availability in Kubernetes)</a>。</p><p>但是使用Kubernetes有它本身的一些挑战以及要让Kubernetes运行起来需要花一些功夫。如果你不熟悉如何运行Kubernetes，你或许可以看看<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/" target="_blank" rel="noopener">这个</a>。</p><p>Kubernetes可以让我们实现零停机的部署，但是必然还是会有可能在任何时候发生服务中断的事件。你的网络可能down掉，你的最新的app镜像可能引起一个严重的bug，或者在最罕见的案例下，你可能面临一个自然灾害。</p><p>当你在使用Kubernetes的时候，或早或晚，你需要设置一个备份。为了以防你的集群进入到一个不可回复的状态，你需要一个备份来回复到集群早前的稳定状态。</p><h1 id="为什么备份和恢复"><a href="#为什么备份和恢复" class="headerlink" title="为什么备份和恢复?"></a>为什么备份和恢复?</h1><p>关于你为什么需要为你的Kubernetes集群准备备份和回复机制，有3个理由:</p><ol><li><strong>为了在灾难后恢复:</strong> 比如某人意外地将你的deployment所处的namespace删除了的情况。</li><li><strong>为了复制环境:</strong> 你想要复制你的生产环境到一个临时环境，以便在一个重大升级前做一些测试。</li><li><strong>迁移Kubernetes集群:</strong> 比方说，你想要迁移的你的Kubernetes集群到另一个环境。</li></ol><h1 id="需要备份什么"><a href="#需要备份什么" class="headerlink" title="需要备份什么?"></a>需要备份什么?</h1><p>现在你知道为什么，让我们看看具体备份要做什么。你需要备份2个东西:</p><ol><li>你的Kubernetes control plane(通常是master节点)是把数据存放在etcd存储中，于是你需要备份etcd所有陈述以便获取所有Kubernetes资源。</li><li>如果你有”有状态”(stateful)容器(通常在实际使用中是会遇到的)，你还需要备份持久卷(persistent volume)。</li></ol><h1 id="如何备份"><a href="#如何备份" class="headerlink" title="如何备份?"></a>如何备份?</h1><p>有各种工具比如Heptio ark和Kube-backup支持搭建于cloud provider上的Kubernetes集群的备份和回复。但是如果你没有在使用已经被管理(指云供应商提供的)的Kubernetes集群呢？你可能会需要亲自上阵，如果你的Kubernetes运行在裸机上，就像我们一样。<br>我们运行拥有3个master的Kubernetes集群，并且有3个etcd member同时运行在每个master上。如果我们失去了一个master，我们还可以恢复master，由于依然满足etcd的最低运行数。现在如果在生产环境下我们失去2个master，我们就需要一个机制去恢复集群运作。</p><p><img src="kubernetes_backup_machanism.jpg" alt="Kubernetes_backup_machanism"></p><p>想知道如何构建多个master的Kubernetes集群？继续阅读吧！</p><h1 id="给etcd做备份"><a href="#给etcd做备份" class="headerlink" title="给etcd做备份:"></a>给etcd做备份:</h1><p>给etcd备份机制的不同之处取决于你是如何在Kubernetes环境中构建etcd集群的。<br>在Kubernetes环境中有2种方法来设置etcd集群:</p><ol><li>内部etcd集群: 这表示你正在Kubernetes集群中运行容器/pod形式的etcd集群，并且管理这些pod的责任在于Kubernetes。</li><li>外部etcd集群: 大多数情况下Etcd集群以Linux service的形式运行在Kubernetes集群外，并且提供endpoint给Kubernetes集群以便Kubernetes集群来读写。</li></ol><h2 id="给内部Etcd集群做备份的策略"><a href="#给内部Etcd集群做备份的策略" class="headerlink" title="给内部Etcd集群做备份的策略:"></a>给内部Etcd集群做备份的策略:</h2><p>为了给一个内部etcd pod取备份，我们需要使用Kubernetes CronJob机能，这个方法不需要在宿主机(node)上安装任何etcdctl客户端。<br>以下是定义了Kubernetes CronJob，用来每分钟获取etcd备份:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1beta1<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob<span class="token key atrule">metadata</span><span class="token punctuation">:</span><span class="token key atrule">name</span><span class="token punctuation">:</span> backup<span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">spec</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># activeDeadlineSeconds: 100</span><span class="token key atrule">schedule</span><span class="token punctuation">:</span> <span class="token string">"*/1 * * * *"</span>  <span class="token key atrule">jobTemplate</span><span class="token punctuation">:</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">template</span><span class="token punctuation">:</span>        <span class="token key atrule">spec</span><span class="token punctuation">:</span>          <span class="token key atrule">containers</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> backup            <span class="token comment" spellcheck="true"># Same image as in /etc/kubernetes/manifests/etcd.yaml</span>            <span class="token key atrule">image</span><span class="token punctuation">:</span> k8s.gcr.io/etcd<span class="token punctuation">:</span>3.2.24            <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCDCTL_API              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"3"</span>            <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/bin/sh"</span><span class="token punctuation">]</span>            <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-$(date +%Y-%m-%d_%H:%M:%S_%Z).db"</span><span class="token punctuation">]</span>            <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/kubernetes/pki/etcd              <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs              <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /backup              <span class="token key atrule">name</span><span class="token punctuation">:</span> backup          <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure          <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>          <span class="token key atrule">volumes</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs            <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/kubernetes/pki/etcd              <span class="token key atrule">type</span><span class="token punctuation">:</span> DirectoryOrCreate          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> backup            <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /data/backup              <span class="token key atrule">type</span><span class="token punctuation">:</span> DirectoryOrCreate<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="给外部Etcd集群做备份的策略"><a href="#给外部Etcd集群做备份的策略" class="headerlink" title="给外部Etcd集群做备份的策略:"></a>给外部Etcd集群做备份的策略:</h2><p>如果你在Linux主机上作为一个service来运行etcd集群，你应该设置一个Linux的定时任务(cron job)来备份你的集群。<br>运行以下的命令来备份etcd。</p><pre><code>ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save /path/for/backup/snapshot.db</code></pre><h2 id="灾难恢复-disaster-recovery"><a href="#灾难恢复-disaster-recovery" class="headerlink" title="灾难恢复(disaster recovery)"></a>灾难恢复(disaster recovery)</h2><p>现在，假设Kubernetes集群完全downn掉之后，我们需要通过etcd snapshot恢复Kubernetes集群。<br>一般来说，我们需要启动etcd集群，然后在master节点兼etcd endpoint的这个(这些)主机上执行kubeadm init。<br>保证你把备份了的证书放到/etc/kubernetes/pki(kubeadm init创建集群时创建的存放Kubernetes集群用到的证书的默认目录)目录下。</p><h2 id="内部etcd集群恢复策略"><a href="#内部etcd集群恢复策略" class="headerlink" title="内部etcd集群恢复策略:"></a>内部etcd集群恢复策略:</h2><pre><code>docker run --rm \-v &#39;/data/backup:/backup&#39; \-v &#39;/var/lib/etcd:/var/lib/etcd&#39; \--env ETCDCTL_API=3 \&#39;k8s.gcr.io/etcd:3.2.24&#39; \/bin/sh -c &quot;etcdctl snapshot restore &#39;/backup/etcd-snapshot-2018-12-09_11:12:05_UTC.db&#39; ; mv /default.etcd/member/ /var/lib/etcd/&quot;kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd</code></pre><h2 id="外部etcd集群的恢复策略"><a href="#外部etcd集群的恢复策略" class="headerlink" title="外部etcd集群的恢复策略"></a>外部etcd集群的恢复策略</h2><p>通过以下命令恢复3个etcd节点:</p><pre><code>ETCDCTL_API=3 etcdctl snapshot restore snapshot-188.db \--name master-0 \--initial-cluster master-0=http://10.0.1.188:2380,master-01=http://10.0.1.136:2380,master-2=http://10.0.1.155:2380 \--initial-cluster-token my-etcd-token \--initial-advertise-peer-urls http://10.0.1.188:2380ETCDCTL_API=3 etcdctl snapshot restore snapshot-136.db \--name master-1 \--initial-cluster master-0=http://10.0.1.188:2380,master-1=http://10.0.1.136:2380,master-2=http://10.0.1.155:2380 \--initial-cluster-token my-etcd-token \--initial-advertise-peer-urls http://10.0.1.136:2380ETCDCTL_API=3 etcdctl snapshot restore snapshot-155.db \--name master-2 \--initial-cluster master-0=http://10.0.1.188:2380,master-1=http://10.0.1.136:2380,master-2=http://10.0.1.155:2380 \--initial-cluster-token my-etcd-token \--initial-advertise-peer-urls http://10.0.1.155:2380The above three commands will give you three restored folders on three nodes named master:0.etcd, master-1.etcd and master-2.etcd</code></pre><p>现在，停止所有节点上的etcd服务，用恢复了的文件夹替换为所有节点上etcd的文件夹，再启动etcd服务。现在你可以看到所有节点，但是有可能只能看见master是ready状态，你需要重新使用现存的ca.crt(你应该做一个备份)文件将另外2个节点join进集群。<br>在master上运行以下命令:</p><pre><code>kubeadm token create --print-join-command</code></pre><p>这会给你kubeadm join命令，添加一个参数<code>--ignore-preflight-errors</code>在另外2个节点上运行此命令，让它们变为ready状态。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>其中一个处理master故障的方法是创建多master的Kubernetes集群，但是即便这样也不能让你完全消除Kubernetes etcd备份和恢复的需求，而且你也有可能会意外地销毁HA环境下的数据。</p>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python脚本中动态加载模块(module)</title>
      <link href="2019/08/29/python-jiao-ben-zhong-dong-tai-jia-zai-mo-kuai-module/"/>
      <url>2019/08/29/python-jiao-ben-zhong-dong-tai-jia-zai-mo-kuai-module/</url>
      
        <content type="html"><![CDATA[<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://github.com/wawastein/zabbix-cloudwatch/blob/master/zabbix-scripts/scripts/aws_discovery.py" target="_blank" rel="noopener">wawastein/zabbix-cloudwatch</a></p><h1 id="内容简介"><a href="#内容简介" class="headerlink" title="内容简介"></a>内容简介</h1><p>在Github上看到一个在Zabbix Userparameter中调用AWS的API的<a href="https://github.com/wawastein/zabbix-cloudwatch/blob/master/zabbix-scripts/scripts/aws_discovery.py" target="_blank" rel="noopener">Python脚本</a>，里面用到了<code>importlib</code>这个模块，觉得挺实用的，做下笔记。</p><p><strong>使用场景：</strong></p><p>在编写Python脚本的时候，我们可能会想设计成一个入口(gateway)脚本文件接收一个服务(service)参数，然后<br>在gateway脚本中根据service参数去动态加载一个Python模块文件(module)，从而达到“解耦”的目的。</p><h1 id="代码范本"><a href="#代码范本" class="headerlink" title="代码范本"></a>代码范本</h1><p>先看脚本目录结构。</p><p><img src="dir_structure.png" alt="file dir structure"></p><p>aws_discovery.py为主脚本，<code>./aws_discovery.py --service &#39;rds&#39;</code>将会动态加载<code>./discovery/rds.py</code>模块。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/env python</span><span class="token comment" spellcheck="true"># coding=utf-8</span><span class="token keyword">import</span> argparse<span class="token keyword">import</span> importlib<span class="token comment" spellcheck="true"># argparse模块用来处理传给脚本的参数。</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>    description<span class="token operator">=</span><span class="token string">"Description of this script."</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># "--service" 为此脚本的标志(flag)；dest会成为一个属性名记录在parser对象中。</span><span class="token comment" spellcheck="true"># 假设此脚本为"gateway.py"，那么运行脚本时使用以下方式传参：</span><span class="token comment" spellcheck="true"># python gateway.py --service &lt;service_name></span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--service"</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">"service"</span><span class="token punctuation">,</span>                        help<span class="token operator">=</span><span class="token string">"Service to discover instances in"</span><span class="token punctuation">,</span>                        required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 从parser获取参数集合，args对象实际为Namespace类实例，而Namespace就是一个简单的对象，</span><span class="token comment" spellcheck="true"># 里面就是一堆在add_argument方法中定义的属性。</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># import_module方法第一个参数为将导入的模块的文件名，第二个参数为模块文件的目录名。</span>discovery_module <span class="token operator">=</span> importlib<span class="token punctuation">.</span>import_module<span class="token punctuation">(</span><span class="token string">".{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>args<span class="token punctuation">.</span>service<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"discovery"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过克隆创建k8s节点基础环境</title>
      <link href="2019/08/23/tong-guo-ke-long-chuang-jian-k8s-jie-dian-ji-chu-huan-jing/"/>
      <url>2019/08/23/tong-guo-ke-long-chuang-jian-k8s-jie-dian-ji-chu-huan-jing/</url>
      
        <content type="html"><![CDATA[<h1 id="通过克隆创建k8s节点基础环境"><a href="#通过克隆创建k8s节点基础环境" class="headerlink" title="通过克隆创建k8s节点基础环境"></a>通过克隆创建k8s节点基础环境</h1><h2 id="克隆虚拟机步骤"><a href="#克隆虚拟机步骤" class="headerlink" title="克隆虚拟机步骤"></a>克隆虚拟机步骤</h2><ol><li><p>安装完虚拟机后，安装yum的epel源，执行yum update。</p></li><li><p>然后创建虚拟机快照，名为initial centos7</p></li><li><p>通过vmware从快照克隆</p></li><li><p>克隆后在开机前选择新虚拟机 –&gt; 设置 –&gt; 网络适配器 –&gt; 高级 –&gt; MAC地址(生成)</p></li><li><p>给新虚拟机分配固定IP地址</p><pre><code># vmware dhcp guestOS 固定IP在宿主机C:\ProgramData\VMware\vmnetdhcp.conf中追加下列内容并保存：host k8s-sysnode-01.local {    # MAC地址    hardware ethernet 00:50:56:39:8F:D3;    fixed-address 192.168.79.131;}</code></pre></li><li><p>重启服务VMware DHCP Service</p></li></ol><h2 id="登入新虚拟机修改基础配置"><a href="#登入新虚拟机修改基础配置" class="headerlink" title="登入新虚拟机修改基础配置"></a>登入新虚拟机修改基础配置</h2><ol><li><p>修改hostname</p><pre class="line-numbers language-bash"><code class="language-bash">hostnamectl set-hostname k8s-sysnode-01.localvim /etc/hosts<span class="token comment" spellcheck="true"># 添加本机的host alias</span><span class="token comment" spellcheck="true"># 192.168.79.131  k8s-sysnode-01.local</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>修改UUID</p><pre class="line-numbers language-bash"><code class="language-bash">uuidgen ens33vim /etc/sysconfig/network-scripts/ifcfg-ens33<span class="token comment" spellcheck="true"># 更改UUID</span><span class="token comment" spellcheck="true"># UUID=0755d500-6a50-48c1-94cf-600bf9a48e88</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> kubeadm </tag>
            
            <tag> vmware </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在Kubernetes中正确地安装Calico</title>
      <link href="2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/"/>
      <url>2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/</url>
      
        <content type="html"><![CDATA[<p>下述内容以 <code>Calico</code> 当前(2019-6-11)最新版本 <code>v3.7</code> 为基础。<br>k8s集群的安装方式以kubeadm工具安装为前提。</p><p><strong>阅读本文前请先阅读以下文章：<a href="../shi-yong-kubeadm-an-zhuang-k8s-ji-qun">使用Kubeadm安装k8s集群</a></strong></p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul><li><p><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#本文目的">本文目的</a></p></li><li><p><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#系统要求">系统要求</a></p><ul><li><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#开通防火墙">开通防火墙</a></li><li><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#修改NetworkManager服务的配置以允许Calico管理网卡">修改NetworkManager服务的配置以允许Calico管理网卡</a></li></ul></li><li><p><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#运行Calico的方式">运行Calico的方式</a></p><ul><li><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#安装calico作为策略policy和网络networking推荐用法">安装Calico作为策略(policy)和网络(networking)(推荐用法)</a><ul><li><a href="/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/#使用Kubernetes-api作为数据存储--少于50个节点的情况">使用Kubernetes API作为数据存储–少于50个节点的情况</a></li></ul></li></ul></li></ul><h1 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h1><ul><li>解读 <a href="https://docs.projectCalico.org/v3.7/getting-started/kubernetes/requirements" target="_blank" rel="noopener">Calico 官网</a>上关于作为 Kubernetes cni 安装 Calico 的部分。</li></ul><h1 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h1><p>在安装Calico之前，有必要先了解安装前提的系统要求，包括: <strong>节点硬件</strong>、<strong>存储架构</strong>、<strong>网络基础设施</strong>、<strong>OS特权</strong>、<strong>Kubernetes版本</strong>、<strong>Kubernetes参数</strong>、<strong>内核依赖</strong>。</p><p>详细的说明请参见<a href="https://docs.projectCalico.org/v3.7/getting-started/kubernetes/requirements" target="_blank" rel="noopener">官网</a>。</p><h2 id="开通防火墙"><a href="#开通防火墙" class="headerlink" title="开通防火墙"></a>开通防火墙</h2><p>我把ens33这个网卡加入到了firewalld的public zone中，这个网卡是nat网卡，打算直接使用nat网卡来安装k8s集群。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@k8s-master-01 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># firewall-cmd --get-active-zones</span>public  interfaces: ens33<span class="token punctuation">[</span>root@k8s-master-01 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><dl><dt>ens33</dt><dd>这个是通过vmware workstation创建的NAT网卡。</dd></dl><p><img src="vmware_nat.png" alt="nat_iface"></p><p><a href="#目录">返回目录</a></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Calico networking (BGP)</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>179/tcp --permanent<span class="token comment" spellcheck="true"># Calico networking with Typha enabled</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>5473/tcp --permanent<span class="token comment" spellcheck="true"># flannel networking (VXLAN)</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>4789/udp --permanent<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="修改NetworkManager服务的配置以允许Calico管理网卡"><a href="#修改NetworkManager服务的配置以允许Calico管理网卡" class="headerlink" title="修改NetworkManager服务的配置以允许Calico管理网卡"></a>修改NetworkManager服务的配置以允许Calico管理网卡</h2><p>Calico 必须能够在宿主机上管理 <code>cali*</code> 网卡。当 <code>IPIP</code> 被启用时(默认)，Calico 还需要能够管理 <code>tunl*</code> 网卡。</p><p><strong>注意</strong>: 许多Linux发行版都安装了 <code>NetworkManager</code>。默认情况下 <code>NetworkManager</code> 不允许 Calico 管理网卡。如果你的节点有 <code>NetworkManager</code>，在安装 Calico 之前，你需要按照这个步骤去<a href="https://docs.projectcalico.org/v3.7/maintenance/troubleshooting#configure-networkmanager" target="_blank" rel="noopener">防止 NetworkManager 控制 Calico 网卡</a>。</p><p><code>NetworkManager</code> 会篡改 <code>default namespace</code> 中的网卡的路由表，而这些路由表将要被 Calico 的虚拟网卡链接到容器。这会干扰 <code>Calico agent</code> 导致不能正确路由。</p><p>创建这个配置文件在 <code>/etc/NetworkManager/conf.d/calico.conf</code>，用来防止 <code>NetworkManager</code> 去干扰网卡:</p><pre><code>[keyfile]unmanaged-devices=interface-name:cali*;interface-name:tunl*</code></pre><p><a href="#目录">返回目录</a></p><h1 id="运行Calico的方式"><a href="#运行Calico的方式" class="headerlink" title="运行Calico的方式"></a>运行Calico的方式</h1><p>大致上来说，Calico 有以下三种使用场景:</p><ul><li><a href="https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/calico" target="_blank" rel="noopener">安装Calico作为策略(policy)和网络(networking)(推荐用法)</a>。</li><li><a href="https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/flannel" target="_blank" rel="noopener">安装Calico作为策略(policy)和flannel作为网络(networking)</a>。</li><li><a href="https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/other" target="_blank" rel="noopener">安装Calico作为策略(policy)(进阶用法)</a>。</li></ul><p><strong>注意</strong>: 策略是指Kubernetes的<a href="https://kubernetes.io/docs/concepts/services-networking/networkpolicies/" target="_blank" rel="noopener">网络策略(network policy)</a>。</p><h2 id="安装Calico作为策略-policy-和网络-networking-推荐用法"><a href="#安装Calico作为策略-policy-和网络-networking-推荐用法" class="headerlink" title="安装Calico作为策略(policy)和网络(networking)(推荐用法)"></a>安装Calico作为策略(policy)和网络(networking)(推荐用法)</h2><p>Calico的数据存储(datastore)有2种方式，一个是使用 <code>Kubernetes API</code>；一个是使用etcd。<br>在使用不同数据存储和集群规模不同的情况下，安装配置的过程会有所不同:</p><ul><li><a href="https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/calico#installing-with-the-kubernetes-api-datastore50-nodes-or-less" target="_blank" rel="noopener">Kubernetes API datastore–少于50个节点</a></li><li><a href="https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/calico#installing-with-the-kubernetes-api-datastoremore-than-50-nodes" target="_blank" rel="noopener">Kubernetes API datastore–多于50个节点</a></li><li><a href="https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore" target="_blank" rel="noopener">etcd datastore</a></li></ul><p><a href="#目录">返回目录</a></p><h3 id="使用Kubernetes-API作为数据存储–少于50个节点的情况"><a href="#使用Kubernetes-API作为数据存储–少于50个节点的情况" class="headerlink" title="使用Kubernetes API作为数据存储–少于50个节点的情况"></a>使用Kubernetes API作为数据存储–少于50个节点的情况</h3><h4 id="1-先下载Calico的manifest文件。"><a href="#1-先下载Calico的manifest文件。" class="headerlink" title="1.  先下载Calico的manifest文件。"></a>1.  先下载Calico的manifest文件。</h4><p><code>curl https://docs.projectcalico.org/v3.7/manifests/calico.yaml -O</code></p><h4 id="2-根据使用-kubeadm-进行-k8s-集群初始化时传递的-pod-network-cidr-参数，或许需要修改-calico-yaml。"><a href="#2-根据使用-kubeadm-进行-k8s-集群初始化时传递的-pod-network-cidr-参数，或许需要修改-calico-yaml。" class="headerlink" title="2.  根据使用 kubeadm 进行 k8s 集群初始化时传递的 --pod-network-cidr 参数，或许需要修改 calico.yaml。"></a>2.  根据使用 <code>kubeadm</code> 进行 k8s 集群初始化时传递的 <code>--pod-network-cidr</code> 参数，或许需要修改 calico.yaml。</h4><blockquote><p>–pod-network-cidr 这个参数是用来给 <code>controller-manager</code> 用作自动分配pod子网(用作给每个node上的pod分配IP address)。<br>根据<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/#options" target="_blank" rel="noopener">官方文档</a>，如果在初始化时没有指定此参数，则不会自动分配pod子网。<br>另外在初始化时，要根据节点本身的网卡IP地址和将要使用的pod网络插件来<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">指定合理的<code>--pod-network-cidr</code>的数值</a>。</p></blockquote><blockquote><p>你的 <code>pod</code> 网络绝对不能和宿主机(节点)的任何网络重叠，否则将会产生问题。如果你发现你的网络插件的默认 <code>pod</code> 网络和你的一些宿主机网络冲突，你应该在 <code>kubeadm</code> 初始化时通过 <code>--pod-network-cidr</code> 传递一个合适的 <code>CIDR</code>，并且替换网络插件的 <code>YAML</code> 文件中的相应的值。</p></blockquote><p><a href="#目录">返回目录</a></p><p>以下是我准备用来安装 <code>kubernetes master</code>(control-plane with etcd)的 <code>vmware</code> 虚拟机(CentOS7)的IP信息。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@k8s-master-01 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ip address show</span>1: lo: <span class="token operator">&lt;</span>LOOPBACK,UP,LOWER_UP<span class="token operator">></span> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens33: <span class="token operator">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="token operator">></span> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:8a:5d:f6 brd ff:ff:ff:ff:ff:ff    inet 192.168.79.128/24 brd 192.168.79.255 scope global noprefixroute dynamic ens33       valid_lft 1448sec preferred_lft 1448sec    inet6 fe80::e587:6e91:1cee:5ef8/64 scope <span class="token function">link</span> noprefixroute       valid_lft forever preferred_lft forever3: virbr0: <span class="token operator">&lt;</span>NO-CARRIER,BROADCAST,MULTICAST,UP<span class="token operator">></span> mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:e0:fa:4c brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: <span class="token operator">&lt;</span>BROADCAST,MULTICAST<span class="token operator">></span> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:e0:fa:4c brd ff:ff:ff:ff:ff:ff5: docker0: <span class="token operator">&lt;</span>NO-CARRIER,BROADCAST,MULTICAST,UP<span class="token operator">></span> mtu 1500 qdisc noqueue state DOWN group default    link/ether 02:42:1e:9f:2c:14 brd ff:ff:ff:ff:ff:ff    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0       valid_lft forever preferred_lft foreverYou have new mail <span class="token keyword">in</span> /var/spool/mail/root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="#目录">返回目录</a></p><p>然后看一下 <code>calico.yaml</code> 中的 <code>CIDR</code> 这个环境变量的设置。</p><pre><code># The default IPv4 pool to create on startup if none exists. Pod IPs will be# chosen from this range. Changing this value after installation will have# no effect. This should fall within `--cluster-cidr`.- name: CALICO_IPV4POOL_CIDR  value: &quot;192.168.0.0/16&quot;</code></pre><p>可以看到 Calico 默认的 IPv4 CIDR 和我的虚拟机的主网卡的IP地址网段有重叠部分。Calico 的默认 CIDR – <code>192.168.0.0/16</code> 包括了我的虚拟机上 <code>public zone</code> 的网卡 <code>ens33</code> 的 <code>192.168.79.128/24</code>。<br>在网上找了一个<a href="https://cloud.tencent.com/document/product/215/20046#.E7.A7.81.E6.9C.89.E7.BD.91.E7.BB.9C.E7.BD.91.E6.AE.B5" target="_blank" rel="noopener">私有网络CIDR的规范</a>，决定在以下网段中找一个网段作为<code>--pod-network-cidr</code>:</p><blockquote><ul><li>10.0.0.0 - 10.255.255.255（掩码范围需在16 - 28之间）</li><li>172.16.0.0 - 172.31.255.255（掩码范围需在16 - 28之间）</li><li>192.168.0.0 - 192.168.255.255 （掩码范围需在16 - 28之间）</li></ul></blockquote><p>于是就决定用这个网段作为 <code>pod CIDR</code> 了: <code>172.16.0.0/16</code>，所以修改 <strong>calico.yaml</strong>中上述 <code>CALICO_IPV4POOL_CIDR</code> 的值。</p><h4 id="3-对于多网卡环境下的主机需要配置-IP-autodetection-methods。"><a href="#3-对于多网卡环境下的主机需要配置-IP-autodetection-methods。" class="headerlink" title="3.  对于多网卡环境下的主机需要配置 IP autodetection methods。"></a>3.  对于多网卡环境下的主机需要配置 <code>IP autodetection methods</code>。</h4><p>当 Calico 被用作<strong>路由</strong>，每个 node 必须配置一个 <strong>IPv4</strong> 地址 <strong>和/或者</strong> 一个 <strong>IPv6</strong> 地址，用作 node 间的路由。为了排除节点特定的 IP 地址的配置，calico/node 这个容器可以被配置为自动检测 IP 地址配置。在许多系统中，一个主机上或许会有多个物理网卡，或者可能有多个 IP 地址配置到一个物理网卡。在这些情况下，自动检测模式下会有多个地址可选，所以难以确认正确的地址。</p><p>为了改善选择正确地址的过程，Calico 提供了 <strong>IP 自动检测的方法</strong>，这些方法基于合适的条件给这个选择定义一些限制。</p><p><a href="#目录">返回目录</a></p><p>以下部分描述了可用的 IP 自动检测方法。</p><ul><li><p><strong>first-found</strong></p><p><code>first-found</code> 方法列举所有网卡 IP 地址然后返回第一个有效网卡上的第一个有效的 IP 地址(基于IP版本和地址的类型)。确切已知的 “local” 网卡会被忽略，例如 docker 网桥。网卡和 IP 地址的顺序根据不同系统会有差异。</p><p>这个是默认的检测方法。然而，由于这个方法只会作非常简单的假设，强烈推荐要么给节点配置一个特定的 IP 地址(应该是通过给 kubelet 指定参数)，要么使用另外一种检测方法。</p><p>e.g.</p><pre><code>IP_AUTODETECTION_METHOD=first-foundIP6_AUTODETECTION_METHOD=first-found</code></pre></li><li><p><strong>can-reach=DESTINATION</strong></p><p><code>can-reach</code> 方法使用你的本地路由来决定使用哪个 IP 地址来到达提供的目的地。可以使用 IP 地址或者域名。</p><p>使用 IP 地址的例子:</p><pre><code>IP_AUTODETECTION_METHOD=can-reach=8.8.8.8IP6_AUTODETECTION_METHOD=can-reach=2001:4860:4860::8888</code></pre><p>使用域名的例子:</p><pre><code>IP_AUTODETECTION_METHOD=can-reach=www.google.comIP6_AUTODETECTION_METHOD=can-reach=www.google.com</code></pre></li><li><p><strong>interface=INTERFACE-REGEX</strong></p><p><code>interface</code> 方法使用提供的网卡正则表达式(golang语法)去列举匹配到的网卡然后返回在第一个匹配到的网卡上的第一个 IP 地址。网卡和 IP 地址的顺序根据不同系统会有差异。</p><p>网卡 eth0, eth1, eth2 etc. 的有效 IP 地址的例子:</p><pre><code>IP_AUTODETECTION_METHOD=interface=eth.*IP6_AUTODETECTION_METHOD=interface=eth.*</code></pre></li></ul><p><a href="#目录">返回目录</a></p><h4 id="4-适用manifest。"><a href="#4-适用manifest。" class="headerlink" title="4.  适用manifest。"></a>4.  适用manifest。</h4><pre><code>kubectl apply -f calico.yaml</code></pre><p><a href="#目录">返回目录</a></p>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calico </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用kubeadm安装k8s集群</title>
      <link href="2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/"/>
      <url>2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/</url>
      
        <content type="html"><![CDATA[<h1 id="使用kubeadm安装k8s集群"><a href="#使用kubeadm安装k8s集群" class="headerlink" title="使用kubeadm安装k8s集群"></a>使用kubeadm安装k8s集群</h1><p>本文前提: 完成<a href="../tong-guo-ke-long-chuang-jian-k8s-jie-dian-ji-chu-huan-jing">通过克隆创建k8s节点基础环境</a>。</p><h1 id="OS基础信息"><a href="#OS基础信息" class="headerlink" title="OS基础信息"></a>OS基础信息</h1><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@k8s-master-01 helm<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># uname -a</span>Linux k8s-master-01.local 3.10.0-957.21.3.el7.x86_64 <span class="token comment" spellcheck="true">#1 SMP Tue Jun 18 16:35:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</span><span class="token punctuation">[</span>root@k8s-master-01 helm<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># cat /etc/os-release</span>NAME<span class="token operator">=</span><span class="token string">"CentOS Linux"</span>VERSION<span class="token operator">=</span><span class="token string">"7 (Core)"</span>ID<span class="token operator">=</span><span class="token string">"centos"</span>ID_LIKE<span class="token operator">=</span><span class="token string">"rhel fedora"</span>VERSION_ID<span class="token operator">=</span><span class="token string">"7"</span>PRETTY_NAME<span class="token operator">=</span><span class="token string">"CentOS Linux 7 (Core)"</span>ANSI_COLOR<span class="token operator">=</span><span class="token string">"0;31"</span>CPE_NAME<span class="token operator">=</span><span class="token string">"cpe:/o:centos:centos:7"</span>HOME_URL<span class="token operator">=</span><span class="token string">"https://www.centos.org/"</span>BUG_REPORT_URL<span class="token operator">=</span><span class="token string">"https://bugs.centos.org/"</span>CENTOS_MANTISBT_PROJECT<span class="token operator">=</span><span class="token string">"CentOS-7"</span>CENTOS_MANTISBT_PROJECT_VERSION<span class="token operator">=</span><span class="token string">"7"</span>REDHAT_SUPPORT_PRODUCT<span class="token operator">=</span><span class="token string">"centos"</span>REDHAT_SUPPORT_PRODUCT_VERSION<span class="token operator">=</span><span class="token string">"7"</span><span class="token punctuation">[</span>root@k8s-master-01 helm<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#开通端口">开通端口</a></li><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#安装CRI">安装CRI</a>  <ul><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#配置Docker代理">配置Docker代理</a></li></ul></li><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#安装kubeadmkubelet和kubectl">安装kubeadm、kubelet和kubectl</a></li><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#添加kubeadmkubectl命令行自动完成">添加kubeadm、kubectl命令行自动完成</a></li><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#初始化一个master节点">初始化一个master节点</a></li><li><a href="/2019/08/23/shi-yong-kubeadm-an-zhuang-k8s-ji-qun/#加入一个worker节点">加入一个worker节点</a></li></ul><h2 id="开通端口"><a href="#开通端口" class="headerlink" title="开通端口"></a>开通端口</h2><p>根据需求开通相应端口。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># firewall-cmd</span>firewall-cmd --zone<span class="token operator">=</span>public --list-all<span class="token comment" spellcheck="true"># 16443 for apiserver VIP</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>16443/tcp --permanent<span class="token comment" spellcheck="true"># master ports</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>6443/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>4001/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>2379-2380/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>10250/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>10251/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>10252/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>30000-32767/tcp --permanentfirewall-cmd --reload<span class="token comment" spellcheck="true"># worker node ports</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>10250/tcp --permanentfirewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>30000-32767/tcp --permanentfirewall-cmd --reload<span class="token comment" spellcheck="true"># Calico networking (BGP)</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>179/tcp --permanent<span class="token comment" spellcheck="true"># Calico networking with Typha enabled</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>5473/tcp --permanent<span class="token comment" spellcheck="true"># flannel networking (VXLAN)</span>firewall-cmd --zone<span class="token operator">=</span>public --add-port<span class="token operator">=</span>4789/udp --permanentfirewall-cmd --reload<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="#目录">返回目录</a></p><h2 id="安装CRI"><a href="#安装CRI" class="headerlink" title="安装CRI"></a>安装CRI</h2><p>CRI以Docker为例。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Install Docker CE</span><span class="token comment" spellcheck="true">## Set up the repository</span><span class="token comment" spellcheck="true">### Install required packages.</span>yum <span class="token function">install</span> yum-utils device-mapper-persistent-data lvm2<span class="token comment" spellcheck="true">### Add Docker repository.</span>yum-config-manager \  --add-repo \  https://download.docker.com/linux/centos/docker-ce.repo<span class="token comment" spellcheck="true">## Install Docker CE.</span><span class="token comment" spellcheck="true"># yum update &amp;&amp; yum install docker-ce-18.06.2.ce</span><span class="token function">cd</span> /root/docker_rpms/docker-ce-18.06.2.ce <span class="token operator">&amp;&amp;</span> \yum localinstall container-selinux-2.99-1.el7_6.noarch.rpm \docker-ce-18.06.2.ce-3.el7.x86_64.rpm<span class="token comment" spellcheck="true">## Create /etc/docker directory.</span><span class="token function">mkdir</span> /etc/docker<span class="token comment" spellcheck="true"># Setup daemon.</span><span class="token function">cat</span> <span class="token operator">></span> /etc/docker/daemon.json <span class="token operator">&lt;&lt;</span><span class="token string">EOF{  "exec-opts": ["native.cgroupdriver=systemd"],  "log-driver": "json-file",  "log-opts": {    "max-size": "20m"  },  "storage-driver": "overlay2",  "storage-opts": [    "overlay2.override_kernel_check=true"  ]}EOF</span><span class="token function">mkdir</span> -p /etc/systemd/system/docker.service.d<span class="token comment" spellcheck="true"># Restart Docker</span>systemctl daemon-reloadsystemctl restart docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="#目录">返回目录</a></p><h3 id="配置Docker代理"><a href="#配置Docker代理" class="headerlink" title="配置Docker代理"></a>配置Docker代理</h3><p>在Docker被墙的情况下可以通过配置Docker代理来爬楼梯。<br>此处使用的楼梯服务为shadowsocks。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mkdir</span> /etc/systemd/system/docker.service.d<span class="token function">cat</span> <span class="token operator">></span> /etc/systemd/system/docker.service.d/http-proxy.conf <span class="token operator">&lt;&lt;</span> <span class="token string">EOF[Service]Environment="HTTP_PROXY=http://192.168.79.1:1080/"Environment="HTTPS_PROXY=http://192.168.79.1:1080/"Environment="NO_PROXY=localhost,127.0.0.1,192.168.*,172.16.*,172.17.*,10.*"EOF</span>systemctl daemon-reloadsystemctl restart dockersystemctl show docker --property Environment<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="#目录">返回目录</a></p><h2 id="安装kubeadm、kubelet和kubectl"><a href="#安装kubeadm、kubelet和kubectl" class="headerlink" title="安装kubeadm、kubelet和kubectl"></a>安装kubeadm、kubelet和kubectl</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cat</span> <span class="token operator">&lt;&lt;</span>EOF <span class="token operator">></span> /etc/yum.repos.d/kubernetes.repo<span class="token punctuation">[</span>kubernetes<span class="token punctuation">]</span>name<span class="token operator">=</span>Kubernetesbaseurl<span class="token operator">=</span>https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled<span class="token operator">=</span>1gpgcheck<span class="token operator">=</span>1repo_gpgcheck<span class="token operator">=</span>1gpgkey<span class="token operator">=</span>https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgexclude<span class="token operator">=</span>kube*EOF<span class="token comment" spellcheck="true"># Set SELinux in permissive mode (effectively disabling it)</span>setenforce 0<span class="token function">sed</span> -i <span class="token string">'s/^SELINUX=enforcing$/SELINUX=permissive/'</span> /etc/selinux/config<span class="token comment" spellcheck="true"># yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes</span><span class="token function">cd</span> /root/k8s_rpms/kubernetes-v1.14.0 <span class="token operator">&amp;&amp;</span> \yum localinstall -y \conntrack-tools-1.4.4-4.el7.x86_64.rpm \cri-tools-1.12.0-0.x86_64.rpm \kubeadm-1.14.0-0.x86_64.rpm \kubectl-1.14.0-0.x86_64.rpm \kubelet-1.14.0-0.x86_64.rpm \kubernetes-cni-0.7.5-0.x86_64.rpm \libnetfilter_cthelper-1.0.0-9.el7.x86_64.rpm \libnetfilter_cttimeout-1.0.0-6.el7.x86_64.rpm \libnetfilter_queue-1.0.2-2.el7_2.x86_64.rpm \socat-1.7.3.2-2.el7.x86_64.rpmsystemctl <span class="token function">enable</span> --now kubelet<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="#目录">返回目录</a></p><p>请注意:</p><ul><li><p>通过命令 <code>setenforce 0</code> 和 <code>sed ...</code> 可以将 <code>SELinux</code> 设置为 <code>permissive</code> 模式(将其禁用)。 只有执行这一操作之后，容器才能访问宿主的文件系统，进而能够正常使用 Pod 网络。您必须这么做，直到 <code>kubelet</code> 做出升级支持 <code>SELinux</code> 为止。</p></li><li><p>一些 RHEL/CentOS 7 的用户曾经遇到过: 由于 <code>iptables</code> 被绕过导致网络请求被错误的路由。您得保证 <strong>在您的 <code>sysctl</code> 配置中 <code>net.bridge.bridge-nf-call-iptables</code> 被设为1</strong>。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cat</span> <span class="token operator">&lt;&lt;</span>EOF <span class="token operator">></span>  /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables <span class="token operator">=</span> 1net.bridge.bridge-nf-call-iptables <span class="token operator">=</span> 1EOFsysctl --system<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>保证在执行这一步之前 <code>br_netfilter</code> 已经被加载。可以通过 <code>lsmod | grep br_netfilter</code> 确认是否已加载。并且可以通过 <code>modprobe br_netfilter</code> 命令加载。</p></li></ul><h2 id="初始化一个master节点"><a href="#初始化一个master节点" class="headerlink" title="初始化一个master节点"></a>初始化一个master节点</h2><p>初始化一个master节点的详细步骤可以参阅<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/" target="_blank" rel="noopener">官网</a>以及我整理的另一篇的文章。</p><pre><code># 请先阅读上面链接，对执行kubeadm的OS基础条件和将要使用的网络组件的要求有所了解。# on the first masterkubeadm init --kubernetes-version=v1.14.0 --apiserver-advertise-address=&lt;ip-address&gt; --pod-network-cidr=172.16.0.0/16</code></pre><h2 id="加入一个worker节点"><a href="#加入一个worker节点" class="headerlink" title="加入一个worker节点"></a>加入一个worker节点</h2><p>k8s 用到的镜像可以事先使用 Docker 或者 <code>kubeadm config images pull</code> 下载好。如果需要设置梯子，可以参考<a href="#配置Docker代理">配置Docker代理</a>。<br>这里，为了不重复下载一遍镜像，我把镜像从 master-01 上导出到这个 <code>worker node</code> 上的 <code>/root/k8s_images/</code> 下按照组织分类的各自目录下了。</p><ol><li><p>导入k8s的镜像。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cd</span> /root/k8s_images/k8s.gcr.io <span class="token operator">&amp;&amp;</span> \<span class="token keyword">for</span> image <span class="token keyword">in</span> <span class="token variable"><span class="token variable">`</span><span class="token function">ls</span> -1<span class="token variable">`</span></span><span class="token punctuation">;</span> <span class="token keyword">do</span>    docker load -i <span class="token variable">$image</span><span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>导入calico的镜像。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cd</span> /root/k8s_images/calico <span class="token operator">&amp;&amp;</span> \<span class="token keyword">for</span> image <span class="token keyword">in</span> <span class="token variable"><span class="token variable">`</span><span class="token function">ls</span> -1<span class="token variable">`</span></span><span class="token punctuation">;</span> <span class="token keyword">do</span>    docker load -i <span class="token variable">$image</span><span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>在master上查看加入节点时使用的token。</p><pre><code>kubeadm token list</code></pre><p>如果没有有效的token，则创建:</p><pre><code>kubeadm token create --print-join-command</code></pre><p>然后在 worker node 上运行上述命令的结果。</p></li></ol><p>注意:</p><ul><li>在执行上述添加 worker 节点命令之前，务必确认已经满足pod网络插件的前提条件，详细参考。</li><li>此处网络插件以 Calico 为例。</li></ul><p><a href="#目录">返回目录</a></p><h2 id="添加kubeadm、kubectl命令行自动完成"><a href="#添加kubeadm、kubectl命令行自动完成" class="headerlink" title="添加kubeadm、kubectl命令行自动完成"></a>添加kubeadm、kubectl命令行自动完成</h2><ul><li><p>kubeadm 命令行自动完成。</p><pre><code># Load the kubeadm completion code for bash into the current shellsource &lt;(kubeadm completion bash)# Write bash completion code to a file and source it from .bash_profilekubeadm completion bash &gt; ~/.kube/kubeadm_completion.bash.incprintf &quot;\n# Kubeadm shell completion\nsource &#39;$HOME/.kube/kubeadm_completion.bash.inc&#39;\n&quot; &gt;&gt; $HOME/.bash_profilesource $HOME/.bash_profile</code></pre></li><li><p>kubectl 命令行自动完成。</p><pre><code>source &lt;(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first.echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc # add autocomplete permanently to your bash shell.</code></pre></li></ul><p><a href="#目录">返回目录</a></p>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> kubeadm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test post</title>
      <link href="2019/08/22/dear/"/>
      <url>2019/08/22/dear/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="110" src="//music.163.com/outchain/player?type=4&id=350667152&auto=1&height=90"></iframe></div><p>❤️ 亲亲的(づ￣3￣)づ╭❤～</p><h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><hr><p>The standard Lorem Ipsum passage, used since the 1500s<br>“Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.”</p><p>Section 1.10.32 of “de Finibus Bonorum et Malorum”, written by Cicero in 45 BC<br>“Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?”</p><p>1914 translation by H. Rackham<br>“But I must explain to you how all this mistaken idea of denouncing pleasure and praising pain was born and I will give you a complete account of the system, and expound the actual teachings of the great explorer of the truth, the master-builder of human happiness. No one rejects, dislikes, or avoids pleasure itself, because it is pleasure, but because those who do not know how to pursue pleasure rationally encounter consequences that are extremely painful. Nor again is there anyone who loves or pursues or desires to obtain pain of itself, because it is pain, but because occasionally circumstances occur in which toil and pain can procure him some great pleasure. To take a trivial example, which of us ever undertakes laborious physical exercise, except to obtain some advantage from it? But who has any right to find fault with a man who chooses to enjoy a pleasure that has no annoying consequences, or one who avoids a pain that produces no resultant pleasure?”</p><p>Section 1.10.33 of “de Finibus Bonorum et Malorum”, written by Cicero in 45 BC<br>“At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.”</p><p>1914 translation by H. Rackham<br>“On the other hand, we denounce with righteous indignation and dislike men who are so beguiled and demoralized by the charms of pleasure of the moment, so blinded by desire, that they cannot foresee the pain and trouble that are bound to ensue; and equal blame belongs to those who fail in their duty through weakness of will, which is the same as saying through shrinking from toil and pain. These cases are perfectly simple and easy to distinguish. In a free hour, when our power of choice is untrammelled and when nothing prevents our being able to do what we like best, every pleasure is to be welcomed and every pain avoided. But in certain circumstances and owing to the claims of duty or the obligations of business it will frequently occur that pleasures have to be repudiated and annoyances accepted. The wise man therefore always holds in these matters to this principle of selection: he rejects pleasures to secure other greater pleasures, or else he endures pains to avoid worse pains.”</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
